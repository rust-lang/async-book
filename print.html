<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Asynchronous Programming in Rust</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Asynchronous Programming in Rust</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/rust-lang/async-book" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <p>NOTE: this guide is currently undergoing a rewrite after a long time without much work. It is work in progress, much is missing, and what exists is a bit rough.</p>
<h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>This book is a guide to asynchronous programming in Rust. It is designed to help you take your first steps and to discover more about advanced topics. We don't assume any experience with asynchronous programming (in Rust or another language), but we do assume you're familiar with Rust already. If you want to learn about Rust, you could start with <a href="https://doc.rust-lang.org/stable/book/">The Rust Programming Language</a>.</p>
<p>This book has two main parts: <a href="part-guide/intro.html">part one</a> is a beginners guide, it is designed to be read in-order and to take you from total beginner to intermediate level. Part two is a collection of stand-alone chapters on more advanced topics. It should be useful once you've worked through part one or if you already have some experience with async Rust.</p>
<p>You can navigate this book in multiple ways:</p>
<ul>
<li>You can read it front to back, in order. This is the recommend path for newcomers to async Rust, at least for <a href="part-guide/intro.html">part one</a> of the book.</li>
<li>There is a summary contents on the left-hand side of the webpage.</li>
<li>If you want information about a broad topic, you could start with the topic index.</li>
<li>If you want to find all discussion about a specific topic, you could start with the detailed index.</li>
<li>You could see if your question is answered in the FAQs.</li>
</ul>
<h2 id="what-is-async-programming-and-why-would-you-do-it"><a class="header" href="#what-is-async-programming-and-why-would-you-do-it">What is Async Programming and why would you do it?</a></h2>
<p>In concurrent programming, the program does multiple things at the same time (or at least appears to). Programming with threads is one form of concurrent programming. Code within a thread is written in sequential style and the operating system executes threads concurrently. With async programming, concurrency happens entirely within your program (the operating system is not involved). An async runtime (which is just another crate in Rust) manages async tasks in conjunction with the programmer explicitly yielding control by using the <code>await</code> keyword.</p>
<p>Because the operating system is not involved, <em>context switching</em> in the async world is very fast. Furthermore, async tasks have much lower memory overhead than operating system threads. This makes async programming a good fit for systems which need to handle very many concurrent tasks and where those tasks spend a lot of time waiting (for example, for client responses or for IO). It also makes async programming a good fit for microcontrollers with very limited amounts of memory and no operating system that provides threads.</p>
<p>Async programming also offers the programmer fine-grained control over how tasks are executed (levels of parallelism and concurrency, control flow, scheduling, and so forth). This means that async programming can be expressive as well as ergonomic for many uses. In particular, async programming in Rust has a powerful concept of cancellation and supports many different flavours of concurrency (expressed using constructs including <code>spawn</code> and its variations, <code>join</code>, <code>select</code>, <code>for_each_concurrent</code>, etc.). These allow composable and reusable implementations of concepts like timeouts, pausing, and throttling.</p>
<h2 id="hello-world"><a class="header" href="#hello-world">Hello, world!</a></h2>
<p>Just to give you a taste of what async Rust looks like, here is a 'hello, world' example. There is no concurrency, and it doesn't really take advantage of being async. It does define and use an async function, and it does print "hello, world!":</p>
<pre><pre class="playground"><code class="language-rust edition2021">// Define an async function.
async fn say_hello() {
    println!("hello, world!");
}

#[tokio::main] // Boilerplate which lets us write `async fn main`, we'll explain it later.
async fn main() {
    // Call an async function and await its result.
    say_hello().await;
}</code></pre></pre>
<p>We'll explain everything in detail later. For now, note how we define an asynchronous function using <code>async fn</code> and call it using <code>.await</code> - an async function in Rust doesn't do anything unless it is <code>await</code>ed<sup class="footnote-reference" id="fr-blocking-1"><a href="#footnote-blocking">1</a></sup>.</p>
<p>Like all examples in this book, if you want to see the full example (including <code>Cargo.toml</code>, for example) or to run it yourself locally, you can find them in the book's GitHub repo: e.g., <a href="https://github.com/rust-lang/async-book/tree/master/examples/hello-world">examples/hello-world</a>.</p>
<h2 id="development-of-async-rust"><a class="header" href="#development-of-async-rust">Development of Async Rust</a></h2>
<p>The async features of Rust have been in development for a while, but it is not a 'finished' part of the language. Async Rust (at least the parts available in the stable compiler and standard libraries) is reliable and performant. It is used in production in some of the most demanding situations at the largest tech companies. However, there are some missing parts and rough edges (rough in the sense of ergonomics rather than reliability). You are likely to stumble upon some of these parts during your journey with async Rust. For most missing parts, there are workarounds and these are covered in this book.</p>
<p>Currently, working with async iterators (also known as streams) is where most users find some rough parts. Some uses of async in traits are not yet well-supported. There is not a good solution for async destruction.</p>
<p>Async Rust is being actively worked on. If you want to follow development, you can check out the Async Working Group's <a href="https://rust-lang.github.io/wg-async/meetings.html">home page</a> which includes their <a href="https://rust-lang.github.io/wg-async/vision/roadmap.html">roadmap</a>. Or you could read the async <a href="https://github.com/rust-lang/rust-project-goals/issues/105">project goal</a> within the Rust Project.</p>
<p>Rust is an open source project. If you'd like to contribute to development of async Rust, start at the <a href="https://github.com/rust-lang/rust/blob/master/CONTRIBUTING.md">contributing docs</a> in the main Rust repo.</p>
<hr>
<ol class="footnote-definition"><li id="footnote-blocking">
<p>This is actually a bad example because <code>println</code> is <em>blocking IO</em> and it is generally a bad idea to do blocking IO in async functions. We'll explain what blocking IO is in <a href="">chapter TODO</a> and why you shouldn't do blocking IO in an async function in <a href="">chapter TODO</a>. <a href="#fr-blocking-1">↩</a></p>
</li>
</ol><div style="break-before: page; page-break-before: always;"></div><h1 id="navigation"><a class="header" href="#navigation">Navigation</a></h1>
<p>TODO Intro to navigation</p>
<ul>
<li><a href="navigation/topics.html">By topic</a></li>
<li><a href="navigation/">FAQs</a></li>
<li><a href="navigation/index.html">Index</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="topic-index"><a class="header" href="#topic-index">Topic index</a></h1>
<h2 id="concurrency-and-parallelism"><a class="header" href="#concurrency-and-parallelism">Concurrency and parallelism</a></h2>
<ul>
<li><a href="navigation/../part-guide/concurrency.html#concurrency-and-parallelism">Introduction</a></li>
<li><a href="navigation/../part-guide/async-await.html#spawning-tasks">Running async tasks in parallel using <code>spawn</code></a></li>
<li><a href="navigation/../part-guide/concurrency-primitives.html">Running futures concurrently using <code>join</code> and <code>select</code></a></li>
</ul>
<h2 id="correctness-and-safety"><a class="header" href="#correctness-and-safety">Correctness and safety</a></h2>
<ul>
<li>Cancellation
<ul>
<li><a href="navigation/../part-guide/more-async-await.html#cancellation">Introduction</a></li>
<li><a href="navigation/../part-guide/concurrency-primitives.html">In <code>select</code> and <code>try_join</code></a></li>
</ul>
</li>
</ul>
<h2 id="performance"><a class="header" href="#performance">Performance</a></h2>
<ul>
<li>Blocking
<ul>
<li><a href="navigation/../part-guide/more-async-await.html#blocking-and-cancellation">Introduction</a></li>
</ul>
</li>
</ul>
<h2 id="testing"><a class="header" href="#testing">Testing</a></h2>
<ul>
<li><a href="navigation/../part-guide/more-async-await.html#unit-tests">Unit test syntax</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="index"><a class="header" href="#index">Index</a></h1>
<ul>
<li>
<p>Async/<code>async</code></p>
<ul>
<li><a href="navigation/../part-guide/more-async-await.html#async-blocks">blocks</a></li>
<li><a href="navigation/../part-guide/more-async-await.html#async-closures">closures</a></li>
<li><a href="navigation/../part-guide/async-await.html#async-functions">functions</a></li>
<li><a href="navigation/../part-guide/more-async-await.html#async-traits">traits</a></li>
<li><a href="navigation/../part-guide/concurrency.html#async-programming">c.f., threads</a></li>
</ul>
</li>
<li>
<p><a href="navigation/../part-guide/async-await.html#await"><code>await</code></a></p>
</li>
<li>
<p><a href="navigation/../part-guide/more-async-await.html#blocking-and-cancellation">Blocking</a></p>
<ul>
<li><a href="navigation/../part-guide/more-async-await.html#blocking-io">IO</a></li>
</ul>
</li>
<li>
<p><a href="navigation/../part-guide/more-async-await.html#cancellation">Cancellation</a></p>
<ul>
<li><a href="navigation/../part-guide/more-async-await.html#cancellation"><code>CancellationToken</code></a></li>
<li><a href="navigation/../part-guide/concurrency-primitives.html#race-select">In <code>select</code></a></li>
</ul>
</li>
<li>
<p><a href="navigation/../part-guide/concurrency.html">Concurrency</a></p>
<ul>
<li><a href="navigation/../part-guide/concurrency.html#concurrency-and-parallelism">c.f., parallelism</a></li>
<li><a href="navigation/../part-guide/concurrency-primitives.html">Primitives (<code>join</code>, <code>select</code>, etc.)</a></li>
</ul>
</li>
<li>
<p><a href="navigation/../part-guide/async-await.html#the-runtime">Executor</a></p>
</li>
<li>
<p><a href="navigation/../part-guide/async-await.html#futures-and-tasks">Futures</a></p>
<ul>
<li><code>Future</code> trait</li>
</ul>
</li>
<li>
<p>IO</p>
<ul>
<li><a href="navigation/../part-guide/more-async-await.html#blocking-io">Blocking</a></li>
</ul>
</li>
<li>
<p><a href="navigation/../part-guide/concurrency-primitives.html#join"><code>join</code></a></p>
</li>
<li>
<p><a href="navigation/../part-guide/async-await.html#joining-tasks">Joining tasks</a></p>
</li>
<li>
<p><a href="navigation/../part-guide/async-await.html#joinhandle"><code>JoinHandle</code></a></p>
<ul>
<li><a href="navigation/../part-guide/more-async-await.html#cancellation"><code>abort</code></a></li>
</ul>
</li>
<li>
<p>Multitasking</p>
<ul>
<li><a href="navigation/../part-guide/concurrency.html#async-programming">Cooperative</a></li>
<li><a href="navigation/../part-guide/concurrency.html#processes-and-threads">Pre-emptive</a></li>
</ul>
</li>
<li>
<p><a href="navigation/../part-guide/concurrency.html#concurrency-and-parallelism">Parallelism</a></p>
<ul>
<li><a href="navigation/../part-guide/concurrency.html#concurrency-and-parallelism">c.f., concurrency</a></li>
</ul>
</li>
<li>
<p><a href="navigation/../part-guide/concurrency-primitives.html#race-select"><code>race</code></a></p>
</li>
<li>
<p><a href="navigation/../part-guide/async-await.html#the-runtime">Reactor</a></p>
</li>
<li>
<p><a href="navigation/../part-guide/async-await.html#the-runtime">Runtimes</a></p>
</li>
<li>
<p><a href="navigation/../part-guide/async-await.html#the-runtime">Scheduler</a></p>
</li>
<li>
<p><a href="navigation/../part-guide/concurrency-primitives.html#race-select"><code>select</code></a></p>
</li>
<li>
<p><a href="navigation/../part-guide/async-await.html#spawning-tasks">Spawning tasks</a></p>
</li>
<li>
<p><a href="navigation/../part-guide/async-await.html#futures-and-tasks">Tasks</a></p>
<ul>
<li><a href="navigation/../part-guide/async-await.html#spawning-tasks">Spawning</a></li>
</ul>
</li>
<li>
<p>Testing</p>
<ul>
<li><a href="navigation/../part-guide/more-async-await.html#unit-tests">Unit tests</a></li>
</ul>
</li>
<li>
<p><a href="navigation/../part-guide/concurrency.html#processes-and-threads">Threads</a></p>
</li>
<li>
<p><a href="navigation/../part-guide/async-await.html#the-runtime">Tokio</a></p>
</li>
<li>
<p>Traits</p>
<ul>
<li><a href="navigation/../part-guide/more-async-await.html#async-traits">async</a></li>
<li><code>Future</code>
<a href="navigation/../part-guide/concurrency-primitives.html#join"><code>try_join</code></a></li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="part-1-a-guide-to-asynchronous-programming-in-rust"><a class="header" href="#part-1-a-guide-to-asynchronous-programming-in-rust">Part 1: A guide to asynchronous programming in Rust</a></h1>
<p>This part of the book is a tutorial-style guide to async Rust. It is aimed at newcomers to async programming in Rust. It should be useful whether or not you've done async programming in other languages. If you have, you might skip the first section or skim it as a refresher. You might also want to read this <a href="part-guide/">comparison to async in other languages</a> sooner rather than later.</p>
<h2 id="core-concepts"><a class="header" href="#core-concepts">Core concepts</a></h2>
<p>We'll start by discussing different models of <a href="part-guide/concurrency.html">concurrent programming</a>, using processes, threads, or async tasks. The first chapter will cover the essential parts of Rust's async model before we get into the nitty-gritty of async programming in the <a href="part-guide/async-await.html">second chapter</a> where we introduce the async and await programming paradigm. We cover some more async programming concepts in the <a href="part-guide/more-async-await.html">following chapter</a>.</p>
<p>One of the main motivations for async programming is more performant IO, which we cover in the <a href="part-guide/io.html">next chapter</a>. We also cover <em>blocking</em> in detail in the same chapter. Blocking is a major hazard in async programming where a thread is blocked from making progress by an operation (often IO) which synchronously waits.</p>
<p>Another motivation for async programming is that it facilitates new models for <a href="part-guide/concurrency-primitives.html">abstraction and composition of concurrent code</a>. After covering that, we move on to <a href="part-guide/sync.html">synchronization</a> between concurrent tasks.</p>
<p>There is a chapter on <a href="part-guide/tools.html">tools for async programming</a>.</p>
<p>The last few chapters cover some more specialised topics, starting with <a href="part-guide/dtors.html">async destruction and clean-up</a> (which is a common requirement, but since there is currently not a good built-in solution, is a bit of a specialist topic).</p>
<p>The next two chapters in the guide go into detail on <a href="part-guide/futures.html">futures</a> and <a href="part-guide/runtimes.html">runtimes</a>, two fundamental building blocks for async programming.</p>
<p>Finally, we cover <a href="part-guide/timers-signals.html">timers and signal handling</a> and <a href="part-guide/streams.html">async iterators</a> (aka streams). The latter are how we program with sequences of async events (c.f., individual async events which are represented using futures or async functions). This is an area where the language is being actively developed and can be a little rough around the edges.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="concurrent-programming"><a class="header" href="#concurrent-programming">Concurrent programming</a></h1>
<p>The goal of this chapter is to give you a high-level idea of how async concurrency works and how it is different from concurrency with threads. I think it is important to have a good mental model of what is going on before getting in to the practicalities, but if you're the kind of person who likes to see some real code first, you might like to read the next chapter or two and then come back to this one.</p>
<p>We'll start with some motivation, then cover <a href="part-guide/concurrency.html#sequential-execution">sequential programming</a>, <a href="part-guide/concurrency.html#processes-and-threads">programming with threads or processes</a>, and then <a href="part-guide/concurrency.html#async-programming">async programming</a>. The chapter finishes with a section on <a href="part-guide/concurrency.html#concurrency-and-parallelism">concurrency and parallelism</a>.</p>
<p>Users want their computers to do multiple things. Sometimes users want to do those things at the same time (e.g., be listening to a music app at the same time as typing in their editor). Sometimes doing multiple tasks at the same time is more efficient (e.g., getting some work done in the editor while a large file downloads). Sometimes there are multiple users wanting to use a single computer at the same time (e.g., multiple clients connected to a server).</p>
<p>To give a lower-level example, a music program might need to keep playing music while the user interacts with the user interface (UI). To 'keep playing music', it might need to stream music data from the server, process that data from one format to another, and send the processed data to the computer's audio system via the operating system (OS). For the user, it might need to send and receive data or commands to the server in response to the user instructions, it might need to send signals to the subsystem playing music (e.g., if the user changes track or pauses), it might need to update the graphical display (e.g., highlighting a button or changing the track name), and it must keep the mouse cursor or text inputs responsive while doing all of the above.</p>
<p>Doing multiple things at once (or appearing to do so) is called concurrency. Programs (in conjunction with the OS) must manage their concurrency and there are many ways to do that. We'll describe some of those ways in this chapter, but we'll start with purely sequential code, i.e., no concurrency at all.</p>
<h2 id="sequential-execution"><a class="header" href="#sequential-execution">Sequential execution</a></h2>
<p>The default mode of execution in most programming languages (including Rust) is sequential execution.</p>
<pre><code>do_a_thing();
println!("hello!");
do_another_thing();
</code></pre>
<p>Each statement is completed before the next one starts<sup class="footnote-reference" id="fr-obs1-1"><a href="#footnote-obs1">1</a></sup>. Nothing happens in between those statements<sup class="footnote-reference" id="fr-obs2-1"><a href="#footnote-obs2">2</a></sup>. This might sound trivial but it is a really useful property for reasoning about our code. However, it also means we waste a lot of time. In the above example, while we're waiting for <code>println!("hello!")</code> to happen, we could have executed <code>do_another_thing()</code>. Perhaps we could even have executed all three statements at the same time.</p>
<p>Whenever IO<sup class="footnote-reference" id="fr-io-def-1"><a href="#footnote-io-def">3</a></sup> happens (printing using <code>println!</code> is IO - it is outputting text to the console via a call to the OS), the program will wait for the IO to complete<sup class="footnote-reference" id="fr-io-complete-1"><a href="#footnote-io-complete">4</a></sup> before executing the next statement. Waiting for IO to complete before continuing with execution <em>blocks</em> the program from making other progress. Blocking IO is the easiest kind of IO to use, implement, and reason about, but it is also the least efficient - in a sequential world, the program can do nothing while it waits for the IO to complete.</p>
<h2 id="processes-and-threads"><a class="header" href="#processes-and-threads">Processes and threads</a></h2>
<p>Processes and threads are concepts which are provided by the operating system to provide concurrency. There is one process per executable, so supporting multiple processes means a computer can run multiple programs<sup class="footnote-reference" id="fr-proc-program-1"><a href="#footnote-proc-program">5</a></sup> concurrently; there can be multiple threads per process, which means there can also be concurrency <em>within</em> a process.</p>
<p>There are many small differences in the way that processes and threads are handled. The most important difference is that memory is shared between threads but not between processes<sup class="footnote-reference" id="fr-shmem-1"><a href="#footnote-shmem">6</a></sup>. That means that communication between processes happens by some kind of message passing, similar to communicating between programs running on different computers. From a program's perspective, the single process is their whole world; creating new processes means running new programs. Creating new threads, however, is just part of the program's regular execution.</p>
<p>Because of these distinctions between processes and threads, they feel very different to a programmer. But from the OS's perspective they are very similar and we'll discuss their properties as if they were a single concept. We'll talk about threads, but unless we note otherwise, you should understand that to mean 'threads or processes'.</p>
<p>The OS is responsible for <em>scheduling</em> threads, which means it decides when threads run and for how long. Most modern computers have multiple cores, so they can run multiple threads at literally the same time. However, it is common to have many more threads than cores, so the OS will run each thread for a small amount of time and then pause it and run a different thread for some time<sup class="footnote-reference" id="fr-sched-1"><a href="#footnote-sched">7</a></sup>. When multiple threads are run on a single core in this fashion, it is called <em>interleaving</em> or <em>time-slicing</em>. Since the OS chooses when to pause a thread's execution, it is called <em>pre-emptive multitasking</em> (multitasking here just means running multiple threads at the same time); the OS <em>pre-empts</em> execution of a thread (or more verbosely, the OS pre-emptively pauses execution. It is pre-emptive because the OS is pausing the thread to make time for another thread, before the first thread would otherwise pause, to ensure that the second thread can execute before it becomes a problem that it can't).</p>
<p>Let's look at IO again. What happens when a thread blocks waiting for IO? In a system with threads, then the OS will pause the thread (it's just going to be waiting in any case) and wake it up again when the IO is complete<sup class="footnote-reference" id="fr-busywait-1"><a href="#footnote-busywait">8</a></sup>. Depending on the scheduling algorithm, it might take some time after the IO completes until the OS wakes up the thread waiting for IO, since the OS might wait for other threads to get some work done. So now things are much more efficient: while one thread waits for IO, another thread (or more likely, many threads due to multitasking) can make progress. But, from the perspective of the thread doing IO, things are still sequential - it waits for the IO to finish before starting the next operation.</p>
<p>A thread can also choose to pause itself by calling a <code>sleep</code> function, usually with a timeout. In this case the OS pauses the thread at the threads own request. Similar to pausing due to pre-emption or IO, the OS will wake the thread up again later (after the timeout) to continue execution.</p>
<p>When an OS pauses one thread and starts another (for any reason), it is called <em>context switching</em>. The context being switched includes the registers, operating system records, and the contents of many caches. That's a non-trivial amount of work. Together with the transfer of control to the OS and back to a thread, and the costs of working with stale caches, context switching is an expensive operation.</p>
<p>Finally, note that some hardware or OSs do not support processes or threads, this is more likely in the embedded world.</p>
<h2 id="async-programming"><a class="header" href="#async-programming">Async programming</a></h2>
<p>Async programming is a kind of concurrency with the same high-level goals as concurrency with threads (do many things at the same time), but a different implementation. The two big differences between async concurrency and concurrency with threads, is that async concurrency is managed entirely within the program with no help from the OS<sup class="footnote-reference" id="fr-threads-1"><a href="#footnote-threads">9</a></sup>, and that multitasking is cooperative rather than pre-emptive<sup class="footnote-reference" id="fr-other-1"><a href="#footnote-other">10</a></sup> (we'll explain that in a minute). There are many different models of async concurrency, we'll compare them later on in the guide, but for now we'll focus only on Rust's model.</p>
<p>To distinguish them from threads, we'll call a sequence of executions in async concurrency a task (they're also called <em>green threads</em>, but this sometimes has connotations of pre-emptive scheduling and implementation details like one stack per task). The way a task is executed, scheduled, and represented in memory is very different to a thread, but for a high-level intuition, it can be useful to think of tasks as just like threads, but managed entirely within the program, rather than by the OS.</p>
<p>In an async system, there is still a scheduler which decides which task to run next (it's part of the program, not part of the OS). However, the scheduler cannot pre-empt a task. Instead a task must voluntarily give up control and allow another task to be scheduled. Because tasks must cooperate (by giving up control), this is called cooperative multitasking.</p>
<p>Using cooperative rather than pre-emptive multitasking has many implications:</p>
<ul>
<li>between points where control might be yielded, you can guarantee that code will be executed sequentially - you'll never be unexpectedly paused,</li>
<li>if a task takes a long time between yield points (e.g., by doing blocking IO or performing long-running computation), other tasks will not be able to make progress,</li>
<li>implementing a scheduler is much simpler and scheduling (and context switching) has fewer overheads.</li>
</ul>
<p>Async concurrency is much more efficient than concurrency with threads. The memory overheads are much lower and context switching is a much cheaper operation - it doesn't require handing control to the OS and back to the program and there is much less data to switch. However, there can still be some cache effects - although the OS's caches such as the <a href="https://en.wikipedia.org/wiki/Translation_lookaside_buffer">TLB</a> don't need to be changed, tasks are likely to operate on different parts of memory, so data required by the newly scheduled task may not be in a memory cache.</p>
<p>Asynchronous <em>IO</em> is an alternative to blocking IO (it's sometimes called non-blocking IO). Async IO is not directly tied to async concurrency, but the two are often used together. In async IO, a program initiates IO with one system call and then can either check or be notified when the IO completes. That means the program is free to get other work done while the IO takes place. In Rust, the mechanics of async IO are handled by the async runtime (the scheduler is also part of the runtime, we'll discuss runtimes in more detail later in this book, but essentially the runtime is just a library which takes care of some of the fundamental async stuff).</p>
<p>From the perspective of the whole system, blocking IO in a concurrent system with threads and non-blocking IO in an async concurrent system are similar. In both cases, IO takes time and other work gets done while the IO is happening:</p>
<ul>
<li>With threads, the thread doing IO requests IO from the OS, the thread is paused by the OS, other threads get work done, and when the IO is done, the OS wakes up the thread so it can continue execution with the result of the IO.</li>
<li>With async, the task doing IO requests IO from the runtime, the runtime requests IO from the OS but the OS returns control to the runtime. The runtime pauses the IO task and schedules other tasks to get work done. When the IO is done, the runtime wakes up the IO task so it can continue execution with the result of the IO.</li>
</ul>
<p>The advantage of using async IO, is that the overheads are much lower so a system can support orders of magnitude more tasks than threads. That makes async concurrency particularly well-suited for tasks with lots of users which spend a lot of time waiting for IO (if they don't spend a lot of time waiting and instead do lots of CPU-bound work, then there is not so much advantage to the low-overheads because the bottleneck will be CPU and memory resources).</p>
<p>Threads and async are not mutually exclusive: many programs use both. Some programs have parts which are better implemented using threads and parts which are better implemented using async. For example, a database server may use async techniques to manage network communication with clients, but use OS threads for computation on data. Alternatively, a program may be written only using async concurrency, but the runtime will execute tasks on multiple threads. This is necessary for a program to make use of multiple CPU cores. We'll cover the intersection of threads and async tasks in a number of places later in the book.</p>
<h2 id="concurrency-and-parallelism-1"><a class="header" href="#concurrency-and-parallelism-1">Concurrency and Parallelism</a></h2>
<p>So far we've been talking about concurrency (doing, or appearing to do, many things at the same time), and we've hinted at parallelism (the presence of multiple CPU cores which facilitates literally doing many things at the same time). These terms are sometimes used interchangeably, but they are distinct concepts. In this section, we'll try to precisely define these terms and the difference between them. I'll use simple pseudo-code to illustrate things.</p>
<p>Imagine a single task broken into a bunch of sub-tasks:</p>
<pre><code>task1 {
  subTask1-1()
  subTask1-2()
  ...
  subTask1-100()
}
</code></pre>
<p>Let's pretend to be a processor which executes such pseudocode. The obvious way to do so is to first do <code>subTask1-1</code> then do <code>subTask1-2</code> and so on until we've completed all sub-tasks. This is sequential execution.</p>
<p>Now consider multiple tasks. How might we execute them? We might start one task, do all the sub-tasks until the whole task is complete, then start on the next. The two tasks are being executed sequentially (and the sub-tasks within each task are also being executed sequentially). Looking at just the sub-tasks, you'd execute them like this:</p>
<pre><code>subTask1-1()
subTask1-2()
...
subTask1-100()
subTask2-1()
subTask2-2()
...
subTask2-100()

</code></pre>
<p>Alternatively, you could do <code>subTask1</code>, then put <code>task1</code> aside (remembering how far you got) and pick up the next task and do the first sub-task from that one, then go back to <code>task1</code> to do a sub-task. The two tasks would be interleaved, we call this concurrent execution of the two tasks. It might look like:</p>
<pre><code>subTask1-1()
subTask2-1()
subTask1-2()
subTask2-2()
...
subTask1-100()
subTask2-100()

</code></pre>
<p>Unless one task can observe the results or side-effects of a different task, then from the task's perspective, the sub-tasks are still being executed sequentially.</p>
<p>There's no reason we have to limit ourselves to two tasks, we could interleave any number and do so in any order.</p>
<p>Note that no matter how much concurrency we add, the whole job takes the same amount of time to complete (in fact it might take longer with more concurrency due to the overheads of context switching between them). However, for a given sub-task, we might get it finished earlier than in the purely sequential execution (for a user, this might feel more responsive).</p>
<p>Now, imagine it's not just you processing the tasks, you've got some processor friends to help you out. You can work on tasks at the same time and get the work done faster! This is <em>parallel</em> execution (which is also concurrent). You might execute the sub-tasks like:</p>
<pre><code>Processor 1           Processor 2
==============        ==============
subTask1-1()          subTask2-1()
subTask1-2()          subTask2-2()
...                   ...
subTask1-100()        subTask2-100()
</code></pre>
<p>If there are more than two processors, we can process even more tasks in parallel. We could also do some interleaving of tasks on each processor or sharing of tasks between processors.</p>
<p>In real code, things are a bit more complicated. Some sub-tasks (e.g., IO) don't require a processor to actively participate, they just need starting and some time later collecting the results. And some sub-tasks might require the results (or side-effects) of a sub-task from a different task in order to make progress (synchronization). Both these scenarios limit the effective ways that tasks can be concurrently executed and that, together with ensuring some concept of fairness, is why scheduling is important.</p>
<h3 id="enough-silly-examples-lets-try-to-define-things-properly"><a class="header" href="#enough-silly-examples-lets-try-to-define-things-properly">Enough silly examples, let's try to define things properly</a></h3>
<p>Concurrency is about ordering of computations and parallelism is about the mode of execution.</p>
<p>Given two computations, we say they are sequential (i.e., not concurrent) if we can observe that one happens before the other, or that they are concurrent if we cannot observe (or alternatively, it does not matter) that one happens before the other.</p>
<p>Two computations happen in parallel if they are literally happening at the same time. We can think of parallelism as a resource: the more parallelism is available, the more computations can happen in a fixed period of time (assuming that computation happens at the same speed). Increasing the concurrency of a system without increasing parallelism can never make it faster (although it can make the system more responsive and it may make it feasible to implement optimizations which would otherwise be impractical).</p>
<p>To restate, two computations may happen one after the other (neither concurrent nor parallel), their execution may be interleaved on a single CPU core (concurrent, but not parallel), or they may be executed at the same time on two cores (concurrent and parallel)<sup class="footnote-reference" id="fr-p-not-c-1"><a href="#footnote-p-not-c">11</a></sup>.</p>
<p>Another useful framing<sup class="footnote-reference" id="fr-turon-1"><a href="#footnote-turon">12</a></sup> is that concurrency is a way of organizing code and parallelism is a resource. This is a powerful statement! That concurrency is about organising code rather than executing code is important because from the perspective of the processor, concurrency without parallelism simply doesn't exist. It's particularly relevant for async concurrency because that is implemented entirely in user-side code - not only is it 'just' about organizing code, but you can easily prove that to yourself by just reading the source code. That parallelism is a resource is also useful because it reminds us that for parallelism and performance, only the number of processor cores is important, not how the code is organized with respect to concurrency (e.g., how many threads there are).</p>
<p>Both threaded and async systems can offer both concurrency and parallelism. In both cases, concurrency is controlled by code (spawning threads or tasks) and parallelism is controlled by the scheduler, which is part of the OS for threads (configured by the OS's API), and part of the runtime library for async (configured by choice of runtime, how the runtime is implemented, and options that the runtime provides to client code). There is however, a practical difference due to convention and common defaults. In threaded systems, each concurrent thread is executed in parallel using as much parallelism as possible. In async systems, there is no strong default: a system may run all tasks in a single thread, it may assign multiple tasks to a single thread and lock that thread to a core (so groups of tasks execute in parallel, but within a group each task executes concurrently, but never in parallel with other tasks within the group), or tasks may be run in parallel with or without limits. For the first part of this guide, we will use the Tokio runtime which primarily supports the last model. I.e., the behavior regarding parallelism is similar to concurrency with threads. Furthermore, we'll see features in async Rust which explicitly support concurrency but not parallelism, independent of the runtime.</p>
<h2 id="summary"><a class="header" href="#summary">Summary</a></h2>
<ul>
<li>There are many models of execution. We described sequential execution, threads and processes, and asynchronous programming.
<ul>
<li>Threads are an abstraction provided (and scheduled) by the OS. They usually involve pre-emptive multitasking, are parallel by default, and have fairly high overheads of management and context switching.</li>
<li>Asynchronous programming is managed by a user-space runtime. Multi-tasking is cooperative. It has lower overheads than threads, but feels a bit different to programming with threads since it uses different programming primitives (<code>async</code> and <code>await</code>, and futures, rather than first-class threads).</li>
</ul>
</li>
<li>Concurrency and parallelism are different but closely related concepts.
<ul>
<li>Concurrency is about ordering of computation (operations are concurrent if their order of execution cannot be observed).</li>
<li>Parallelism is about computing on multiple processors (operations are parallel if they are literally happening at the same time).</li>
</ul>
</li>
<li>Both OS threads and async programming provide concurrency and parallelism; async programming can also offer constructs for flexible or fine-grained concurrency which are not part of most operating systems' threads API.</li>
</ul>
<hr>
<ol class="footnote-definition"><li id="footnote-obs1">
<p>This isn't really true: modern compilers and CPUs will reorganize your code and run it any order they like. Sequential statements are likely to overlap in many different ways. However, this should never be <em>observable</em> to the program itself or its users. <a href="#fr-obs1-1">↩</a></p>
</li>
<li id="footnote-obs2">
<p>This isn't true either: even when one program is purely sequential, other programs might be running at the same time; more on this in the next section. <a href="#fr-obs2-1">↩</a></p>
</li>
<li id="footnote-io-def">
<p>IO is an acronym of input/output. It means any communication from the program to the world outside the program. That might be reading or writing to disk or the network, writing to the terminal, getting user input from a keyboard or mouse, or communicating with the OS or another program running in the system. IO is interesting in the context of concurrency because it takes several orders of magnitude longer to happen than nearly any task a program might do internally. That typically means lots of waiting, and that waiting time is an opportunity to do other work. <a href="#fr-io-def-1">↩</a></p>
</li>
<li id="footnote-io-complete">
<p>Exactly when IO is complete is actually rather complicated. From the program's perspective a single IO call is complete when control is returned from the OS. This usually indicates that data has been sent to some hardware or other program, but it doesn't necessarily mean that the data has actually been written to disk or displayed to the user, etc. That might require more work in the hardware or periodic flushing of caches, or for another program to read the data. Mostly we don't need to worry about this, but it's good to be aware of. <a href="#fr-io-complete-1">↩</a></p>
</li>
<li id="footnote-proc-program">
<p>from the user's perspective, a single program may include multiple processes, but from the OS's perspective each process is a separate program. <a href="#fr-proc-program-1">↩</a></p>
</li>
<li id="footnote-shmem">
<p>Some OSs do support sharing memory between processes, but using it requires special treatment and most memory is not shared. <a href="#fr-shmem-1">↩</a></p>
</li>
<li id="footnote-sched">
<p>Exactly how the OS chooses which thread to run and for how long (and on which core), is a key part of scheduling. There are many options, both high-level strategies and options to configure those strategies. Making good choices here is crucial for good performance, but it is complicated and we won't dig into it here. <a href="#fr-sched-1">↩</a></p>
</li>
<li id="footnote-busywait">
<p>There's another option which is that the thread can <em>busy wait</em> by just spinning in a loop until the IO is finished. This is not very efficient since other threads won't get to run and is uncommon in most modern systems. You may come across it in the implementations of locks or in very simple embedded systems. <a href="#fr-busywait-1">↩</a></p>
</li>
<li id="footnote-threads">
<p>We'll start our explanation assuming a program only has a single thread, but expand on that later. There will probably be other processes running on the system, but they don't really affect how async concurrency works. <a href="#fr-threads-1">↩</a></p>
</li>
<li id="footnote-other">
<p>There are some programming languages (or even libraries) which have concurrency which is managed within the program (without the OS), but with a pre-emptive scheduler rather than relying on cooperation between threads. Go is a well-known example. These systems don't require <code>async</code> and <code>await</code> notation, but have other downsides including making interop with other languages or the OS much more difficult, and having a heavyweight runtime. Very early versions of Rust had such a system, but no traces of it remained by 1.0. <a href="#fr-other-1">↩</a></p>
</li>
<li id="footnote-p-not-c">
<p>Can computation be parallel but not concurrent? Sort of but not really. Imagine two tasks (a and b) which consist of one sub-task each (1 and 2 belonging to a and b, respectively). By the use of synchronisation, we can't start sub-task 2 until sub-task 1 is complete and task a has to wait for sub-task 2 to complete until it is complete. Now a and b run on different processors. If we look at the tasks as black boxes, we can say they are running in parallel, but in a sense they are not concurrent because their ordering is fully determined. However, if we look at the sub-tasks we can see that they are neither parallel or concurrent. <a href="#fr-p-not-c-1">↩</a></p>
</li>
<li id="footnote-turon">
<p>Which I think is due to Aaron Turon and is reflected in some of the design of Rust's standard library, e.g., in the <a href="https://doc.rust-lang.org/std/thread/fn.available_parallelism.html">available_parallelism</a> function. <a href="#fr-turon-1">↩</a></p>
</li>
</ol><div style="break-before: page; page-break-before: always;"></div><h1 id="async-and-await"><a class="header" href="#async-and-await">Async and Await</a></h1>
<p>In this chapter we'll get started doing some async programming in Rust and we'll introduce the <code>async</code> and <code>await</code> keywords.</p>
<p><code>async</code> is an annotation on functions (and other items, such as traits, which we'll get to later); <code>await</code> is an operator used in expressions. But before we jump into those keywords, we need to cover a few core concepts of async programming in Rust, this follows from the discussion in the previous chapter, here we'll relate things directly to Rust programming.</p>
<h2 id="rust-async-concepts"><a class="header" href="#rust-async-concepts">Rust async concepts</a></h2>
<h3 id="the-runtime"><a class="header" href="#the-runtime">The runtime</a></h3>
<p>Async tasks must be managed and scheduled. There are typically more tasks than cores available so they can't all be run at once. When one stops executing another must be picked to execute. If a task is waiting on IO or some other event, it should not be scheduled, but when that completes, it should be scheduled. That requires interacting with the OS and managing IO work.</p>
<p>Many programming languages provide a runtime. Commonly, this runtime does a lot more than manage async tasks - it might manage memory (including garbage collection), have a role in exception handling, provide an abstraction layer over the OS, or even be a full virtual machine. Rust is a low-level language and strives towards minimal runtime overhead. The async runtime therefore has a much more limited scope than many other languages' runtimes. There are also many ways to design and implement an async runtime, so Rust lets you choose one depending on your requirements, rather than providing one. This does mean that getting started with async programming requires an extra step.</p>
<p>As well as running and scheduling tasks, a runtime must interact with the OS to manage async IO. It must also provide timer functionality to tasks (which intersects with IO management). There are no strong rules about how a runtime must be structured, but some terms and division of responsibilities are common:</p>
<ul>
<li><em>reactor</em> or <em>event loop</em> or <em>driver</em> (equivalent terms): dispatches IO and timer events, interacts with the OS, and does the lowest-level driving forward of execution,</li>
<li><em>scheduler</em>: determines when tasks can execute and on which OS threads,</li>
<li><em>executor</em> or <em>runtime</em>: combines the reactor and scheduler, and is the user-facing API for running async tasks; <em>runtime</em> is also used to mean the whole library of functionality (e.g., everything in the Tokio crate, not just the Tokio executor which is represented by the <a href="https://docs.rs/tokio/latest/tokio/runtime/struct.Runtime.html"><code>Runtime</code></a> type).</li>
</ul>
<p>As well as the executor as described above, a runtime crate typically includes many utility traits and functions. These might include traits (e.g., <code>AsyncRead</code>) and implementations for IO, functionality for common IO tasks such as networking or accessing the file system, locks, channels, and other synchronisation primitives, utilities for timing, utilities for working with the OS (e.g., signal handling), utility functions for working with futures and streams (async iterators), or monitoring and observation tools. We'll cover many of those in this guide.</p>
<p>There are many async runtimes to choose from. Some have very different scheduling policies, or are optimised for a specific task or domain. For most of this guide we'll use the <a href="https://tokio.rs/">Tokio</a> runtime. It's a general purpose runtime and is the most popular runtime in the ecosystem. It's a great choice for getting started and for production work. In some circumstances, you might get better performance or be able to write simpler code with a different runtime. Later in this guide we'll discuss some of the other available runtimes and why you might choose one or another, or even write your own.</p>
<p>To get up and running as quickly as possible, you need just a little boilerplate. You'll need to include the Tokio crate as a dependency in your Cargo.toml (just like any other crate):</p>
<pre><code>[dependencies]
tokio = { version = "1", features = ["full"] }
</code></pre>
<p>And you'll use the <code>tokio::main</code> annotation on your <code>main</code> function so that it can be an async function (which is otherwise not permitted in Rust):</p>
<pre><pre class="playground"><code class="language-rust norun">#[tokio::main]
async fn main() { ... }</code></pre></pre>
<p>That's it! You're ready to write some asynchronous code!</p>
<p>The <code>#[tokio::main]</code> annotation initializes the Tokio runtime and starts an async task for running the code in <code>main</code>. Later in this guide we'll explain in more detail what that annotation is doing and how to use async code without it (which will give you more flexibility).</p>
<h3 id="futures-rs-and-the-ecosystem"><a class="header" href="#futures-rs-and-the-ecosystem">Futures-rs and the ecosystem</a></h3>
<p>TODO context and history, what futures-rs is for - was used a lot, probably don't need it now, overlap with Tokio and other runtimes (sometimes with subtle semantic differences), why you might need it (working with futures directly, esp writing your own, streams, some utils)</p>
<p>Other ecosystem stuff - Yosh's crates, alt runtimes, experimental stuff, other?</p>
<h3 id="futures-and-tasks"><a class="header" href="#futures-and-tasks">Futures and tasks</a></h3>
<p>The basic unit of async concurrency in Rust is the <em>future</em>. A future is just a regular old Rust object (a struct or enum, usually) which implements the <a href="https://doc.rust-lang.org/std/future/trait.Future.html">'Future'</a> trait. A future represents a deferred computation. That is, a computation that will be ready at some point in the future.</p>
<p>We'll talk a lot about futures in this guide, but it's easiest to get started without worrying too much about them. We'll mention them quite a bit in the next few sections, but we won't really define them or use them directly until later. One important aspect of futures is that they can be combined to make new, 'bigger' futures (we'll talk a lot more about <em>how</em> they can be combined later).</p>
<p>I've used the term 'async task' quite a bit in an informal way in the previous chapter and this one. I've used the term to mean a logical sequence of execution; analogous to a thread but managed within a program rather than externally by the OS. It is often useful to think in terms of tasks, however, Rust itself has no concept of a task and the term is used to mean different things! It is confusing! To make it worse, runtimes do have a concept of a task and different runtimes have slightly different concepts of tasks.</p>
<p>From here on in, I'm going to try to be precise about the terminology around tasks. When I use just 'task' I mean the abstract concept of a sequence of computation that may occur concurrently with other tasks. I'll use 'async task' to mean exactly the same thing, but in contrast to a task which is implemented as an OS thread. I'll use 'runtime's task' to mean whatever kind of task a runtime imagines, and 'tokio task' (or some other specific runtime) to mean Tokio's idea of a task.</p>
<p>An async task in Rust is just a future (usually a 'big' future made by combining many others). In other words, a task is a future which is executed. However, there are times when a future is 'executed' without being a runtime's task. This kind of a future is intuitively a <em>task</em> but not a <em>runtime's task</em>. I'll spell this out more when we get to an example of it.</p>
<h2 id="async-functions"><a class="header" href="#async-functions">Async functions</a></h2>
<p>The <code>async</code> keyword is a modifier on function declarations. E.g., we can write <code>pub async fn send_to_server(...)</code>. An async function is simply a function declared using the <code>async</code> keyword, and what that means is that it is a function which can be executed asynchronously, in other words the caller <em>can choose not to</em> wait for the function to complete before doing something else.</p>
<p>In more mechanical terms, when an async function is called, the body is not executed as it would be for a regular function. Instead the function body and its arguments are packaged into a future which is returned in lieu of a real result. The caller can then decide what to do with that future (if the caller wants the result 'straight away', then it will <code>await</code> the future, see the next section).</p>
<p>Within an async function, code is executed in the usual, sequential way<sup class="footnote-reference" id="fr-preempt-1"><a href="#footnote-preempt">1</a></sup>, being async makes no difference. You can call synchronous functions from async functions, and execution proceeds as usual. One extra thing you can do within an async function is use <code>await</code> to await other async functions (or futures), which <em>may</em> cause yielding of control so that another task can execute.</p>
<h2 id="await"><a class="header" href="#await"><code>await</code></a></h2>
<p>We stated above that a future is a computation that will be ready at some point in the future. To get the result of that computation, we use the <code>await</code> keyword. If the result is ready immediately or can be computed without waiting, then <code>await</code> simply does that computation to produce the result. However, if the result is not ready, then <code>await</code> hands control over to the scheduler so that another task can proceed (this is cooperative multitasking mentioned in the previous chapter).</p>
<p>The syntax for using await is <code>some_future.await</code>, i.e., it is a postfix keyword used with the <code>.</code> operator. That means it can be used ergonomically in chains of method calls and field accesses.</p>
<p>Consider the following functions:</p>
<pre><pre class="playground"><code class="language-rust norun"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// An async function, but it doesn't need to wait for anything.
async fn add(a: u32, b: u32) -&gt; u32 {
  a + b
}

async fn wait_to_add(a: u32, b: u32) -&gt; u32 {
  sleep(1000).await;
  a + b
}
<span class="boring">}</span></code></pre></pre>
<p>If we call <code>add(15, 3).await</code> then it will return immediately with the result <code>18</code>. If we call <code>wait_to_add(15, 3).await</code>, we will eventually get the same answer, but while we wait another task will get an opportunity to run.</p>
<p>In this silly example, the call to <code>sleep</code> is a stand-in for doing some long-running task where we have to wait for the result. This is usually an IO operation where the result is data read from an external source or confirmation that writing to an external destination succeeded. Reading looks something like <code>let data = read(...).await?</code>. In this case <code>await</code> will cause the current task to wait while the read happens. The task will resume once reading is completed (other tasks could get some work done while the reading task waits). The result of reading could be data successfully read or an error (handled by the <code>?</code>).</p>
<p>Note that if we call <code>add</code> or <code>wait_to_add</code> or <code>read</code> without using <code>.await</code> we won't get any answer!</p>
<p>What?</p>
<p>Calling an async function returns a future, it doesn't immediately execute the code in the function. Furthermore, a future does not do any work until it is awaited<sup class="footnote-reference" id="fr-poll-1"><a href="#footnote-poll">2</a></sup>. This is in contrast to some other languages where an async function returns a future which begins executing immediately.</p>
<p>This is an important point about async programming in Rust. After a while it will be second nature, but it often trips up beginners, especially those who have experience with async programming in other languages.</p>
<p>An important intuition about futures in Rust is that they are inert objects. To get any work done they must be driven forward by an external force (usually an async runtime).</p>
<p>We've described <code>await</code> quite operationally (it runs a future, producing a result), but we talked in the previous chapter about async tasks and concurrency, how does <code>await</code> fit into that mental model? First, let's consider pure sequential code: logically, calling a function simply executes the code in the function (with some assignment of variables). In other words, the current task continues executing the next 'chunk' of code which is defined by the function. Similarly, in an async context, calling a non-async function simply continues execution with that function. Calling an async function finds the code to run, but doesn't run it. <code>await</code> is an operator which continues execution of the current task, or if the current task can't continue right now, gives another task an opportunity to continue.</p>
<p><code>await</code> can only be used inside an async context, for now that means inside an async function (we'll see more kinds of async contexts later). To understand why, remember that <code>await</code> might hand over control to the runtime so that another task can execute. There is only a runtime to hand control to in an async context. For now, you can imagine the runtime like a global variable which is only accessible in async functions, we'll explain later how it really works.</p>
<p>Finally, for one more perspective on <code>await</code>: we mentioned earlier that futures can be combined together to make 'bigger' futures. <code>async</code> functions are one way to define a future, and <code>await</code> is one way to combine futures. Using <code>await</code> on a future combines that future into the future produced by the async function it's used inside. We'll talk in more detail about this perspective and other ways to combine futures later.</p>
<h2 id="some-asyncawait-examples"><a class="header" href="#some-asyncawait-examples">Some async/await examples</a></h2>
<p>Let's start by revisiting our 'hello, world!' example:</p>
<pre><pre class="playground"><code class="language-rust edition2021">// Define an async function.
async fn say_hello() {
    println!("hello, world!");
}

#[tokio::main] // Boilerplate which lets us write `async fn main`, we'll explain it later.
async fn main() {
    // Call an async function and await its result.
    say_hello().await;
}</code></pre></pre>
<p>You should now recognise the boilerplate around <code>main</code>. It's for initializing the Tokio runtime and creating an initial task to run the async <code>main</code> function.</p>
<p><code>say_hello</code> is an async function, when we call it, we have to follow the call with <code>.await</code> to run it as part of the current task. Note that if you remove the <code>.await</code>, then running the program does nothing! Calling <code>say_hello</code> returns a future, but it is never executed so <code>println</code> is never called (the compiler will warn you, at least).</p>
<p>Here's a slightly more realistic example, taken from the <a href="https://tokio.rs/tokio/tutorial/hello-tokio">Tokio tutorial</a>.</p>
<pre><pre class="playground"><code class="language-rust norun">#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    // Open a connection to the mini-redis address.
    let mut client = client::connect("127.0.0.1:6379").await?;

    // Set the key "hello" with value "world"
    client.set("hello", "world".into()).await?;

    // Get key "hello"
    let result = client.get("hello").await?;

    println!("got value from the server; result={:?}", result);

    Ok(())
}</code></pre></pre>
<p>The code is a bit more interesting, but we're essentially doing the same thing - calling async functions and then awaiting to execute the result. This time we're using <code>?</code> for error handling - it works just like in synchronous Rust.</p>
<p>For all the talk so far about concurrency, parallelism, and asynchrony, both these examples are 100% sequential. Just calling and awaiting async functions does not introduce any concurrency unless there are other tasks to schedule while the awaiting task is waiting. To prove this to ourselves, lets look at another simple (but contrived) example:</p>
<pre><pre class="playground"><code class="language-rust edition2021">use std::io::{stdout, Write};
use tokio::time::{sleep, Duration};

async fn say_hello() {
    print!("hello, ");
    // Flush stdout so we see the effect of the above `print` immediately.
    stdout().flush().unwrap();
}

async fn say_world() {
    println!("world!");
}

#[tokio::main]
async fn main() {
    say_hello().await;
    // An async sleep function, puts the current task to sleep for 1s.
    sleep(Duration::from_millis(1000)).await;
    say_world().await;
}</code></pre></pre>
<p>Between printing "hello" and "world", we put the current task to sleep<sup class="footnote-reference" id="fr-async-sleep-1"><a href="#footnote-async-sleep">3</a></sup> for one second. Observe what happens when we run the program: it prints "hello", does nothing for one second, then prints "world". That is because executing a single task is purely sequential. If we had some concurrency, then that one second nap would be an excellent opportunity to get some other work done, like printing "world". We'll see how to do that in the next section.</p>
<h2 id="spawning-tasks"><a class="header" href="#spawning-tasks">Spawning tasks</a></h2>
<p>We've talked about async and await as a way to run code in an async task. And we've said that <code>await</code> can put the current task to sleep while it waits for IO or some other event. When that happens, another task can run, but how do those other tasks come about? Just like we use <code>std::thread::spawn</code> to spawn a new task, we can use <a href="https://docs.rs/tokio/latest/tokio/task/fn.spawn.html"><code>tokio::spawn</code></a> to spawn a new async task. Note that <code>spawn</code> is a function of Tokio, the runtime, not from Rust's standard library, because tasks are purely a runtime concept.</p>
<p>Here's a tiny example of running an async function on a separate task by using <code>spawn</code>:</p>
<pre><pre class="playground"><code class="language-rust edition2021">use tokio::{spawn, time::{sleep, Duration}};

async fn say_hello() {
    // Wait for a while before printing to make it a more interesting race.
    sleep(Duration::from_millis(100)).await;
    println!("hello");
}

async fn say_world() {
    sleep(Duration::from_millis(100)).await;
    println!("world!");
}

#[tokio::main]
async fn main() {
    spawn(say_hello());
    spawn(say_world());
    // Wait for a while to give the tasks time to run.
    sleep(Duration::from_millis(1000)).await;
}</code></pre></pre>
<p>Similar to the last example, we have two functions printing "hello" and "world!". But this time we run them concurrently (and in parallel) rather than sequentially. If you run the program a few times you should see the strings printing in both orders - sometimes "hello" first, sometimes "world!" first. A classic concurrent race!</p>
<p>Let's dive into what is happening here. There are three concepts in play: futures, tasks, and threads. The <code>spawn</code> function takes a future (which remember can be made up of many smaller futures) and runs it as a new Tokio task. Tasks are the concept which the Tokio runtime schedules and manages (not individual futures). Tokio (in its default configuration) is a multi-threaded runtime which means that when we spawn a new task, that task may be run on a different OS thread from the task it was spawned from (it may be run on the same thread, or it may start on one thread and then be moved to another later on).</p>
<p>So, when a future is spawned as a task it runs <em>concurrently</em> with the task it was spawned from and any other tasks. It may also run in parallel to those tasks if it is scheduled on a different thread.</p>
<p>To summarise, when we write two statements following each other in Rust, they are executed sequentially (whether in async code or not). When we write <code>await</code>, that does not change the concurrency of sequential statements. E.g., <code>foo(); bar();</code> is strictly sequential - <code>foo</code> is called and afterwards, <code>bar</code> is called. That is true whether <code>foo</code> and <code>bar</code> are async functions or not. <code>foo().await; bar().await;</code> is also strictly sequential, <code>foo</code> is fully evaluated and then <code>bar</code> is fully evaluated. In both cases another thread might be interleaved with the sequential execution and in the second case, another async task might be interleaved at the await points, but the two statements are executed sequentially <em>with respect to each other</em> in both cases.</p>
<p>If we use either <code>thread::spawn</code> or <code>tokio::spawn</code> we introduce concurrency and potentially parallelism, in the first case between threads and in the second between tasks.</p>
<p>Later in the guide we'll see cases where we execute futures concurrently, but never in parallel.</p>
<h3 id="joining-tasks"><a class="header" href="#joining-tasks">Joining tasks</a></h3>
<p>If we want to get the result of executing a spawned task, then the spawning task can wait for it to finish and use the result, this is called <em>joining</em> the tasks (analogous to <a href="https://doc.rust-lang.org/std/thread/struct.JoinHandle.html#method.join">joining</a> threads, and the APIs for joining are similar).</p>
<p>When a task is spawned, the spawn function returns a <a href="https://docs.rs/tokio/latest/tokio/task/struct.JoinHandle.html"><code>JoinHandle</code></a>. If you just want the task to do it's own thing executing, the <code>JoinHandle</code> can be discarded (dropping the <code>JoinHandle</code> does not affect the spawned task). But if you want the spawning task to wait for the spawned task to complete and then use the result, you can <code>await</code> the <code>JoinHandle</code> to do so.</p>
<p>For example, let's revisit our 'Hello, world!' example one more time:</p>
<pre><pre class="playground"><code class="language-rust edition2021">use tokio::{spawn, time::{sleep, Duration}};

async fn say_hello() {
    // Wait for a while before printing to make it a more interesting race.
    sleep(Duration::from_millis(100)).await;
    println!("hello");
}

async fn say_world() {
    sleep(Duration::from_millis(100)).await;
    println!("world");
}

#[tokio::main]
async fn main() {
    let handle1 = spawn(say_hello());
    let handle2 = spawn(say_world());
    
    let _ = handle1.await;
    let _ = handle2.await;

    println!("!");
}</code></pre></pre>
<p>The code is similar to last time, but instead of just calling <code>spawn</code>, we save the returned <code>JoinHandle</code>s and later <code>await</code> them. Since we're waiting for those tasks to complete before we exit the <code>main</code> function, we no longer need the <code>sleep</code> in <code>main</code>.</p>
<p>The two spawned tasks are still executing concurrently. If you run the program a few times you should see both orderings. However, the <code>await</code>ed join handles are a limit on the concurrency: the final exclamation mark ('!') will <em>always</em> be printed last (you could experiment with moving <code>println!("!");</code> relative to the <code>await</code>s. You'll probably need to change with the sleep times too to get observable effects).</p>
<p>If we immediately <code>await</code>ed the <code>JoinHandle</code> of the first <code>spawn</code> rather than saved it and later <code>await</code>ed (i.e., written <code>spawn(say_hello()).await;</code>), then we'd have spawned another task to run the 'hello' future, but the spawning task would have waited for it to finish before doing anything else. In other words, there is no possible concurrency! You almost never want to do this (because why bother with the spawn? Just write the sequential code).</p>
<h3 id="joinhandle"><a class="header" href="#joinhandle"><code>JoinHandle</code></a></h3>
<p>We'll quickly look at <code>JoinHandle</code> in a little more depth. The fact that we can <code>await</code> a <code>JoinHandle</code> is a clue that a <code>JoinHandle</code> is itself a future. <code>spawn</code> is not an <code>async</code> function, it's a regular function that returns a future (<code>JoinHandle</code>). It does some work (to schedule the task) before returning the future (unlike an async future), which is why we don't <em>need</em> to <code>await</code> <code>spawn</code>. Awaiting a <code>JoinHandle</code> waits for the spawned task to complete and then returns the result. In the above example, there was no result, we just waited for the task to complete. <code>JoinHandle</code> is a generic type and it's type parameter is the type returned by the spawned task. In the above example, the type would be <code>JoinHandle&lt;()&gt;</code>, a future that results in a <code>String</code> would produce a <code>JoinHandle</code> with type <code>JoinHandle&lt;String&gt;</code>.</p>
<p><code>await</code>ing a <code>JoinHandle</code> returns a <code>Result</code> (which is why we used <code>let _ = ...</code> in the above example, it avoids a warning about an unused <code>Result</code>). If the spawned task completed successfully, then the task's result will be in the <code>Ok</code> variant. If the task panicked or was aborted (a form of <a href="part-guide/../part-reference/cancellation.html">cancellation</a>), then the result will be an <code>Err</code> containing a <a href="https://docs.rs/tokio/latest/tokio/task/struct.JoinError.html"><code>JoinError</code> docs</a>. If you are not using cancellation via <code>abort</code> in your project, then <code>unwrapping</code> the result of <code>JoinHandle.await</code> is a reasonable approach, since that is effectively propagating a panic from the spawned task to the spawning task.</p>
<hr>
<ol class="footnote-definition"><li id="footnote-preempt">
<p>like any other thread, the thread the async function is running on may be pre-empted by the operating system and paused so another thread can get some work done. However, from the function's point of view this is not observable without inspecting data which may have been modified by other threads (and which could have been modified by another thread executing in parallel without the current thread being paused). <a href="#fr-preempt-1">↩</a></p>
</li>
<li id="footnote-poll">
<p>Or polled, which is a lower-level operation than <code>await</code> and happens behind the scenes when using <code>await</code>. We'll talk about polling later when we talk about futures in detail. <a href="#fr-poll-1">↩</a></p>
</li>
<li id="footnote-async-sleep">
<p>Note that we're using an async sleep function here, if we were to use <a href="https://doc.rust-lang.org/std/thread/fn.sleep.html"><code>sleep</code></a> from std we'd put the whole thread to sleep. That wouldn't make any difference in this toy example but in a real program it would mean other tasks could not be scheduled on that thread during that time. That is very bad. <a href="#fr-async-sleep-1">↩</a></p>
</li>
</ol><div style="break-before: page; page-break-before: always;"></div><h1 id="more-asyncawait-topics"><a class="header" href="#more-asyncawait-topics">More async/await topics</a></h1>
<h2 id="unit-tests"><a class="header" href="#unit-tests">Unit tests</a></h2>
<p>How to unit test async code? The issue is that you can only await from inside an async context, and unit tests in Rust are not async. Luckily, most runtimes provide a convenience attribute for tests similar to the one for <code>async main</code>. Using Tokio, it looks like this:</p>
<pre><pre class="playground"><code class="language-rust norun"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_something() {
  // Write a test here, including all the `await`s you like.
}
<span class="boring">}</span></code></pre></pre>
<p>There are many ways to configure the test, see the <a href="https://docs.rs/tokio/latest/tokio/attr.test.html">docs</a> for details.</p>
<p>There are some more advanced topics in testing async code (e.g., testing for race conditions, deadlock, etc.), and we'll cover some of those <a href="part-guide/">later</a> in this guide.</p>
<h2 id="blocking-and-cancellation"><a class="header" href="#blocking-and-cancellation">Blocking and cancellation</a></h2>
<p>Blocking and cancellation are important to keep in mind when programming with async Rust. These concepts are not localised to any particular feature or function, but are ubiquitous properties of the system which you must understand to write correct code.</p>
<h3 id="blocking-io"><a class="header" href="#blocking-io">Blocking IO</a></h3>
<p>We say a thread (note we're talking about OS threads here, not async tasks) is blocked when it can't make any progress. That's usually because it is waiting for the OS to complete a task on its behalf (usually I/O). Importantly, while a thread is blocked, the OS knows not to schedule it so that other threads can make progress. This is fine in a multithreaded program because it lets other threads make progress while the blocked thread is waiting. However, in an async program, there are other tasks which should be scheduled on the same OS thread, but the OS doesn't know about those and keeps the whole thread waiting. This means that rather than the single task waiting for its I/O to complete (which is fine), many tasks have to wait (which is not fine).</p>
<p>We'll talk soon about non-blocking/async I/O. For now, just know that non-blocking I/O is I/O which the async runtime knows about and so will only the current task will wait, the thread will not be blocked. It is very important to only use non-blocking I/O from an async task, never blocking I/O (which is the only kind provided in Rust's standard library).</p>
<h3 id="blocking-computation"><a class="header" href="#blocking-computation">Blocking computation</a></h3>
<p>You can also block the thread by doing computation (this is not quite the same as blocking I/O, since the OS is not involved, but the effect is similar). If you have a long-running computation (with or without blocking I/O) without yielding control to the runtime, then that task will never give the runtime's scheduler a chance to schedule other tasks. Remember that async programming uses cooperative multitasking. Here a task is not cooperating, so other tasks won't get a chance to get work done. We'll discuss ways to mitigate this later.</p>
<p>There are many other ways to block a whole thread, and we'll come back to blocking several times in this guide.</p>
<h3 id="cancellation"><a class="header" href="#cancellation">Cancellation</a></h3>
<p>Cancellation means stopping a future (or task) from executing. Since in Rust (and in contrast to many other async/await systems), futures must be driven forward by an external force (like the async runtime), if a future is no longer driven forward then it will not execute any more. If a future is dropped (remember, a future is just a plain old Rust object), then it can never make any more progress and is canceled.</p>
<p>Cancellation can be initiated in a few ways:</p>
<ul>
<li>By simply dropping a future (if you own it).</li>
<li>Calling <a href="https://docs.rs/tokio/latest/tokio/task/struct.JoinHandle.html#method.abort"><code>abort</code></a> on a task's 'JoinHandle' (or an <code>AbortHandle</code>).</li>
<li>Via a <a href="https://docs.rs/tokio-util/latest/tokio_util/sync/struct.CancellationToken.html"><code>CancellationToken</code></a> (which requires the future being canceled to notice the token and cooperatively cancel itself).</li>
<li>Implicitly, by a function or macro like <a href="https://docs.rs/tokio/latest/tokio/macro.select.html"><code>select</code></a>.</li>
</ul>
<p>The middle two are specific to Tokio, though most runtimes provide similar facilities. Using a <code>CancellationToken</code> requires cooperation of the future being canceled, but the others do not. In these other cases, the canceled future will get no notification of cancellation and no opportunity to clean up (besides its destructor). Note that even if a future has a cancellation token, it can still be canceled via the other methods which won't trigger the cancellation token.</p>
<p>From the perspective of writing async code (in async functions, blocks, futures, etc.), the code might stop executing at any <code>await</code> (including hidden ones in macros) and never start again. In order for your code to be correct (specifically to be <em>cancellation safe</em>), it must work correctly whether it completes normally or whether it terminates at any await point<sup class="footnote-reference" id="fr-cfThreads-1"><a href="#footnote-cfThreads">1</a></sup>.</p>
<pre><pre class="playground"><code class="language-rust norun"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn some_function(input: Option&lt;Input&gt;) {
    let Some(input) = input else {
        return;           // Might terminate here (`return`).
    };

    let x = foo(input)?;  // Might terminate here (`?`).

    let y = bar(x).await; // Might terminate here (`await`).

    // ...

    //                       Might terminate here (implicit return).
}
<span class="boring">}</span></code></pre></pre>
<p>An example of how this can go wrong is if an async function reads data into an internal buffer, then awaits the next datum. If reading the data is destructive (i.e., cannot be re-read from the original source) and the async function is canceled, then the internal buffer will be dropped, and the data in it will be lost. It is important to consider how a future and any data it touches will be impacted by canceling the future, restarting the future, or starting a new future which touches the same data.</p>
<p>We'll be coming back to cancellation and cancellation safety a few times in this guide, and there is a whole <a href="part-guide/">chapter</a> on the topic in the reference section.</p>
<h2 id="async-blocks"><a class="header" href="#async-blocks">Async blocks</a></h2>
<p>A regular block (<code>{ ... }</code>) groups code together in the source and creates a scope of encapsulation for names. At runtime, the block is executed in order and evaluates to the value of its last expression (or the unit type (<code>()</code>) if there is no trailing expression).</p>
<p>Similarly to async functions, an async block is a deferred version of a regular block. An async block scopes code and names together, but at runtime it is not immediately executed and evaluates to a future. To execute the block and obtain the result, it must be <code>await</code>ed. E.g.:</p>
<pre><pre class="playground"><code class="language-rust norun"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let s1 = {
    let a = 42;
    format!("The answer is {a}")
};

let s2 = async {
    let q = question().await;
    format!("The question is {q}")
};
<span class="boring">}</span></code></pre></pre>
<p>If we were to execute this snippet, <code>s1</code> would be a string which could be printed, but <code>s2</code> would be a future; <code>question()</code> would not have been called. To print <code>s2</code>, we first have to <code>s2.await</code>.</p>
<p>An async block is the simplest way to start an async context and create a future. It is commonly used to create small futures which are only used in one place.</p>
<p>Unfortunately, control flow with async blocks is a little quirky. Because an async block creates a future rather than straightforwardly executing, it behaves more like a function than a regular block with respect to control flow. <code>break</code> and <code>continue</code> cannot go 'through' an async block like they can with regular blocks; instead you have to use <code>return</code>:</p>
<pre><pre class="playground"><code class="language-rust norun"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>loop {
    {
        if ... {
            // ok
            continue;
        }
    }

    async {
        if ... {
            // not ok
            // continue;

            // ok - continues with the next execution of the `loop`, though note that if there was
            // code in the loop after the async block that would be executed.
            return;
        }
    }.await
}
<span class="boring">}</span></code></pre></pre>
<p>To implement <code>break</code> you would need to test the value of the block (a common idiom is to use <a href="https://doc.rust-lang.org/std/ops/enum.ControlFlow.html"><code>ControlFlow</code></a> for the value of the block, which also allows use of <code>?</code>).</p>
<p>Likewise, <code>?</code> inside an async block will terminate execution of the future in the presence of an error, causing the <code>await</code>ed block to take the value of the error, but won't exit the surrounding function (like <code>?</code> in a regular block would). You'll need another <code>?</code> after <code>await</code> for that:</p>
<pre><pre class="playground"><code class="language-rust norun"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async {
    let x = foo()?;   // This `?` only exits the async block, not the surrounding function.
    consume(x);
    Ok(())
}.await?
<span class="boring">}</span></code></pre></pre>
<p>Annoyingly, this often confuses the compiler since (unlike functions) the 'return' type of an async block is not explicitly stated. You'll probably need to add some type annotations on variables or use turbofished types to make this work, e.g., <code>Ok::&lt;_, MyError&gt;(())</code> instead of <code>Ok(())</code> in the above example.</p>
<p>A function which returns an async block is pretty similar to an async function. Writing <code>async fn foo() -&gt; ... { ... }</code> is roughly equivalent to <code>fn foo() -&gt; ... { async { ... } }</code>. In fact, from the caller's perspective they are equivalent, and changing from one form to the other is not a breaking change. Furthermore, you can override one with the other when implementing an async trait (see below). However, you do have to adjust the type, making the <code>Future</code> explicit in the async block version: <code>async fn foo() -&gt; Foo</code> becomes <code>fn foo() -&gt; impl Future&lt;Output = Foo&gt;</code> (you might also need to make other bounds explicit, e.g., <code>Send</code> and <code>'static</code>).</p>
<p>You would usually prefer the async function version since it is simpler and clearer. However, the async block version is more flexible since you can execute some code when the function is called (by writing it outside the async block) and some code when the result is awaited (the code inside the async block).</p>
<h2 id="async-closures"><a class="header" href="#async-closures">Async closures</a></h2>
<ul>
<li>closures
<ul>
<li>coming soon (https://github.com/rust-lang/rust/pull/132706, https://blog.rust-lang.org/inside-rust/2024/08/09/async-closures-call-for-testing.html)</li>
<li>async blocks in closures vs async closures</li>
</ul>
</li>
</ul>
<h2 id="lifetimes-and-borrowing"><a class="header" href="#lifetimes-and-borrowing">Lifetimes and borrowing</a></h2>
<ul>
<li>Mentioned the static lifetime above</li>
<li>Lifetime bounds on futures (<code>Future + '_</code>, etc.)</li>
<li>Borrowing across await points</li>
<li>I don't know, I'm sure there are more lifetime issues with async functions ...</li>
</ul>
<h2 id="send--static-bounds-on-futures"><a class="header" href="#send--static-bounds-on-futures"><code>Send + 'static</code> bounds on futures</a></h2>
<ul>
<li>Why they're there, multi-threaded runtimes</li>
<li>spawn local to avoid them</li>
<li>What makes an async fn <code>Send + 'static</code> and how to fix bugs with it</li>
</ul>
<h2 id="async-traits"><a class="header" href="#async-traits">Async traits</a></h2>
<ul>
<li>syntax
<ul>
<li>The <code>Send + 'static</code> issue and working around it
<ul>
<li>trait_variant</li>
<li>explicit future</li>
<li>return type notation (https://blog.rust-lang.org/inside-rust/2024/09/26/rtn-call-for-testing.html)</li>
</ul>
</li>
</ul>
</li>
<li>overriding
<ul>
<li>future vs async notation for methods</li>
</ul>
</li>
<li>object safety</li>
<li>capture rules (https://blog.rust-lang.org/2024/09/05/impl-trait-capture-rules.html)</li>
<li>history and async-trait crate</li>
</ul>
<h2 id="recursion"><a class="header" href="#recursion">Recursion</a></h2>
<ul>
<li>Allowed (relatively new), but requires some explicit boxing
<ul>
<li>forward reference to futures, pinning</li>
<li>https://rust-lang.github.io/async-book/07_workarounds/04_recursion.html</li>
<li>https://blog.rust-lang.org/2024/03/21/Rust-1.77.0.html#support-for-recursion-in-async-fn</li>
<li>async-recursion macro (https://docs.rs/async-recursion/latest/async_recursion/)</li>
</ul>
</li>
</ul>
<hr>
<ol class="footnote-definition"><li id="footnote-cfThreads">
<p>It is interesting to compare cancellation in async programming with canceling threads. Canceling a thread is possible (e.g., using <code>pthread_cancel</code> in C, there is no direct way to do this in Rust), but it is almost always a very, very bad idea since the thread being canceled can terminate anywhere. In contrast, canceling an async task can only happen at an await point. As a consequence, it is very rare to cancel an OS thread without terminating the whole process and so as a programmer, you generally don't worry about this happening. In async Rust however, cancellation is definitely something which <em>can</em> happen. We'll be discussing how to deal with that as we go along. <a href="#fr-cfThreads-1">↩</a></p>
</li>
</ol><div style="break-before: page; page-break-before: always;"></div><h1 id="io-and-issues-with-blocking"><a class="header" href="#io-and-issues-with-blocking">IO and issues with blocking</a></h1>
<h2 id="blocking-and-non-blocking-io"><a class="header" href="#blocking-and-non-blocking-io">Blocking and non-blocking IO</a></h2>
<ul>
<li>High level view</li>
<li>How async IO fits with async concurrency</li>
<li>Why blocking IO is bad</li>
<li>forward ref to streams for streams/sinks</li>
</ul>
<h2 id="read-and-write"><a class="header" href="#read-and-write">Read and Write</a></h2>
<ul>
<li>async Read and Write traits
<ul>
<li>part of the runtime</li>
</ul>
</li>
<li>how to use</li>
<li>specific implementations
<ul>
<li>network vs disk
<ul>
<li>tcp, udp</li>
<li>file system is not really async, but io_uring (ref to that chapter)</li>
</ul>
</li>
<li>practical examples</li>
<li>stdout, etc.</li>
<li>pipe, fd, etc.</li>
</ul>
</li>
</ul>
<h2 id="memory-management"><a class="header" href="#memory-management">Memory management</a></h2>
<ul>
<li>Issues with buffer management and async IO</li>
<li>Different solutions and pros and cons
<ul>
<li>zero-copy approach</li>
<li>shared buffer approach</li>
</ul>
</li>
<li>Utility crates to help with this, Bytes, etc.</li>
</ul>
<h2 id="advanced-topics-on-io"><a class="header" href="#advanced-topics-on-io">Advanced topics on IO</a></h2>
<ul>
<li>buf read/write</li>
<li>Read + Write, split, join</li>
<li>copy</li>
<li>simplex and duplex</li>
<li>cancelation</li>
</ul>
<h2 id="the-os-view-of-io"><a class="header" href="#the-os-view-of-io">The OS view of IO</a></h2>
<ul>
<li>Different kinds of IO and mechanisms, completion IO, reference to completion IO chapter in adv section
<ul>
<li>different runtimes can faciliate this</li>
<li>mio for low-level interface</li>
</ul>
</li>
</ul>
<h2 id="other-blocking-operations"><a class="header" href="#other-blocking-operations">Other blocking operations</a></h2>
<ul>
<li>Why this is bad</li>
<li>Long running CPU work
<ul>
<li>Using Tokio for just CPU work: https://thenewstack.io/using-rustlangs-async-tokio-runtime-for-cpu-bound-tasks/</li>
</ul>
</li>
<li>Solutions
<ul>
<li>spawn blocking</li>
<li>thread pool</li>
<li>etc.</li>
</ul>
</li>
<li>yielding to the runtime
<ul>
<li>not the same as Rust's yield keyword</li>
<li>await doesn't yield</li>
<li>implicit yields in Tokio</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="composing-futures-concurrently"><a class="header" href="#composing-futures-concurrently">Composing futures concurrently</a></h1>
<p>In this chapter we're going to cover more ways in which futures can be composed. In particular, some new ways in which futures can be executed concurrently (but not in parallel). Superficially, the new functions/macros we introduce in this chapter are pretty simple. However, the underlying concepts can be pretty subtle. We'll start with a recap on futures, concurrency, and parallelism, but you might also want to revisit the earlier section comparing <a href="part-guide/concurrency.html#concurrency-and-parallelism">concurrency with parallelism</a>.</p>
<p>A futures is a deferred computation. A future can be progressed by using <code>await</code>, which hands over control to the runtime, causing the current task to wait for the result of the computation. If <code>a</code> and <code>b</code> are futures, then they can be sequentially composed (that is, combined to make a future which executes <code>a</code> to completion and then <code>b</code> to completion) by <code>await</code>ing one then the other: <code>async { a.await; b.await}</code>.</p>
<p>We have also seen parallel composition of futures using <code>spawn</code>: <code>async { let a = spawn(a); let b = spawn(b); (a.await, b.await)}</code> runs the two futures in parallel. Note that the <code>await</code>s in the tuple are not awaiting the futures themselves, but are awaiting <code>JoinHandle</code>s to get the results of the futures when they complete.</p>
<p>In this chapter we introduce two ways to compose futures concurrently without parallelism: <code>join</code> and <code>select</code>/<code>race</code>. In both cases, the futures run concurrently by time-slicing; each of the composed futures takes turns to execute a little bit before the next gets a turn. This is done <em>without involving the async runtime</em> (and therefore without multiple OS threads and without any potential for parallelism). The composing construct interleaves the futures locally. You can think of these constructs being like mini-executors which execute their component futures within a single async task.</p>
<p>The fundamental difference between join and select/race is how they handle futures completing their work: a join finishes when all futures finish, a select/race finishes when one future finishes (all the others are cancelled). There are also variations of both for handling errors.</p>
<p>These constructs (or similar concepts) are often used with streams, we'll touch on this below, but we'll talk more about that in the <a href="part-guide/streams.html">streams chapter</a>.</p>
<p>If you want parallelism (or you don't explicitly not want parallelism), spawning tasks is often a simpler alternative to these composition constructs. Spawning tasks is usually less error-prone, more general, and performance is more predictable. On the other hand, spawning is inherently less <a href="part-guide/../part-reference/structured.html">structured</a>, which can make lifecycle and resource management harder to reason about.</p>
<p>It's worth considering the performance issue in a little more depth. The potential performance problem with concurrent composition is the fairness of time sharing. If you have 100 tasks in your program, then typically the optimal way to share resources is for each task to get 1% of the processor time (or if the tasks are all waiting, then for each to have the same chance of being woken up). If you spawn 100 tasks, then this is usually what happens (roughly). However, if you spawn two tasks and join 99 futures on one of those tasks, then the scheduler will only know about two tasks and one task will get 50% of the time and the 99 futures will each get 0.5%.</p>
<p>Usually the distribution of tasks is not so biased, and very often we use join/select/etc. for things like timeouts where this behaviour is actually desirable. But it is worth considering to ensure that your program has the performance characteristics you want.</p>
<h2 id="join"><a class="header" href="#join">Join</a></h2>
<p>Tokio's <a href="https://docs.rs/tokio/latest/tokio/macro.join.html"><code>join</code> macro</a> takes a list of futures and runs them all to completion concurrently (returning all the results as a tuple). It returns when all the futures have completed. The futures are always executed on the same thread (concurrently and not in parallel).</p>
<p>Here's a simple example:</p>
<pre><pre class="playground"><code class="language-rust norun">async fn main() {
  let (result_1, result_2) = join!(do_a_thing(), do_a_thing());
  // Use `result_1` and `result_2`.
}</code></pre></pre>
<p>Here, the two executions of <code>do_a_thing</code> happen concurrently, and the results are ready when they are both done. Notice that we don't <code>await</code> to get the results. <code>join!</code> implicitly awaits it's futures and produces a value. It does not create a future. You do still need to use it within an async context (e.g., from within an async function).</p>
<p>Although you can't see it in the example above, <code>join!</code> takes expressions which evaluate to futures<sup class="footnote-reference" id="fr-into-1"><a href="#footnote-into">1</a></sup>. <code>join</code> does not create an async context in it's body and you shouldn't <code>await</code> the futures passed to <code>join</code> (otherwise they'll be evaluated before the joined futures).</p>
<p>Because all the futures are executed on the same thread, if any future blocks the thread, then none of them can make progress. If using a mutex or other lock, this can easily lead to deadlock if one future is waiting for a lock held by another future.</p>
<p>[<code>join</code>] does not care about the result of the futures. In particular, if a future is cancelled or returns an error, it does not affect the others - they continue to execute. If you want 'fail fast' behaviour, use <a href="https://docs.rs/tokio/latest/tokio/macro.try_join.html"><code>try_join</code></a>. <code>try_join</code> works similarly to <code>join</code>, however, if any future returns an <code>Err</code>, then all the other futures are cancelled and <code>try_join</code> returns the error immediately.</p>
<p>Back in the earlier chapter on <a href="part-guide/async-await.html">async/await</a>, we used the word 'join' to talk about joining spawned tasks. As the name suggests, joining futures and tasks is related: joining means we execute multiple futures concurrently and wait for the result before continuing. The syntax is different: using a <code>JoinHandle</code> vs the <code>join</code> macro, but the idea is similar. The key difference is that when joining tasks, the tasks execute concurrently and in parallel, whereas using <code>join!</code>, the futures execute concurrently but not in parallel. Furthermore, spawned tasks are scheduled on the runtime's scheduler, whereas with <code>join!</code> the futures are 'scheduled' locally (on the same task and within the temporal scope of the macro's execution). Another difference is that if a spawned task panics, the panic is caught by the runtime, but if a future in <code>join</code> panics, then the whole task panics.</p>
<h3 id="alternatives"><a class="header" href="#alternatives">Alternatives</a></h3>
<p>Running futures concurrently and collecting their results is a common requirement. You should probably use <code>spawn</code> and <code>JoinHandle</code>s unless you have a good reason not to (i.e., you explicitly do not want parallelism, and even then you might prefer to use <a href="https://docs.rs/tokio/latest/tokio/task/fn.spawn_local.html"><code>spawn_local</code></a>). The <a href="https://docs.rs/tokio/latest/tokio/task/struct.JoinSet.html"><code>JoinSet</code></a> abstraction manages such spawned tasks in a way similar to <code>join!</code>.</p>
<p>Most runtimes (and <a href="https://docs.rs/futures/latest/futures/macro.join.html">futures.rs</a>) have an equivalent to Tokio's <code>join</code> macro and they mostly behave the same way. There are also <code>join</code> functions, which are similar to the macro but a little less flexible. E.g., futures.rs has <a href="https://docs.rs/futures/latest/futures/future/fn.join.html"><code>join</code></a> for joining two futures, <a href="https://docs.rs/futures/latest/futures/future/fn.join3.html"><code>join3</code></a>, <a href="https://docs.rs/futures/latest/futures/future/fn.join4.html"><code>join4</code></a>, and <a href="https://docs.rs/futures/latest/futures/future/fn.join5.html"><code>join5</code></a> for joining the obvious number of futures, and <a href="https://docs.rs/futures/latest/futures/future/fn.join_all.html">join_all</a> for joining a collection of futures (as well as <code>try_</code> variations of each of these).</p>
<p><a href="https://docs.rs/futures-concurrency/latest">Futures-concurrency</a> also provides functionality for join (and try_join). In the futures-concurrency style, these operations are trait methods on groups of futures such as tuples, <code>Vec</code>s, or arrays. E.g., to join two futures, you would write <code>(fut1, fut2).join().await</code> (note that <code>await</code> is explicit here).</p>
<p>If the set of futures you wish to join together varies dynamically (e.g., new futures are created as input comes in over the network), or you want the results as they complete rather than when all the futures have completed, then you'll need to use streams and the <a href="https://docs.rs/futures/latest/futures/stream/struct.FuturesUnordered.html"><code>FuturesUnordered</code></a> or <a href="https://docs.rs/futures/latest/futures/stream/struct.FuturesOrdered.html"><code>FuturesOrdered</code></a> functionality. We'll cover these in the <a href="part-guide/streams.html">streams</a> chapter.</p>
<h2 id="raceselect"><a class="header" href="#raceselect">Race/select</a></h2>
<p>The counterpart to joining futures is racing them (aka selecting on them). With race/select the futures are executed concurrently, but rather than waiting for all the futures to complete, we only wait for the first one to complete and then cancel the others. Although this sounds similar to joining, it is significantly more interesting (and sometimes error-prone) because now we have to reason about cancellation.</p>
<p>Here's an example using Tokio's <a href="https://docs.rs/tokio/latest/tokio/macro.select.html"><code>select</code></a> macro:</p>
<pre><pre class="playground"><code class="language-rust norun">async fn main() {
  select! {
    result = do_a_thing() =&gt; {
      println!("computation completed and returned {result});
    }
    _ = timeout() =&gt; {
      println!("computation timed-out");
    }
  }
}</code></pre></pre>
<p>You'll notice things are already more interesting than with the <code>join</code> macro because we handle the results of the futures within the <code>select</code> macro. It looks a bit like a <code>match</code> expression, but with <code>select</code>, all branches are run concurrently and the body of the branch which finishes first is executed with its result (the other branches are not executed and the futures are cancelled by <code>drop</code>ping). In the example, <code>do_a_thing</code> and <code>timeout</code> execute concurrently and the first to complete will have it's block executed (i.e., only one <code>println</code> will run), the other future will be cancelled. As with the <code>join</code> macro, awaiting the futures is implicit.</p>
<p>Tokio's <code>select</code> macro supports a bunch of features:</p>
<ul>
<li>pattern matching: the syntax on the left of <code>=</code> on each branch can be a pattern and the block is only executed if the result of the future matches the pattern. If the pattern does not match, then the future is no longer polled (but other futures are). This can be useful for futures which optionally return a value, e.g., <code>Some(x) = do_a_thing() =&gt; { ... }</code>.</li>
<li><code>if</code> guards: each branch may have an <code>if</code> guard. When the <code>select</code> macro runs, after evaluating each expression to produce a future, the <code>if</code> guard is evaluated and the future is only polled if the guard is true. E.g., <code>x = = do_a_thing() if false =&gt; { ... }</code> will never be polled. Note that the <code>if</code> guard is not re-evaluated during polling, only when the macro is initialized.</li>
<li><code>else</code> branch: <code>select</code> can have an <code>else</code> branch <code>else =&gt; { ... }</code>, this is executed if all the futures have stopped and none of the blocks have been executed. If this happens without an <code>else</code> branch, then <code>select</code> will panic.</li>
</ul>
<p>The value of the <code>select!</code> macro is the value of the executed branch (just like <code>match</code>), so all branches must have the same type. E.g., if we wanted to use the result of the above example outside of the <code>select</code>, we'd write it like</p>
<pre><pre class="playground"><code class="language-rust norun">async fn main() {
  let result = select! {
    result = do_a_thing() =&gt; {
      Some(result)
    }
    _ = timeout() =&gt; {
      None
    }
  };

  // Use `result`
}</code></pre></pre>
<p>As with <code>join!</code>, <code>select!</code> does not treat <code>Result</code>s in any special way (other than the pattern matching mentioned previously) and if a branch completes with an error, then all other branches will be cancelled and the error will be used as the result of select (in the same way as if the branch has completed successfully).</p>
<p>The <code>select</code> macro intrinsically uses cancellation, so if you're trying to avoid cancellation in your program, you must avoid <code>select!</code>. In fact, <code>select</code> is often the primary source of cancellation in an async program. As discussed <a href="part-guide/../part-reference/cancellation.html">elsewhere</a>, cancellation has many subtle issues which can lead to bugs. In particular, note that <code>select</code> cancels futures by simply dropping them. This will not notify the future being dropped or trigger any cancellation tokens, etc.</p>
<p><code>select!</code> is often used in a loop to handle streams or other sequences of futures. This adds an extra layer of complexity and opportunities for bugs. In the simple case that we create a new, independent future on each iteration of the loop, things are not much more complicated. However, this is rarely what is needed. Generally we want to preserve some state between iterations. It is common to use <code>select</code> in a loop with streams, where each iteration of the loop handles one result from the stream. E.g.:</p>
<pre><pre class="playground"><code class="language-rust norun">async fn main() {
  let mut stream = ...;

  loop {
    select! {
      result = stream.next() =&gt; {
        match result {
          Some(x) =&gt; println!("received: {x}"),
          None =&gt; break,
        }
      }
      _ = timeout() =&gt; {
        println!("time out!");
        break;
      }
    }
  }
}</code></pre></pre>
<p>In this example, we read values from <code>stream</code> and print them until there are none left or waiting for a result times out. What happens to any remaining data in the stream in the timeout case depends on the implementation of the stream (it might be lost! Or duplicated!). This is an example of why behaviour in the face of cancellation can be important (and tricky).</p>
<p>We may want to reuse a future, not just a stream, across iterations. For example, we may want to race against a timeout future where the timeout applies to all iterations rather than applying a new timeout for each iteration. This is possible by creating the future outside of the loop and referencing it:</p>
<pre><pre class="playground"><code class="language-rust norun">async fn main() {
  let mut stream = ...;
  let mut timeout = timeout();

  loop {
    select! {
      result = stream.next() =&gt; {
        match result {
          Some(x) =&gt; println!("received: {x}"),
          None =&gt; break,
        }
      }
      // Create a reference to `timeout` rather than moving it.
      _ = &amp;mut timeout =&gt; {
        println!("time out!");
        break;
      }
    }
  }
}</code></pre></pre>
<p>There are a couple of important details when using <code>select!</code> in a loop with futures or streams created outside of the <code>select!</code>. These are a fundamental consequence of how <code>select</code> works, so I'll introduce them by stepping through the details of <code>select</code>, using <code>timeout</code> in the last example as an example.</p>
<ul>
<li><code>timeout</code> is created outside of the loop and initialised with some time to count down.</li>
<li>On each iteration of the loop, <code>select</code> creates a reference to <code>timeout</code>, but does not change its state.</li>
<li>As <code>select</code> executes, it polls <code>timeout</code> which will return <code>Pending</code> while there is time left and <code>Ready</code> when the time elapses, at which point its block is executed.</li>
</ul>
<p>In the above example, when <code>timeout</code> is ready, we <code>break</code> out of the loop. But what if we didn't do that? In that case, <code>select</code> would simply poll <code>timeout</code> again, which the <code>Future</code> <a href="https://doc.rust-lang.org/std/future/trait.Future.html#tymethod.poll">docs</a> say should not happen! <code>select</code> can't help this, it doesn't have any state (between iterations) to decide if <code>timeout</code> should be polled. Depending on how <code>timeout</code> is written, this might cause a panic, a logic error, or some kind of crash.</p>
<p>You can prevent this kind of bug in several ways:</p>
<ul>
<li>Use a <a href="part-guide/futures.html#fusing">fused</a> <a href="https://docs.rs/futures/latest/futures/future/trait.FutureExt.html#method.fuse">future</a> or <a href="https://docs.rs/futures/latest/futures/stream/trait.StreamExt.html#method.fuse">stream</a> so that re-polling is safe.</li>
<li>Ensure that your code is structured so that futures are never re-polled, e.g., by breaking out of the loop (as in the previous example), or by using an <code>if</code> guard.</li>
</ul>
<p>Now, lets consider the type of <code>&amp;mut timeout</code>. Lets assume that <code>timeout()</code> returns a type which implements <code>Future</code>, which might be an anonymous type from an async function, or it might be a named type like <code>Timeout</code>. Lets assume the latter because it makes the examples easier (but the logic applies in either case). Given that <code>Timeout</code> implents <code>Future</code>, will <code>&amp;mut Timeout</code> implement <code>Future</code>? Not necessarily! There is a <a href="https://doc.rust-lang.org/std/future/trait.Future.html#impl-Future-for-%26mut+F">blanket <code>impl</code></a> which makes this true, but only if <code>Timeout</code> implements <code>Unpin</code>. That is not the case for all futures, so often you'll get a type error writing code like the last example. Such an error is easily fixed though by using the <code>pin</code> macro, e.g., <code>let mut timeout = pin!(timeout());</code></p>
<p>Cancellation with <code>select</code> in a loop is a rich source of subtle bugs. These usually happen where a future contains some state involving some data but not the data itself. When the future is dropped by cancellation, that state is lost but the underlying data is not updated. This can lead to data being lost or processed multiple times.</p>
<h3 id="alternatives-1"><a class="header" href="#alternatives-1">Alternatives</a></h3>
<p>Futures.rs has its own <a href="https://docs.rs/futures/latest/futures/macro.select.html"><code>select</code> macro</a> and futures-concurrency has a <a href="https://docs.rs/futures-concurrency/latest/futures_concurrency/future/trait.Race.html">Race trait</a> which are alternatives to Tokio's <code>select</code> macro. These both have the same core semantics of concurrently racing multiple futures, processing the result of the first and cancelling the others, but they have different syntax and vary in the details.</p>
<p>Futures.rs' <code>select</code> is superficially similar to Tokio's; to summarize the differences, in the futures.rs version:</p>
<ul>
<li>Futures must always be fused (enforced by type-checking).</li>
<li><code>select</code> has <code>default</code> and <code>complete</code> branches, rather than an <code>else</code> branch.</li>
<li><code>select</code> does not support <code>if</code> guards.</li>
</ul>
<p>Futures-concurrency's <code>Race</code> has a very different syntax, similar to it's version of <code>join</code>, e.g., <code>(future_a, future_b).race().await</code> (it works on <code>Vec</code>s and arrays as well as tuples). The syntax is less flexible than the macros, but fits in nicely with most async code. Note that if you use <code>race</code> within a loop, you can still have the same issues as with <code>select</code>.</p>
<p>As with <code>join</code>, spawning tasks and letting them execute in parallel is often a good alternative to using <code>select</code>. However, cancelling the remaining tasks after the first completes requires some extra work. This can be done using channels or a cancellation token. In either case, cancellation requires some action by the task being cancelled which means the task can do some tidying up or other graceful shutdown.</p>
<p>A common use for <code>select</code> (especially inside a loop) is working with streams. There are stream combinator methods which can replace some uses of select. For example, <a href="https://docs.rs/futures-concurrency/latest/futures_concurrency/stream/trait.Merge.html"><code>merge</code></a> in futures-concurrency is a good alternative to merge multiple streams together.</p>
<h2 id="final-words"><a class="header" href="#final-words">Final words</a></h2>
<p>In this section we've talked about two ways to run groups of futures concurrently. Joining futures means waiting for them all to finish; selecting (aka racing) futures means waiting for the first to finish. In contrast to spawning tasks, these compositions make no use of parallelism.</p>
<p>Both <code>join</code> and <code>select</code> operate on sets of futures which are known in advance (often when writing the program, rather than at runtime). Sometimes, the futures to be composed are not known in advance - futures must be added to the set of composed futures as they are being executed. For this we need <a href="part-guide/streams.html">streams</a> which have their own composition operations.</p>
<p>It's worth reiterating that although these composition operators are powerful and expressive, it is often easier and more appropriate to use tasks and spawning: parallelism is often desirable, you're less likely to have bugs around cancellation or blocking, and resource allocation is usually fairer (or at least simpler) and more predictable.</p>
<hr>
<ol class="footnote-definition"><li id="footnote-into">
<p>The expressions must have a type which implements <code>IntoFuture</code>. The expression is evaluated and converted to a future by the macro. I.e., they don't actually have to evaluate to a future, but rather something which can be converted into a future, but this is a pretty minor distinction. The expressions themselves are evaluated sequentially before any of the resulting futures are executed. <a href="#fr-into-1">↩</a></p>
</li>
</ol><div style="break-before: page; page-break-before: always;"></div><h1 id="channels-locking-and-synchronization"><a class="header" href="#channels-locking-and-synchronization">Channels, locking, and synchronization</a></h1>
<p>note on runtime specificness of sync primitves</p>
<p>Why we need async primitives rather than use the sync ones</p>
<h2 id="channels"><a class="header" href="#channels">Channels</a></h2>
<ul>
<li>basically same as the std ones, but await
<ul>
<li>communicate between tasks (same thread or different)</li>
</ul>
</li>
<li>one shot</li>
<li>mpsc</li>
<li>other channels</li>
<li>bounded and unbounded channels</li>
</ul>
<h2 id="locks"><a class="header" href="#locks">Locks</a></h2>
<ul>
<li>async Mutex
<ul>
<li>c.f., std::Mutex - can be held across await points (borrowing the mutex in the guard, guard is Send, scheduler-aware? or just because lock is async?), lock is async (will not block the thread waiting for lock to be available)
<ul>
<li>even a clippy lint for holding the guard across await (https://rust-lang.github.io/rust-clippy/master/index.html#await_holding_lock)</li>
</ul>
</li>
<li>more expensive because it can be held across await
<ul>
<li>use std::Mutex if you can
<ul>
<li>can use try_lock or mutex is expected to not be under contention</li>
</ul>
</li>
</ul>
</li>
<li>lock is not magically dropped when yield (that's kind of the point of a lock!)</li>
<li>deadlock by holding mutex over await
<ul>
<li>tasks deadlocked, but other tasks can make progress so might not look like a deadlock in process stats/tools/OS</li>
<li>usual advice - limit scope, minimise locks, order locks, prefer alternatives</li>
</ul>
</li>
<li>no mutex poisoning</li>
<li>lock_owned</li>
<li>blocking_lock
<ul>
<li>cannot use in async</li>
</ul>
</li>
<li>applies to other locks (should the above be moved before discussion of mutex specifically? Probably yes)</li>
</ul>
</li>
<li>RWLock</li>
<li>Semaphore</li>
<li>yielding</li>
</ul>
<h2 id="other-synchronization-primitives"><a class="header" href="#other-synchronization-primitives">Other synchronization primitives</a></h2>
<ul>
<li>notify, barrier</li>
<li>OnceCell</li>
<li>atomics</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tools-for-async-programming"><a class="header" href="#tools-for-async-programming">Tools for async programming</a></h1>
<ul>
<li>Why we need specialist tools for async</li>
<li>Are there other tools to cover
<ul>
<li>loom</li>
</ul>
</li>
</ul>
<h2 id="monitoring"><a class="header" href="#monitoring">Monitoring</a></h2>
<ul>
<li><a href="https://github.com/tokio-rs/console">Tokio console</a></li>
</ul>
<h2 id="tracing-and-logging"><a class="header" href="#tracing-and-logging">Tracing and logging</a></h2>
<ul>
<li>issues with async tracing</li>
<li>tracing crate (https://github.com/tokio-rs/tracing)</li>
</ul>
<h2 id="debugging"><a class="header" href="#debugging">Debugging</a></h2>
<ul>
<li>Understanding async backtraces (RUST_BACKTRACE and in a debugger)</li>
<li>Techniques for debugging async code</li>
<li>Using Tokio console for debugging</li>
<li>Debugger support (WinDbg?)</li>
</ul>
<h2 id="profiling"><a class="header" href="#profiling">Profiling</a></h2>
<ul>
<li>How async messes up flamegraphs</li>
<li>How to profile async IO</li>
<li>Getting insight into the runtime
<ul>
<li>Tokio metrics</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="destruction-and-clean-up"><a class="header" href="#destruction-and-clean-up">Destruction and clean-up</a></h1>
<ul>
<li>Object destruction and recap of Drop</li>
<li>General clean up requirements in software</li>
<li>Async issues
<ul>
<li>Might want to do stuff async during clean up, e.g., send a final message</li>
<li>Might need to clean up stuff which is still being used async-ly</li>
<li>Might want to clean up when an async task completes or cancels and there is no way to catch that</li>
<li>State of the runtime during clean-up phase (esp if we're panicking or whatever)</li>
<li>No async Drop
<ul>
<li>WIP</li>
</ul>
</li>
<li>forward ref to completion io topic</li>
</ul>
</li>
</ul>
<h2 id="cancellation-1"><a class="header" href="#cancellation-1">Cancellation</a></h2>
<ul>
<li>How it happens (recap of more-async-await.md)
<ul>
<li>drop a future</li>
<li>cancellation token</li>
<li>abort functions</li>
</ul>
</li>
<li>What we can do about 'catching' cancellation
<ul>
<li>logging or monitoring cancellation</li>
</ul>
</li>
<li>How cancellation affects other futures tasks (forward ref to cancellation safety chapter, this should just be a heads-up)</li>
</ul>
<h2 id="panicking-and-async"><a class="header" href="#panicking-and-async">Panicking and async</a></h2>
<ul>
<li>Propagation of panics across tasks (spawn result)</li>
<li>Panics leaving data inconsistent (tokio mutexes)</li>
<li>Calling async code when panicking (make sure you don't)</li>
</ul>
<h2 id="patterns-for-clean-up"><a class="header" href="#patterns-for-clean-up">Patterns for clean-up</a></h2>
<ul>
<li>Avoid needing clean up (abort/restart)</li>
<li>Don't use async for cleanup and don't worry too much</li>
<li>async clean up method + dtor bomb (i.e., separate clean-up from destruction)</li>
<li>centralise/out-source clean-up in a separate task or thread or supervisor object/process</li>
<li>https://tokio.rs/tokio/topics/shutdown</li>
</ul>
<h2 id="why-no-async-drop-yet"><a class="header" href="#why-no-async-drop-yet">Why no async Drop (yet)</a></h2>
<ul>
<li>Note this is advanced section and not necessary to read</li>
<li>Why async Drop is hard</li>
<li>Possible solutions and there issues</li>
<li>Current status</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="futures"><a class="header" href="#futures">Futures</a></h1>
<p>We've talked a lot about futures in the preceding chapters; they're a key part of Rust's async programming story! In this chapter we're going to get into some of the details of what futures are and how they work, and some libraries for working directly with futures.</p>
<h2 id="the-future-and-intofuture-traits"><a class="header" href="#the-future-and-intofuture-traits">The <code>Future</code> and <code>IntoFuture</code> traits</a></h2>
<ul>
<li>Future
<ul>
<li>Output assoc type</li>
<li>No real detail here, polling is in the next section, reference adv sections on Pin, executors/wakers</li>
</ul>
</li>
<li>IntoFuture
<ul>
<li>Usage - general, in await, async builder pattern (pros and cons in using)</li>
</ul>
</li>
<li>Boxing futures, <code>Box&lt;dyn Future&gt;</code> and how it used to be common and necessary but mostly isn't now, except for recursion, etc.</li>
</ul>
<h2 id="polling"><a class="header" href="#polling">Polling</a></h2>
<ul>
<li>what it is and who does it, Poll type
<ul>
<li>ready is final state</li>
</ul>
</li>
<li>how it connects with await</li>
<li>drop = cancel
<ul>
<li>for futures and thus tasks</li>
<li>implications for async programming in general</li>
<li>reference to chapter on cancellation safety</li>
</ul>
</li>
</ul>
<h3 id="fusing"><a class="header" href="#fusing">Fusing</a></h3>
<h2 id="futures-rs-crate"><a class="header" href="#futures-rs-crate">futures-rs crate</a></h2>
<ul>
<li>History and purpose
<ul>
<li>see streams chapter</li>
<li>helpers for writing executors or other low-level futures stuff
<ul>
<li>pinning and boxing</li>
</ul>
</li>
<li>executor as a partial runtime (see alternate runtimes in reference)</li>
</ul>
</li>
<li>TryFuture</li>
<li>convenience futures: pending, ready, ok/err, etc.</li>
<li>combinator functions on FutureExt</li>
<li>alternative to Tokio stuff
<ul>
<li>functions</li>
<li>IO traits</li>
</ul>
</li>
</ul>
<h2 id="futures-concurrency-crate"><a class="header" href="#futures-concurrency-crate">futures-concurrency crate</a></h2>
<p>https://docs.rs/futures-concurrency/latest/futures_concurrency/</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="runtimes-and-runtime-issues"><a class="header" href="#runtimes-and-runtime-issues">Runtimes and runtime issues</a></h1>
<h2 id="running-async-code"><a class="header" href="#running-async-code">Running async code</a></h2>
<ul>
<li>Explicit startup vs async main</li>
<li>tokio context concept</li>
<li>block_on</li>
<li>runtime as reflected in the code (Runtime, Handle)</li>
<li>runtime shutdown</li>
</ul>
<h2 id="threads-and-tasks"><a class="header" href="#threads-and-tasks">Threads and tasks</a></h2>
<ul>
<li>default work stealing, multi-threaded
<ul>
<li>revisit Send + 'static bounds</li>
</ul>
</li>
<li>yield</li>
<li>spawn-local</li>
<li>spawn-blocking (recap), block-in-place</li>
<li>tokio-specific stuff on yielding to other threads, local vs global queues, etc</li>
</ul>
<h2 id="configuration-options"><a class="header" href="#configuration-options">Configuration options</a></h2>
<ul>
<li>thread pool size</li>
<li>single threaded, thread per core etc.</li>
</ul>
<h2 id="alternate-runtimes"><a class="header" href="#alternate-runtimes">Alternate runtimes</a></h2>
<ul>
<li>Why you'd want to use a different runtime or implement your own</li>
<li>What kind of variations exist in the high-level design</li>
<li>Forward ref to adv chapters</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="timers-and-signal-handling"><a class="header" href="#timers-and-signal-handling">Timers and Signal handling</a></h1>
<h2 id="time-and-timers"><a class="header" href="#time-and-timers">Time and Timers</a></h2>
<ul>
<li>runtime integration, don't use thread::sleep, etc.</li>
<li>std Instant and Duration</li>
<li>sleep</li>
<li>interval</li>
<li>timeout
<ul>
<li>special future vs select/race</li>
</ul>
</li>
</ul>
<h2 id="signal-handling"><a class="header" href="#signal-handling">Signal handling</a></h2>
<ul>
<li>what is signal handling and why is it an async issue?</li>
<li>very OS specific</li>
<li>see Tokio docs</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="async-iterators-fka-streams"><a class="header" href="#async-iterators-fka-streams">Async iterators (FKA streams)</a></h1>
<ul>
<li>Stream as an async iterator or as many futures</li>
<li>WIP
<ul>
<li>current status</li>
<li>futures and Tokio Stream traits</li>
<li>nightly trait</li>
</ul>
</li>
<li>lazy like sync iterators</li>
<li>pinning and streams (forward ref to pinning chapter)</li>
<li>fused streams</li>
</ul>
<h2 id="consuming-an-async-iterator"><a class="header" href="#consuming-an-async-iterator">Consuming an async iterator</a></h2>
<ul>
<li>while let with async next</li>
<li>for_each, for_each_concurrent</li>
<li>collect</li>
<li>into_future, buffered</li>
</ul>
<h2 id="stream-combinators"><a class="header" href="#stream-combinators">Stream combinators</a></h2>
<ul>
<li>Taking a future instead of a closure</li>
<li>Some example combinators</li>
<li>unordered variations</li>
<li>StreamGroup</li>
</ul>
<h3 id="joinselectrace-with-streams"><a class="header" href="#joinselectrace-with-streams">join/select/race with streams</a></h3>
<ul>
<li>hazards with select in a loop</li>
<li>fusing</li>
<li>difference to just futures</li>
<li>alternatives to these
<ul>
<li>Stream::merge, etc.</li>
</ul>
</li>
</ul>
<h2 id="implementing-an-async-iterator"><a class="header" href="#implementing-an-async-iterator">Implementing an async iterator</a></h2>
<ul>
<li>Implementing the trait</li>
<li>Practicalities and util functions</li>
<li>async_iter stream macro</li>
</ul>
<h2 id="sinks"><a class="header" href="#sinks">Sinks</a></h2>
<ul>
<li>https://docs.rs/futures/latest/futures/sink/index.html</li>
</ul>
<h2 id="future-work"><a class="header" href="#future-work">Future work</a></h2>
<ul>
<li>current status
<ul>
<li>https://rust-lang.github.io/rfcs/2996-async-iterator.html</li>
</ul>
</li>
<li>async next vs poll</li>
<li>async iteration syntax</li>
<li>(async) generators</li>
<li>lending iterators</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cancellation-and-cancellation-safety"><a class="header" href="#cancellation-and-cancellation-safety">Cancellation and cancellation safety</a></h1>
<p>Internal vs external cancellation
Threads vs futures
drop = cancel
only at await points
useful feature
still somewhat abrubt and surprising
Other cancellation mechanisms
abort
cancellation tokens</p>
<h2 id="cancellation-safety"><a class="header" href="#cancellation-safety">Cancellation safety</a></h2>
<p>Not a memory safety issue or race condition
Data loss or other logic errors
Different definitions/names
tokio's definition
general definition/halt safety
applying a replicated future idea
Simple data loss
Resumption
Issue with select or similar in loops
Splitting state between the future and the context as a root cause</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="structured-concurrency"><a class="header" href="#structured-concurrency">Structured Concurrency</a></h1>
<p>Authors note (TODO): we might want to discuss some parts of this chapter much earlier in the book, in particularly as design principles (first intro is in guide/intro). However, in the interests of better understanding the topic and getting something written down, I'm starting with a separate chapter. It's also still a bit rough.</p>
<p>(Note: the first few sections are talking about the abstract concept of structured concurrency and is not specific to Rust or async programming (c.f., synchronous concurrent programming with threads). I use 'task' to mean any thread or async task or other similar concurrency primitive).</p>
<p>Structured concurrency is a philosophy for designing concurrent programs. For programs to fully adhere to the principals of structured concurrency requires certain language features and libraries, but many of the benefits are available by following the philosophy without such features. Structured concurrency is independent of language and concurrency primitives (threads vs async, etc.). Many people have found the ideas from structured concurrency to be useful when programming with async Rust.</p>
<p>The essential idea of structured concurrency is that tasks are organised into a tree. Child tasks start after their parents and always finish before them. This allows results and errors to always be passed back to parent tasks, and requires that cancellation of parents is always propagated to child tasks. Primarily, temporal scope follows lexical scope, which means that a task should not outlive the function or block where it is created. However, this is not a requirement of structured concurrency as long as longer-lived tasks are reified in the program in some way (typically by using an object to represent the temporal scope of a child task within its parent task).</p>
<p>TODO diagram</p>
<p>Structured concurrency is named by analogy to <a href="https://en.wikipedia.org/wiki/Structured_programming">structured programming</a>, which is the idea that control flow should be structured using functions, loops, etc., rather than arbitrary jumps (<code>goto</code>).</p>
<p>Before we consider structured concurrency, it's helpful to reflect on the sense in which common concurrent designs are unstructured. A typical pattern is that a task is started using some kind of spawning statement. That task then runs to completion concurrently with other tasks in the system (including the task which spawned it). There is no constraint on which task finishes first. The program is essentially just a bag of tasks which live independently and might terminate at any time. Any communication or synchronization of the tasks is ad hoc, and the programmer cannot assume that any other task will still be running.</p>
<p>The practical downsides of unstructured concurrency are that returning results from a task must happen in an extra-linguistic fashion with no language-level guarantees around when or how this happens. Errors may go uncaught because languages' error handling mechanisms cannot be applied to the unconstrained control flow of unstructured concurrency. We also have no guarantees about the relative state of tasks - any task may be running, terminated successfully or with an error, or externally cancelled, independent of the state of any others<sup class="footnote-reference" id="fr-join-1"><a href="#footnote-join">1</a></sup>. All this makes concurrent programs difficult to understand and maintain. This lack of structure is one reason why concurrent programming is considered categorically more difficult than sequential programming.</p>
<p>It's worth noting that structured concurrency is a programming discipline which imposes restrictions on your program. Just like functions and loops are less flexible than goto, structured concurrency is less flexible than just spawning tasks. However, as with structured programming the costs of structured concurrency in flexibility are outweighed by the gains in predictability.</p>
<h2 id="principles-of-structured-concurrency"><a class="header" href="#principles-of-structured-concurrency">Principles of structured concurrency</a></h2>
<p>The key idea of structured concurrency is that all tasks (or threads or whatever) are organized as a tree. I.e., each task (except the main task which is the root) has a single parent and there are no cycles of parents. A child task is started by its parent<sup class="footnote-reference" id="fr-start-parent-1"><a href="#footnote-start-parent">2</a></sup> and must <em>always</em> finish executing before its parent. There are no constraints between siblings. The parent of a task may not change.</p>
<p>When reasoning about programs which implement structured concurrency, the key new fact is that if a task is live, then all of its ancestor tasks must also be live. This doesn't guarantee they are in a good state - they might be in the process of shutting down or handling an error, but they must be running in some form. This means that for any task (except the root task), there is always a live task to send results or errors to. Indeed, the ideal approach is that the language's error handling is extended so that errors are always propagated to the parent task. In Rust, this should apply to both returning <code>Result::Err</code> and to panicking.</p>
<p>Furthermore, the lifetime of child tasks can be represented in the parent task. In the common case, the lifetime of a task (its temporal scope) is tied to the lexical scope in which it is started. For example, all tasks started within a function should complete before the function returns. This is an extremely powerful reasoning tool. Of course, this is too restrictive for all cases, and so the temporal scope of tasks can extend beyond a lexical scope by using an object in the program (often called a 'scope' or 'nursery'). Such an object can be passed or stored, and thus have an arbitrary lifetime. We still have an important reasoning tool: the tasks tied to that object cannot outlive it (in Rust this property lets us integrate tasks with the lifetime system).</p>
<p>The above leads to another benefit of structured concurrency: it lets us reason about resource management across multiple tasks. Cleanup code is called when a resource will no longer be used (e.g., closing a file handle). In sequential code, the problem of when to call cleanup code is solved by ensuring destructors are called when an object goes out of scope. However, in concurrent code, an object might still be in use by another task and so when to clean up is unclear (reference counting or garbage collection are solutions in many cases, but make reasoning about the lifetimes of objects difficult which can lead to errors, and also has runtime overheads).</p>
<p>The principle of a parent task outliving it's children has an important implication for cancellation: if a task is cancelled, then all its child tasks must be cancelled, and their cancellation must complete before the parent's cancellation completes. That in turn has implications for how cancellation can be implemented in a structurally concurrent system.</p>
<p>If a task completes early due to an error (in Rust, this might mean a panic, as well as an early return), then before returning the task must wait for all its child tasks to complete. In practice, an early return must trigger cancellation of child tasks. This is analogous to panicking in Rust: panicking triggers destructors in the current scope before walking up the stack, calling destructors in each scope until the program terminates or the panic is caught. Under structural concurrency, an early return must trigger cancellation of child tasks (and thus cleanup of objects in those tasks) and walks down the tree of tasks cancelling all (transitive) children.</p>
<p>Some designs work very naturally under structured concurrency (e.g., worker tasks with a single job to complete), while others don't fit so well. Generally these patterns are ones where not being tied to a specific task is a feature, e.g., worker pools or background threads. Even using these patterns, the tasks usually shouldn't outlive the whole program and so there is always one task which can be the parent.</p>
<h3 id="implementing-structured-concurrency"><a class="header" href="#implementing-structured-concurrency">Implementing structured concurrency</a></h3>
<p>The exemplar implementation of structured concurrency is the Python <a href="https://trio.readthedocs.io/en/stable/">Trio</a> library. Trio is a general purpose library for async programming and IO designed around the concepts of structured concurrency. Trio programs use the <code>async with</code> construct to define a lexical scope for spawning tasks. Spawned tasks are associated with a <a href="https://trio.readthedocs.io/en/stable/reference-core.html#nurseries-and-spawning">nursery</a> object (which is somewhat like a <a href="https://doc.rust-lang.org/stable/std/thread/struct.Scope.html">Scope</a> in Rust). The lifetime of a task is tied to the dynamic temporal scope of its nursery, and in the common case, the lexical scope of an <code>async with</code> block. This enforces the parent/child relationship between tasks and thus the tree-invariant of structured concurrency.</p>
<p>Error handling uses Python exceptions which are automatically propagated to parent tasks.</p>
<h3 id="partially-structured-concurrency"><a class="header" href="#partially-structured-concurrency">Partially structured concurrency</a></h3>
<p>Like many programming techniques, the full benefits of structured concurrency come from <em>only</em> using it. If all concurrency is structured, then it makes it much easier to reason about the behaviour of the whole program. However, that has requirements on a language which are not easily met; it is easy enough to do unstructured concurrency in Rust, for example. However, even applying the principles of structured concurrency selectively, or thinking in terms of structured concurrency can be useful.</p>
<p>One can use structured concurrency as a design discipline. When designing a program, always consider and document the parent-child relationships between tasks and ensure that a child task terminates before it's parent. This is usually fairly easy under normal execution, but can be difficult in the face of cancellation and panics.</p>
<p>Another element of structured concurrency which is fairly easy to adopt is to always propagate errors to the parent task. Just like regular error handling, the best thing to do might be to ignore the error, but this should be explicit in the code of the parent task.</p>
<p>Another programming discipline to learn from structured concurrency is to cancel all child tasks in the event of cancelling a parent task. This makes the structural concurrency guarantees much more reliable and makes cancellation in general easier to reason about.</p>
<h2 id="practical-structured-concurrency-with-async-rust"><a class="header" href="#practical-structured-concurrency-with-async-rust">Practical structured concurrency with async Rust</a></h2>
<p>Concurrency in Rust (whether async or using threads) is inherently unstructured. Tasks can be arbitrarily spawned, errors and panics on other tasks can be ignored, and cancellation is usually instantaneous and does not propagate to other tasks (see below for why these issues can't be easily solved). However, there are several ways you can get some of the benefits of structured concurrency in your programs:</p>
<ul>
<li>Design your programs at a high level in accordance with structured concurrency.</li>
<li>Stick to structured concurrency idioms where possible (and avoid unstructured idioms).</li>
<li>Use crates to make structured concurrency more ergonomic and reliable.</li>
</ul>
<p>One of the trickiest issues with using structured concurrency with Rust is propagating cancellation to child futures/tasks. If you're using futures and <a href="part-reference/../part-guide/concurrency-primitives.html">composing them concurrently</a>, then this happens naturally if abruptly (dropping a future drops any futures it owns, cancelling them). However, when a task is dropped, there is no opportunity to send a signal to tasks it has spawned (at least not with Tokio<sup class="footnote-reference" id="fr-join_handle-1"><a href="#footnote-join_handle">3</a></sup>).</p>
<p>The implication of this is that you can only assume a weaker invariant than with 'real' structured concurrency: rather than being able to assume that a parent task is always alive, you can only assume that the parent is always alive unless it has been cancelled or it has panicked. While this is sub-optimal, it can still simplify programming because you never have to handle the case of having no parent to handle some result <em>under normal execution</em>.</p>
<p>TODO</p>
<ul>
<li>ownership/lifetimes naturally leading to sc</li>
<li>reasoning about resources</li>
</ul>
<h3 id="applying-structured-concurrency-to-the-design-of-async-programs"><a class="header" href="#applying-structured-concurrency-to-the-design-of-async-programs">Applying structured concurrency to the design of async programs</a></h3>
<p>In terms of designing programs, applying structured concurrency has a few implications:</p>
<ul>
<li>Organising the concurrency of a program in a tree structure, i.e., thinking in terms of parent and child tasks.</li>
<li>Temporal scope should follow lexical scope where possible, or in concrete terms a function shouldn't return (including early returns and panics) until any tasks launched in the function are complete.</li>
<li>Data generally flows from child tasks to parent tasks. Of course, some data will flow from parents to children or in other ways, but primarily, tasks pass the results of their work to their parent tasks for further processing. This includes errors, so parent tasks should handle the errors of their children.</li>
</ul>
<p>If you're writing a library and want to use structured concurrency (or you want the library to be usable in a concurrent-structured program), then it is important that encapsulation of the library component includes temporal encapsulation. I.e., it doesn't start tasks which keep running beyond the API functions returning.</p>
<p>Since Rust can't enforce the rules of structured concurrency, it's important to be aware of, and to document, in which ways the program (or component) is structured and where it violates the structured concurrency discipline.</p>
<p>One useful compromise pattern is to only allow unstructured concurrency at the highest level of abstraction, and only for tasks spawned from the outer-most functions of the main task (ideally only from the <code>main</code> function, but programs often have some setup or configuration code which means that the logical 'top level' of a program is actually a few functions deep). Under such a pattern, a bunch of tasks are spawned from <code>main</code>, usually with distinct responsibilities and limited interaction between each other. These tasks might be restarted, new tasks started by any other task, or have a limited lifetime tied to clients or similar, i.e., they are concurrent-unstructured. Within each of these tasks, structured concurrency is rigorously applied.</p>
<p>TODO why is this useful?</p>
<p>TODO would be great to have a case study here.</p>
<h3 id="structured-and-unstructured-idioms"><a class="header" href="#structured-and-unstructured-idioms">Structured and unstructured idioms</a></h3>
<p>This subsection covers a grab-bag of idioms which work well with a structured approach to concurrency, and a few which make structuring concurrency more difficult.</p>
<p>The easiest way to follow structured concurrency is to use futures and <a href="part-reference/../part-guide/concurrency-primitives.html">concurrent composition</a> rather than tasks and spawning. If you need tasks for parallelism, then you will need to use <code>JoinHandle</code>s or <code>JoinSet</code>s. You must take care that child tasks can clean up properly if the parent task panics or is cancelled. Handles must be checked for errors to ensure errors in child tasks are properly handled.</p>
<p>One way to work around the lack of cancellation propagation is to avoid abruptly cancelling (dropping) any task which may have children. Instead use a signal (e.g., a cancellation token) so that the task can cancel it's children before terminating. Unfortunately this is incompatible with <code>select</code>.</p>
<p>To handle shutting down a program (or component), use an explicit shutdown method rather than dropping the component, so that the shutdown function can wait for child tasks to terminate or cancel them (since <code>drop</code> cannot be async).</p>
<p>A few idioms do not play well with structured concurrency:</p>
<ul>
<li>Spawning tasks without awaiting their completion via a join handle, or dropping those join handles.</li>
<li>Select or race macros/functions. These are not inherently structured, but since they abruptly cancel futures, it's a common source of unstructured cancellation.</li>
<li>Worker tasks or pools. For async tasks the overheads of starting/shutting down tasks is so low that there is likely to be very little benefit of using a pool of tasks rather than a pool of 'data', e.g., a connection pool.</li>
<li>Data with no clear ownership structure - this isn't necessarily in contradiction with structured concurrency, but often leads to design issues.</li>
</ul>
<h3 id="crates-for-structured-concurrency"><a class="header" href="#crates-for-structured-concurrency">Crates for structured concurrency</a></h3>
<p>TODO</p>
<ul>
<li>crates: <a href="https://github.com/nikomatsakis/moro">moro</a>, <a href="https://github.com/najamelan/async_nursery">async-nursery</a></li>
<li>futures-concurrency</li>
</ul>
<h2 id="related-topics"><a class="header" href="#related-topics">Related topics</a></h2>
<p>This section is not necessary to know to use structured concurrency with async Rust, but is useful context included for the curious.</p>
<h3 id="scoped-threads"><a class="header" href="#scoped-threads">Scoped threads</a></h3>
<p>Structured concurrency with Rust threads works pretty well. Although you can't prevent spawning threads with unscoped lifetime, this is easy to avoid. Instead, restrict yourself to using scoped threads, see the <a href="https://doc.rust-lang.org/stable/std/thread/fn.scope.html"><code>scope</code></a> function docs for how. Using scoped threads limits child lifetimes and automatically propagates panics back to the parent thread. The parent thread must check the results of child threads to handle errors though. You can even pass around the <a href="https://doc.rust-lang.org/stable/std/thread/struct.Scope.html"><code>Scope</code></a> object like a Trio nursery. Cancellation is not usually an issue for Rust threads, but if you do make use of thread cancellation, you'll have to integrate that with scoped threads manually.</p>
<p>Specific to Rust, scoped threads allow child threads to borrow data from the parent thread, something not possible with concurrent-unstructured threads. This can be very useful and shows how well structured concurrency and Rust-ownership-style resource management can work together.</p>
<h3 id="async-drop-and-scoped-tasks"><a class="header" href="#async-drop-and-scoped-tasks">Async drop and scoped tasks</a></h3>
<p>In Rust, destructors (<code>drop</code>) are used to ensure resources are cleaned up when an object's lifetime ends. Since futures are just objects, their destructor would be an obvious place to ensure cancellation of child futures. However, in an async program it is very often desirable for cleanup actions to be asynchronous (not doing so can block other tasks). Unfortunately Rust does not currently support asynchronous destructors (async drop). There is ongoing work to support them, but it is difficult for a number of reasons, including that an object with an async destructor might be dropped from non-async context, and that since calling <code>drop</code> is implicit, there is nowhere to write an explicit <code>await</code>.</p>
<p>Given how useful scoped threads are (both in general and for structured concurrency), another good question is why there is no similar construct for async programming ('scoped tasks')? TODO answer this</p>
<h3 id="references"><a class="header" href="#references">References</a></h3>
<p>If you're interested, here are some good blog posts for further reading:</p>
<ul>
<li><a href="https://www.250bpm.com/p/structured-concurrency">Structured Concurrency</a></li>
<li><a href="https://blog.yoshuawuyts.com/tree-structured-concurrency/">Tree-structured concurrency</a></li>
</ul>
<hr>
<ol class="footnote-definition"><li id="footnote-join">
<p>Using join handles mitigates these downsides somewhat, but is an ad hoc mechanism with no reliable guarantees. To get the full benefits of structured concurrency you have to be meticulous about always using them, as well as handling cancellation and errors properly. This is difficult without language or library support; we'll discuss this a bit more below. <a href="#fr-join-1">↩</a></p>
</li>
<li id="footnote-start-parent">
<p>This is not actually a hard requirement for structured concurrency. If the temporal scope of a task can be represented in the program and passed between tasks, then a child task can be started by one task but have another as its parent. <a href="#fr-start-parent-1">↩</a></p>
</li>
<li id="footnote-join_handle">
<p>The semantics of Tokio's <code>JoinHandle</code> is that if the handle is dropped, then the underlying task is 'released' (c.f., dropped), i.e., the result of the child task is not handled by any other task. <a href="#fr-join_handle-1">↩</a></p>
</li>
</ol><div style="break-before: page; page-break-before: always;"></div><h1 id="getting-started"><a class="header" href="#getting-started">Getting Started</a></h1>
<p>Welcome to Asynchronous Programming in Rust! If you're looking to start writing
asynchronous Rust code, you've come to the right place. Whether you're building
a web server, a database, or an operating system, this book will show you
how to use Rust's asynchronous programming tools to get the most out of your
hardware.</p>
<h2 id="what-this-book-covers"><a class="header" href="#what-this-book-covers">What This Book Covers</a></h2>
<p>This book aims to be a comprehensive, up-to-date guide to using Rust's async
language features and libraries, appropriate for beginners and old hands alike.</p>
<ul>
<li>
<p>The early chapters provide an introduction to async programming in general,
and to Rust's particular take on it.</p>
</li>
<li>
<p>The middle chapters discuss key utilities and control-flow tools you can use
when writing async code, and describe best-practices for structuring libraries
and applications to maximize performance and reusability.</p>
</li>
<li>
<p>The last section of the book covers the broader async ecosystem, and provides
a number of examples of how to accomplish common tasks.</p>
</li>
</ul>
<p>With that out of the way, let's explore the exciting world of Asynchronous
Programming in Rust!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="why-async"><a class="header" href="#why-async">Why Async?</a></h1>
<p>We all love how Rust empowers us to write fast, safe software.
But how does asynchronous programming fit into this vision?</p>
<p>Asynchronous programming, or async for short, is a <em>concurrent programming model</em>
supported by an increasing number of programming languages.
It lets you run a large number of concurrent
tasks on a small number of OS threads, while preserving much of the
look and feel of ordinary synchronous programming, through the
<code>async/await</code> syntax.</p>
<h2 id="async-vs-other-concurrency-models"><a class="header" href="#async-vs-other-concurrency-models">Async vs other concurrency models</a></h2>
<p>Concurrent programming is less mature and "standardized" than
regular, sequential programming. As a result, we express concurrency
differently depending on which concurrent programming model
the language is supporting.
A brief overview of the most popular concurrency models can help
you understand how asynchronous programming fits within the broader
field of concurrent programming:</p>
<ul>
<li><strong>OS threads</strong> don't require any changes to the programming model,
which makes it very easy to express concurrency. However, synchronizing
between threads can be difficult, and the performance overhead is large.
Thread pools can mitigate some of these costs, but not enough to support
massive IO-bound workloads.</li>
<li><strong>Event-driven programming</strong>, in conjunction with <em>callbacks</em>, can be very
performant, but tends to result in a verbose, "non-linear" control flow.
Data flow and error propagation is often hard to follow.</li>
<li><strong>Coroutines</strong>, like threads, don't require changes to the programming model,
which makes them easy to use. Like async, they can also support a large
number of tasks. However, they abstract away low-level details that
are important for systems programming and custom runtime implementors.</li>
<li><strong>The actor model</strong> divides all concurrent computation into units called
actors, which communicate through fallible message passing, much like
in distributed systems. The actor model can be efficiently implemented, but it leaves
many practical issues unanswered, such as flow control and retry logic.</li>
</ul>
<p>In summary, asynchronous programming allows highly performant implementations
that are suitable for low-level languages like Rust, while providing
most of the ergonomic benefits of threads and coroutines.</p>
<h2 id="async-in-rust-vs-other-languages"><a class="header" href="#async-in-rust-vs-other-languages">Async in Rust vs other languages</a></h2>
<p>Although asynchronous programming is supported in many languages, some
details vary across implementations. Rust's implementation of async
differs from most languages in a few ways:</p>
<ul>
<li><strong>Futures are inert</strong> in Rust and make progress only when polled. Dropping a
future stops it from making further progress.</li>
<li><strong>Async is zero-cost</strong> in Rust, which means that you only pay for what you use.
Specifically, you can use async without heap allocations and dynamic dispatch,
which is great for performance!
This also lets you use async in constrained environments, such as embedded systems.</li>
<li><strong>No built-in runtime</strong> is provided by Rust. Instead, runtimes are provided by
community maintained crates.</li>
<li><strong>Both single- and multithreaded</strong> runtimes are available in Rust, which have
different strengths and weaknesses.</li>
</ul>
<h2 id="async-vs-threads-in-rust"><a class="header" href="#async-vs-threads-in-rust">Async vs threads in Rust</a></h2>
<p>The primary alternative to async in Rust is using OS threads, either
directly through <a href="https://doc.rust-lang.org/std/thread/"><code>std::thread</code></a>
or indirectly through a thread pool.
Migrating from threads to async or vice versa
typically requires major refactoring work, both in terms of implementation and
(if you are building a library) any exposed public interfaces. As such,
picking the model that suits your needs early can save a lot of development time.</p>
<p><strong>OS threads</strong> are suitable for a small number of tasks, since threads come with
CPU and memory overhead. Spawning and switching between threads
is quite expensive as even idle threads consume system resources.
A thread pool library can help mitigate some of these costs, but not all.
However, threads let you reuse existing synchronous code without significant
code changes—no particular programming model is required.
In some operating systems, you can also change the priority of a thread,
which is useful for drivers and other latency sensitive applications.</p>
<p><strong>Async</strong> provides significantly reduced CPU and memory
overhead, especially for workloads with a
large amount of IO-bound tasks, such as servers and databases.
All else equal, you can have orders of magnitude more tasks than OS threads,
because an async runtime uses a small amount of (expensive) threads to handle
a large amount of (cheap) tasks.
However, async Rust results in larger binary blobs due to the state
machines generated from async functions and since each executable
bundles an async runtime.</p>
<p>On a last note, asynchronous programming is not <em>better</em> than threads,
but different.
If you don't need async for performance reasons, threads can often be
the simpler alternative.</p>
<h3 id="example-concurrent-downloading"><a class="header" href="#example-concurrent-downloading">Example: Concurrent downloading</a></h3>
<p>In this example our goal is to download two web pages concurrently.
In a typical threaded application we need to spawn threads
to achieve concurrency:</p>
<pre><code class="language-rust ignore">fn get_two_sites() {
    // Spawn two threads to do work.
    let thread_one = thread::spawn(|| download("https://www.foo.com"));
    let thread_two = thread::spawn(|| download("https://www.bar.com"));

    // Wait for both threads to complete.
    thread_one.join().expect("thread one panicked");
    thread_two.join().expect("thread two panicked");
}</code></pre>
<p>However, downloading a web page is a small task; creating a thread
for such a small amount of work is quite wasteful. For a larger application, it
can easily become a bottleneck. In async Rust, we can run these tasks
concurrently without extra threads:</p>
<pre><code class="language-rust ignore">async fn get_two_sites_async() {
    // Create two different "futures" which, when run to completion,
    // will asynchronously download the webpages.
    let future_one = download_async("https://www.foo.com");
    let future_two = download_async("https://www.bar.com");

    // Run both futures to completion at the same time.
    join!(future_one, future_two);
}</code></pre>
<p>Here, no extra threads are created. Additionally, all function calls are statically
dispatched, and there are no heap allocations!
However, we need to write the code to be asynchronous in the first place,
which this book will help you achieve.</p>
<h2 id="custom-concurrency-models-in-rust"><a class="header" href="#custom-concurrency-models-in-rust">Custom concurrency models in Rust</a></h2>
<p>On a last note, Rust doesn't force you to choose between threads and async.
You can use both models within the same application, which can be
useful when you have mixed threaded and async dependencies.
In fact, you can even use a different concurrency model altogether,
such as event-driven programming, as long as you find a library that
implements it.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-state-of-asynchronous-rust"><a class="header" href="#the-state-of-asynchronous-rust">The State of Asynchronous Rust</a></h1>
<p>Parts of async Rust are supported with the same stability guarantees as
synchronous Rust. Other parts are still maturing and will change
over time. With async Rust, you can expect:</p>
<ul>
<li>Outstanding runtime performance for typical concurrent workloads.</li>
<li>More frequent interaction with advanced language features, such as lifetimes
and pinning.</li>
<li>Some compatibility constraints, both between sync and async code, and between
different async runtimes.</li>
<li>Higher maintenance burden, due to the ongoing evolution of async runtimes
and language support.</li>
</ul>
<p>In short, async Rust is more difficult to use and can result in a higher
maintenance burden than synchronous Rust,
but gives you best-in-class performance in return.
All areas of async Rust are constantly improving,
so the impact of these issues will wear off over time.</p>
<h2 id="language-and-library-support"><a class="header" href="#language-and-library-support">Language and library support</a></h2>
<p>While asynchronous programming is supported by Rust itself,
most async applications depend on functionality provided
by community crates.
As such, you need to rely on a mixture of
language features and library support:</p>
<ul>
<li>The most fundamental traits, types and functions, such as the
<a href="https://doc.rust-lang.org/std/future/trait.Future.html"><code>Future</code></a> trait
are provided by the standard library.</li>
<li>The <code>async/await</code> syntax is supported directly by the Rust compiler.</li>
<li>Many utility types, macros and functions are provided by the
<a href="https://docs.rs/futures/"><code>futures</code></a> crate. They can be used in any async
Rust application.</li>
<li>Execution of async code, IO and task spawning are provided by "async
runtimes", such as Tokio and async-std. Most async applications, and some
async crates, depend on a specific runtime. See
<a href="01_getting_started/../08_ecosystem/00_chapter.html">"The Async Ecosystem"</a> section for more
details.</li>
</ul>
<p>Some language features you may be used to from synchronous Rust are not yet
available in async Rust. Notably, Rust did not let you declare async
functions in traits until 1.75.0 stable (and still has limitations on dynamic dispatch for those traits). Instead, you need to use workarounds to achieve the same
result, which can be more verbose.</p>
<h2 id="compiling-and-debugging"><a class="header" href="#compiling-and-debugging">Compiling and debugging</a></h2>
<p>For the most part, compiler- and runtime errors in async Rust work
the same way as they have always done in Rust. There are a few
noteworthy differences:</p>
<h3 id="compilation-errors"><a class="header" href="#compilation-errors">Compilation errors</a></h3>
<p>Compilation errors in async Rust conform to the same high standards as
synchronous Rust, but since async Rust often depends on more complex language
features, such as lifetimes and pinning, you may encounter these types of
errors more frequently.</p>
<h3 id="runtime-errors"><a class="header" href="#runtime-errors">Runtime errors</a></h3>
<p>Whenever the compiler encounters an async function, it generates a state
machine under the hood. Stack traces in async Rust typically contain details
from these state machines, as well as function calls from
the runtime. As such, interpreting stack traces can be a bit more involved than
it would be in synchronous Rust.</p>
<h3 id="new-failure-modes"><a class="header" href="#new-failure-modes">New failure modes</a></h3>
<p>A few novel failure modes are possible in async Rust, for instance
if you call a blocking function from an async context or if you implement
the <code>Future</code> trait incorrectly. Such errors can silently pass both the
compiler and sometimes even unit tests. Having a firm understanding
of the underlying concepts, which this book aims to give you, can help you
avoid these pitfalls.</p>
<h2 id="compatibility-considerations"><a class="header" href="#compatibility-considerations">Compatibility considerations</a></h2>
<p>Asynchronous and synchronous code cannot always be combined freely.
For instance, you can't directly call an async function from a sync function.
Sync and async code also tend to promote different design patterns, which can
make it difficult to compose code intended for the different environments.</p>
<p>Even async code cannot always be combined freely. Some crates depend on a
specific async runtime to function. If so, it is usually specified in the
crate's dependency list.</p>
<p>These compatibility issues can limit your options, so make sure to
research which async runtime and what crates you may need early.
Once you have settled in with a runtime, you won't have to worry
much about compatibility.</p>
<h2 id="performance-characteristics"><a class="header" href="#performance-characteristics">Performance characteristics</a></h2>
<p>The performance of async Rust depends on the implementation of the
async runtime you're using.
Even though the runtimes that power async Rust applications are relatively new,
they perform exceptionally well for most practical workloads.</p>
<p>That said, most of the async ecosystem assumes a <em>multi-threaded</em> runtime.
This makes it difficult to enjoy the theoretical performance benefits
of single-threaded async applications, namely cheaper synchronization.
Another overlooked use-case is <em>latency sensitive tasks</em>, which are
important for drivers, GUI applications and so on. Such tasks depend
on runtime and/or OS support in order to be scheduled appropriately.
You can expect better library support for these use cases in the future.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="asyncawait-primer"><a class="header" href="#asyncawait-primer"><code>async</code>/<code>.await</code> Primer</a></h1>
<p><code>async</code>/<code>.await</code> is Rust's built-in tool for writing asynchronous functions
that look like synchronous code. <code>async</code> transforms a block of code into a
state machine that implements a trait called <code>Future</code>. Whereas calling a
blocking function in a synchronous method would block the whole thread,
blocked <code>Future</code>s will yield control of the thread, allowing other
<code>Future</code>s to run.</p>
<p>Let's add some dependencies to the <code>Cargo.toml</code> file:</p>
<pre><code class="language-toml">[dependencies]
futures = "0.3"
</code></pre>
<p>To create an asynchronous function, you can use the <code>async fn</code> syntax:</p>
<pre><pre class="playground"><code class="language-rust edition2018"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn do_something() { /* ... */ }
<span class="boring">}</span></code></pre></pre>
<p>The value returned by <code>async fn</code> is a <code>Future</code>. For anything to happen,
the <code>Future</code> needs to be run on an executor.</p>
<pre><pre class="playground"><code class="language-rust edition2018">// `block_on` blocks the current thread until the provided future has run to
// completion. Other executors provide more complex behavior, like scheduling
// multiple futures onto the same thread.
use futures::executor::block_on;

async fn hello_world() {
    println!("hello, world!");
}

fn main() {
    let future = hello_world(); // Nothing is printed
    block_on(future); // `future` is run and "hello, world!" is printed
}</code></pre></pre>
<p>Inside an <code>async fn</code>, you can use <code>.await</code> to wait for the completion of
another type that implements the <code>Future</code> trait, such as the output of
another <code>async fn</code>. Unlike <code>block_on</code>, <code>.await</code> doesn't block the current
thread, but instead asynchronously waits for the future to complete, allowing
other tasks to run if the future is currently unable to make progress.</p>
<p>For example, imagine that we have three <code>async fn</code>: <code>learn_song</code>, <code>sing_song</code>,
and <code>dance</code>:</p>
<pre><code class="language-rust ignore">async fn learn_song() -&gt; Song { /* ... */ }
async fn sing_song(song: Song) { /* ... */ }
async fn dance() { /* ... */ }</code></pre>
<p>One way to do learn, sing, and dance would be to block on each of these
individually:</p>
<pre><code class="language-rust ignore">fn main() {
    let song = block_on(learn_song());
    block_on(sing_song(song));
    block_on(dance());
}</code></pre>
<p>However, we're not giving the best performance possible this way—we're
only ever doing one thing at once! Clearly we have to learn the song before
we can sing it, but it's possible to dance at the same time as learning and
singing the song. To do this, we can create two separate <code>async fn</code> which
can be run concurrently:</p>
<pre><code class="language-rust ignore">async fn learn_and_sing() {
    // Wait until the song has been learned before singing it.
    // We use `.await` here rather than `block_on` to prevent blocking the
    // thread, which makes it possible to `dance` at the same time.
    let song = learn_song().await;
    sing_song(song).await;
}

async fn async_main() {
    let f1 = learn_and_sing();
    let f2 = dance();

    // `join!` is like `.await` but can wait for multiple futures concurrently.
    // If we're temporarily blocked in the `learn_and_sing` future, the `dance`
    // future will take over the current thread. If `dance` becomes blocked,
    // `learn_and_sing` can take back over. If both futures are blocked, then
    // `async_main` is blocked and will yield to the executor.
    futures::join!(f1, f2);
}

fn main() {
    block_on(async_main());
}</code></pre>
<p>In this example, learning the song must happen before singing the song, but
both learning and singing can happen at the same time as dancing. If we used
<code>block_on(learn_song())</code> rather than <code>learn_song().await</code> in <code>learn_and_sing</code>,
the thread wouldn't be able to do anything else while <code>learn_song</code> was running.
This would make it impossible to dance at the same time. By <code>.await</code>-ing
the <code>learn_song</code> future, we allow other tasks to take over the current thread
if <code>learn_song</code> is blocked. This makes it possible to run multiple futures
to completion concurrently on the same thread.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="under-the-hood-executing-futures-and-tasks"><a class="header" href="#under-the-hood-executing-futures-and-tasks">Under the Hood: Executing <code>Future</code>s and Tasks</a></h1>
<p>In this section, we'll cover the underlying structure of how <code>Future</code>s and
asynchronous tasks are scheduled. If you're only interested in learning
how to write higher-level code that uses existing <code>Future</code> types and aren't
interested in the details of how <code>Future</code> types work, you can skip ahead to
the <code>async</code>/<code>await</code> chapter. However, several of the topics discussed in this
chapter are useful for understanding how <code>async</code>/<code>await</code> code works,
understanding the runtime and performance properties of <code>async</code>/<code>await</code> code,
and building new asynchronous primitives. If you decide to skip this section
now, you may want to bookmark it to revisit in the future.</p>
<p>Now, with that out of the way, let's talk about the <code>Future</code> trait.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-future-trait"><a class="header" href="#the-future-trait">The <code>Future</code> Trait</a></h1>
<p>The <code>Future</code> trait is at the center of asynchronous programming in Rust.
A <code>Future</code> is an asynchronous computation that can produce a value
(although that value may be empty, e.g. <code>()</code>). A <em>simplified</em> version of
the future trait might look something like this:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>trait SimpleFuture {
    type Output;
    fn poll(&amp;mut self, wake: fn()) -&gt; Poll&lt;Self::Output&gt;;
}

enum Poll&lt;T&gt; {
    Ready(T),
    Pending,
}
<span class="boring">}</span></code></pre></pre>
<p>Futures can be advanced by calling the <code>poll</code> function, which will drive the
future as far towards completion as possible. If the future completes, it
returns <code>Poll::Ready(result)</code>. If the future is not able to complete yet, it
returns <code>Poll::Pending</code> and arranges for the <code>wake()</code> function to be called
when the <code>Future</code> is ready to make more progress. When <code>wake()</code> is called, the
executor driving the <code>Future</code> will call <code>poll</code> again so that the <code>Future</code> can
make more progress.</p>
<p>Without <code>wake()</code>, the executor would have no way of knowing when a particular
future could make progress, and would have to be constantly polling every
future. With <code>wake()</code>, the executor knows exactly which futures are ready to
be <code>poll</code>ed.</p>
<p>For example, consider the case where we want to read from a socket that may
or may not have data available already. If there is data, we can read it
in and return <code>Poll::Ready(data)</code>, but if no data is ready, our future is
blocked and can no longer make progress. When no data is available, we
must register <code>wake</code> to be called when data becomes ready on the socket,
which will tell the executor that our future is ready to make progress.
A simple <code>SocketRead</code> future might look something like this:</p>
<pre><code class="language-rust ignore">pub struct SocketRead&lt;'a&gt; {
    socket: &amp;'a Socket,
}

impl SimpleFuture for SocketRead&lt;'_&gt; {
    type Output = Vec&lt;u8&gt;;

    fn poll(&amp;mut self, wake: fn()) -&gt; Poll&lt;Self::Output&gt; {
        if self.socket.has_data_to_read() {
            // The socket has data -- read it into a buffer and return it.
            Poll::Ready(self.socket.read_buf())
        } else {
            // The socket does not yet have data.
            //
            // Arrange for `wake` to be called once data is available.
            // When data becomes available, `wake` will be called, and the
            // user of this `Future` will know to call `poll` again and
            // receive data.
            self.socket.set_readable_callback(wake);
            Poll::Pending
        }
    }
}</code></pre>
<p>This model of <code>Future</code>s allows for composing together multiple asynchronous
operations without needing intermediate allocations. Running multiple futures
at once or chaining futures together can be implemented via allocation-free
state machines, like this:</p>
<pre><code class="language-rust ignore">/// A SimpleFuture that runs two other futures to completion concurrently.
///
/// Concurrency is achieved via the fact that calls to `poll` each future
/// may be interleaved, allowing each future to advance itself at its own pace.
pub struct Join&lt;FutureA, FutureB&gt; {
    // Each field may contain a future that should be run to completion.
    // If the future has already completed, the field is set to `None`.
    // This prevents us from polling a future after it has completed, which
    // would violate the contract of the `Future` trait.
    a: Option&lt;FutureA&gt;,
    b: Option&lt;FutureB&gt;,
}

impl&lt;FutureA, FutureB&gt; SimpleFuture for Join&lt;FutureA, FutureB&gt;
where
    FutureA: SimpleFuture&lt;Output = ()&gt;,
    FutureB: SimpleFuture&lt;Output = ()&gt;,
{
    type Output = ();
    fn poll(&amp;mut self, wake: fn()) -&gt; Poll&lt;Self::Output&gt; {
        // Attempt to complete future `a`.
        if let Some(a) = &amp;mut self.a {
            if let Poll::Ready(()) = a.poll(wake) {
                self.a.take();
            }
        }

        // Attempt to complete future `b`.
        if let Some(b) = &amp;mut self.b {
            if let Poll::Ready(()) = b.poll(wake) {
                self.b.take();
            }
        }

        if self.a.is_none() &amp;&amp; self.b.is_none() {
            // Both futures have completed -- we can return successfully
            Poll::Ready(())
        } else {
            // One or both futures returned `Poll::Pending` and still have
            // work to do. They will call `wake()` when progress can be made.
            Poll::Pending
        }
    }
}</code></pre>
<p>This shows how multiple futures can be run simultaneously without needing
separate allocations, allowing for more efficient asynchronous programs.
Similarly, multiple sequential futures can be run one after another, like this:</p>
<pre><code class="language-rust ignore">/// A SimpleFuture that runs two futures to completion, one after another.
//
// Note: for the purposes of this simple example, `AndThenFut` assumes both
// the first and second futures are available at creation-time. The real
// `AndThen` combinator allows creating the second future based on the output
// of the first future, like `get_breakfast.and_then(|food| eat(food))`.
pub struct AndThenFut&lt;FutureA, FutureB&gt; {
    first: Option&lt;FutureA&gt;,
    second: FutureB,
}

impl&lt;FutureA, FutureB&gt; SimpleFuture for AndThenFut&lt;FutureA, FutureB&gt;
where
    FutureA: SimpleFuture&lt;Output = ()&gt;,
    FutureB: SimpleFuture&lt;Output = ()&gt;,
{
    type Output = ();
    fn poll(&amp;mut self, wake: fn()) -&gt; Poll&lt;Self::Output&gt; {
        if let Some(first) = &amp;mut self.first {
            match first.poll(wake) {
                // We've completed the first future -- remove it and start on
                // the second!
                Poll::Ready(()) =&gt; self.first.take(),
                // We couldn't yet complete the first future.
                // Notice that we disrupt the flow of the `poll` function with the `return` statement.
                Poll::Pending =&gt; return Poll::Pending,
            };
        }
        // Now that the first future is done, attempt to complete the second.
        self.second.poll(wake)
    }
}</code></pre>
<p>These examples show how the <code>Future</code> trait can be used to express asynchronous
control flow without requiring multiple allocated objects and deeply nested
callbacks. With the basic control-flow out of the way, let's talk about the
real <code>Future</code> trait and how it is different.</p>
<pre><code class="language-rust ignore">trait Future {
    type Output;
    fn poll(
        // Note the change from `&amp;mut self` to `Pin&lt;&amp;mut Self&gt;`:
        self: Pin&lt;&amp;mut Self&gt;,
        // and the change from `wake: fn()` to `cx: &amp;mut Context&lt;'_&gt;`:
        cx: &amp;mut Context&lt;'_&gt;,
    ) -&gt; Poll&lt;Self::Output&gt;;
}</code></pre>
<p>The first change you'll notice is that our <code>self</code> type is no longer <code>&amp;mut Self</code>,
but has changed to <code>Pin&lt;&amp;mut Self&gt;</code>. We'll talk more about pinning in <a href="02_execution/../04_pinning/01_chapter.html">a later
section</a>, but for now know that it allows us to create futures that
are immovable. Immovable objects can store pointers between their fields,
e.g. <code>struct MyFut { a: i32, ptr_to_a: *const i32 }</code>. Pinning is necessary
to enable async/await.</p>
<p>Secondly, <code>wake: fn()</code> has changed to <code>&amp;mut Context&lt;'_&gt;</code>. In <code>SimpleFuture</code>,
we used a call to a function pointer (<code>fn()</code>) to tell the future executor that
the future in question should be polled. However, since <code>fn()</code> is just a
function pointer, it can't store any data about <em>which</em> <code>Future</code> called <code>wake</code>.</p>
<p>In a real-world scenario, a complex application like a web server may have
thousands of different connections whose wakeups should all be
managed separately. The <code>Context</code> type solves this by providing access to
a value of type <code>Waker</code>, which can be used to wake up a specific task.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="task-wakeups-with-waker"><a class="header" href="#task-wakeups-with-waker">Task Wakeups with <code>Waker</code></a></h1>
<p>It's common that futures aren't able to complete the first time they are
<code>poll</code>ed. When this happens, the future needs to ensure that it is polled
again once it is ready to make more progress. This is done with the <code>Waker</code>
type.</p>
<p>Each time a future is polled, it is polled as part of a "task". Tasks are
the top-level futures that have been submitted to an executor.</p>
<p><code>Waker</code> provides a <code>wake()</code> method that can be used to tell the executor that
the associated task should be awoken. When <code>wake()</code> is called, the executor
knows that the task associated with the <code>Waker</code> is ready to make progress, and
its future should be polled again.</p>
<p><code>Waker</code> also implements <code>clone()</code> so that it can be copied around and stored.</p>
<p>Let's try implementing a simple timer future using <code>Waker</code>.</p>
<h2 id="applied-build-a-timer"><a class="header" href="#applied-build-a-timer">Applied: Build a Timer</a></h2>
<p>For the sake of the example, we'll just spin up a new thread when the timer
is created, sleep for the required time, and then signal the timer future
when the time window has elapsed.</p>
<p>First, start a new project with <code>cargo new --lib timer_future</code> and add the imports
we'll need to get started to <code>src/lib.rs</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::{
    future::Future,
    pin::Pin,
    sync::{Arc, Mutex},
    task::{Context, Poll, Waker},
    thread,
    time::Duration,
};
<span class="boring">}</span></code></pre></pre>
<p>Let's start by defining the future type itself. Our future needs a way for the
thread to communicate that the timer has elapsed and the future should complete.
We'll use a shared <code>Arc&lt;Mutex&lt;..&gt;&gt;</code> value to communicate between the thread and
the future.</p>
<pre><code class="language-rust ignore">pub struct TimerFuture {
    shared_state: Arc&lt;Mutex&lt;SharedState&gt;&gt;,
}

/// Shared state between the future and the waiting thread
struct SharedState {
    /// Whether or not the sleep time has elapsed
    completed: bool,

    /// The waker for the task that `TimerFuture` is running on.
    /// The thread can use this after setting `completed = true` to tell
    /// `TimerFuture`'s task to wake up, see that `completed = true`, and
    /// move forward.
    waker: Option&lt;Waker&gt;,
}</code></pre>
<p>Now, let's actually write the <code>Future</code> implementation!</p>
<pre><code class="language-rust ignore">impl Future for TimerFuture {
    type Output = ();
    fn poll(self: Pin&lt;&amp;mut Self&gt;, cx: &amp;mut Context&lt;'_&gt;) -&gt; Poll&lt;Self::Output&gt; {
        // Look at the shared state to see if the timer has already completed.
        let mut shared_state = self.shared_state.lock().unwrap();
        if shared_state.completed {
            Poll::Ready(())
        } else {
            // Set waker so that the thread can wake up the current task
            // when the timer has completed, ensuring that the future is polled
            // again and sees that `completed = true`.
            //
            // It's tempting to do this once rather than repeatedly cloning
            // the waker each time. However, the `TimerFuture` can move between
            // tasks on the executor, which could cause a stale waker pointing
            // to the wrong task, preventing `TimerFuture` from waking up
            // correctly.
            //
            // N.B. it's possible to check for this using the `Waker::will_wake`
            // function, but we omit that here to keep things simple.
            shared_state.waker = Some(cx.waker().clone());
            Poll::Pending
        }
    }
}</code></pre>
<p>Pretty simple, right? If the thread has set <code>shared_state.completed = true</code>,
we're done! Otherwise, we clone the <code>Waker</code> for the current task and pass it to
<code>shared_state.waker</code> so that the thread can wake the task back up.</p>
<p>Importantly, we have to update the <code>Waker</code> every time the future is polled
because the future may have moved to a different task with a different
<code>Waker</code>. This will happen when futures are passed around between tasks after
being polled.</p>
<p>Finally, we need the API to actually construct the timer and start the thread:</p>
<pre><code class="language-rust ignore">impl TimerFuture {
    /// Create a new `TimerFuture` which will complete after the provided
    /// timeout.
    pub fn new(duration: Duration) -&gt; Self {
        let shared_state = Arc::new(Mutex::new(SharedState {
            completed: false,
            waker: None,
        }));

        // Spawn the new thread
        let thread_shared_state = shared_state.clone();
        thread::spawn(move || {
            thread::sleep(duration);
            let mut shared_state = thread_shared_state.lock().unwrap();
            // Signal that the timer has completed and wake up the last
            // task on which the future was polled, if one exists.
            shared_state.completed = true;
            if let Some(waker) = shared_state.waker.take() {
                waker.wake()
            }
        });

        TimerFuture { shared_state }
    }
}</code></pre>
<p>Woot! That's all we need to build a simple timer future. Now, if only we had
an executor to run the future on...</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="applied-build-an-executor"><a class="header" href="#applied-build-an-executor">Applied: Build an Executor</a></h1>
<p>Rust's <code>Future</code>s are lazy: they won't do anything unless actively driven to
completion. One way to drive a future to completion is to <code>.await</code> it inside
an <code>async</code> function, but that just pushes the problem one level up: who will
run the futures returned from the top-level <code>async</code> functions? The answer is
that we need a <code>Future</code> executor.</p>
<p><code>Future</code> executors take a set of top-level <code>Future</code>s and run them to completion
by calling <code>poll</code> whenever the <code>Future</code> can make progress. Typically, an
executor will <code>poll</code> a future once to start off. When <code>Future</code>s indicate that
they are ready to make progress by calling <code>wake()</code>, they are placed back
onto a queue and <code>poll</code> is called again, repeating until the <code>Future</code> has
completed.</p>
<p>In this section, we'll write our own simple executor capable of running a large
number of top-level futures to completion concurrently.</p>
<p>For this example, we depend on the <code>futures</code> crate for the <code>ArcWake</code> trait,
which provides an easy way to construct a <code>Waker</code>. Edit <code>Cargo.toml</code> to add
a new dependency:</p>
<pre><code class="language-toml">[package]
name = "timer_future"
version = "0.1.0"
authors = ["XYZ Author"]
edition = "2021"

[dependencies]
futures = "0.3"
</code></pre>
<p>Next, we need the following imports at the top of <code>src/main.rs</code>:</p>
<pre><code class="language-rust ignore">use futures::{
    future::{BoxFuture, FutureExt},
    task::{waker_ref, ArcWake},
};
use std::{
    future::Future,
    sync::mpsc::{sync_channel, Receiver, SyncSender},
    sync::{Arc, Mutex},
    task::Context,
    time::Duration,
};
// The timer we wrote in the previous section:
use timer_future::TimerFuture;</code></pre>
<p>Our executor will work by sending tasks to run over a channel. The executor
will pull events off of the channel and run them. When a task is ready to
do more work (is awoken), it can schedule itself to be polled again by
putting itself back onto the channel.</p>
<p>In this design, the executor itself just needs the receiving end of the task
channel. The user will get a sending end so that they can spawn new futures.
Tasks themselves are just futures that can reschedule themselves, so we'll
store them as a future paired with a sender that the task can use to requeue
itself.</p>
<pre><code class="language-rust ignore">/// Task executor that receives tasks off of a channel and runs them.
struct Executor {
    ready_queue: Receiver&lt;Arc&lt;Task&gt;&gt;,
}

/// `Spawner` spawns new futures onto the task channel.
#[derive(Clone)]
struct Spawner {
    task_sender: SyncSender&lt;Arc&lt;Task&gt;&gt;,
}

/// A future that can reschedule itself to be polled by an `Executor`.
struct Task {
    /// In-progress future that should be pushed to completion.
    ///
    /// The `Mutex` is not necessary for correctness, since we only have
    /// one thread executing tasks at once. However, Rust isn't smart
    /// enough to know that `future` is only mutated from one thread,
    /// so we need to use the `Mutex` to prove thread-safety. A production
    /// executor would not need this, and could use `UnsafeCell` instead.
    future: Mutex&lt;Option&lt;BoxFuture&lt;'static, ()&gt;&gt;&gt;,

    /// Handle to place the task itself back onto the task queue.
    task_sender: SyncSender&lt;Arc&lt;Task&gt;&gt;,
}

fn new_executor_and_spawner() -&gt; (Executor, Spawner) {
    // Maximum number of tasks to allow queueing in the channel at once.
    // This is just to make `sync_channel` happy, and wouldn't be present in
    // a real executor.
    const MAX_QUEUED_TASKS: usize = 10_000;
    let (task_sender, ready_queue) = sync_channel(MAX_QUEUED_TASKS);
    (Executor { ready_queue }, Spawner { task_sender })
}</code></pre>
<p>Let's also add a method to spawner to make it easy to spawn new futures.
This method will take a future type, box it, and create a new <code>Arc&lt;Task&gt;</code> with
it inside which can be enqueued onto the executor.</p>
<pre><code class="language-rust ignore">impl Spawner {
    fn spawn(&amp;self, future: impl Future&lt;Output = ()&gt; + 'static + Send) {
        let future = future.boxed();
        let task = Arc::new(Task {
            future: Mutex::new(Some(future)),
            task_sender: self.task_sender.clone(),
        });
        self.task_sender.try_send(task).expect("too many tasks queued");
    }
}</code></pre>
<p>To poll futures, we'll need to create a <code>Waker</code>.
As discussed in the <a href="02_execution/./03_wakeups.html">task wakeups section</a>, <code>Waker</code>s are responsible
for scheduling a task to be polled again once <code>wake</code> is called. Remember that
<code>Waker</code>s tell the executor exactly which task has become ready, allowing
them to poll just the futures that are ready to make progress. The easiest way
to create a new <code>Waker</code> is by implementing the <code>ArcWake</code> trait and then using
the <code>waker_ref</code> or <code>.into_waker()</code> functions to turn an <code>Arc&lt;impl ArcWake&gt;</code>
into a <code>Waker</code>. Let's implement <code>ArcWake</code> for our tasks to allow them to be
turned into <code>Waker</code>s and awoken:</p>
<pre><code class="language-rust ignore">impl ArcWake for Task {
    fn wake_by_ref(arc_self: &amp;Arc&lt;Self&gt;) {
        // Implement `wake` by sending this task back onto the task channel
        // so that it will be polled again by the executor.
        let cloned = arc_self.clone();
        arc_self
            .task_sender
            .try_send(cloned)
            .expect("too many tasks queued");
    }
}</code></pre>
<p>When a <code>Waker</code> is created from an <code>Arc&lt;Task&gt;</code>, calling <code>wake()</code> on it will
cause a copy of the <code>Arc</code> to be sent onto the task channel. Our executor then
needs to pick up the task and poll it. Let's implement that:</p>
<pre><code class="language-rust ignore">impl Executor {
    fn run(&amp;self) {
        while let Ok(task) = self.ready_queue.recv() {
            // Take the future, and if it has not yet completed (is still Some),
            // poll it in an attempt to complete it.
            let mut future_slot = task.future.lock().unwrap();
            if let Some(mut future) = future_slot.take() {
                // Create a `LocalWaker` from the task itself
                let waker = waker_ref(&amp;task);
                let context = &amp;mut Context::from_waker(&amp;waker);
                // `BoxFuture&lt;T&gt;` is a type alias for
                // `Pin&lt;Box&lt;dyn Future&lt;Output = T&gt; + Send + 'static&gt;&gt;`.
                // We can get a `Pin&lt;&amp;mut dyn Future + Send + 'static&gt;`
                // from it by calling the `Pin::as_mut` method.
                if future.as_mut().poll(context).is_pending() {
                    // We're not done processing the future, so put it
                    // back in its task to be run again in the future.
                    *future_slot = Some(future);
                }
            }
        }
    }
}</code></pre>
<p>Congratulations! We now have a working futures executor. We can even use it
to run <code>async/.await</code> code and custom futures, such as the <code>TimerFuture</code> we
wrote earlier:</p>
<pre><code class="language-rust edition2018 ignore">fn main() {
    let (executor, spawner) = new_executor_and_spawner();

    // Spawn a task to print before and after waiting on a timer.
    spawner.spawn(async {
        println!("howdy!");
        // Wait for our timer future to complete after two seconds.
        TimerFuture::new(Duration::new(2, 0)).await;
        println!("done!");
    });

    // Drop the spawner so that our executor knows it is finished and won't
    // receive more incoming tasks to run.
    drop(spawner);

    // Run the executor until the task queue is empty.
    // This will print "howdy!", pause, and then print "done!".
    executor.run();
}</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="executors-and-system-io"><a class="header" href="#executors-and-system-io">Executors and System IO</a></h1>
<p>In the previous section on <a href="02_execution/./02_future.html">The <code>Future</code> Trait</a>, we discussed this example of
a future that performed an asynchronous read on a socket:</p>
<pre><code class="language-rust ignore">pub struct SocketRead&lt;'a&gt; {
    socket: &amp;'a Socket,
}

impl SimpleFuture for SocketRead&lt;'_&gt; {
    type Output = Vec&lt;u8&gt;;

    fn poll(&amp;mut self, wake: fn()) -&gt; Poll&lt;Self::Output&gt; {
        if self.socket.has_data_to_read() {
            // The socket has data -- read it into a buffer and return it.
            Poll::Ready(self.socket.read_buf())
        } else {
            // The socket does not yet have data.
            //
            // Arrange for `wake` to be called once data is available.
            // When data becomes available, `wake` will be called, and the
            // user of this `Future` will know to call `poll` again and
            // receive data.
            self.socket.set_readable_callback(wake);
            Poll::Pending
        }
    }
}</code></pre>
<p>This future will read available data on a socket, and if no data is available,
it will yield to the executor, requesting that its task be awoken when the
socket becomes readable again. However, it's not clear from this example how
the <code>Socket</code> type is implemented, and in particular it isn't obvious how the
<code>set_readable_callback</code> function works. How can we arrange for <code>wake()</code>
to be called once the socket becomes readable? One option would be to have
a thread that continually checks whether <code>socket</code> is readable, calling
<code>wake()</code> when appropriate. However, this would be quite inefficient, requiring
a separate thread for each blocked IO future. This would greatly reduce the
efficiency of our async code.</p>
<p>In practice, this problem is solved through integration with an IO-aware
system blocking primitive, such as <code>epoll</code> on Linux, <code>kqueue</code> on FreeBSD and
Mac OS, IOCP on Windows, and <code>port</code>s on Fuchsia (all of which are exposed
through the cross-platform Rust crate <a href="https://github.com/tokio-rs/mio"><code>mio</code></a>). These primitives all allow
a thread to block on multiple asynchronous IO events, returning once one of
the events completes. In practice, these APIs usually look something like
this:</p>
<pre><code class="language-rust ignore">struct IoBlocker {
    /* ... */
}

struct Event {
    // An ID uniquely identifying the event that occurred and was listened for.
    id: usize,

    // A set of signals to wait for, or which occurred.
    signals: Signals,
}

impl IoBlocker {
    /// Create a new collection of asynchronous IO events to block on.
    fn new() -&gt; Self { /* ... */ }

    /// Express an interest in a particular IO event.
    fn add_io_event_interest(
        &amp;self,

        /// The object on which the event will occur
        io_object: &amp;IoObject,

        /// A set of signals that may appear on the `io_object` for
        /// which an event should be triggered, paired with
        /// an ID to give to events that result from this interest.
        event: Event,
    ) { /* ... */ }

    /// Block until one of the events occurs.
    fn block(&amp;self) -&gt; Event { /* ... */ }
}

let mut io_blocker = IoBlocker::new();
io_blocker.add_io_event_interest(
    &amp;socket_1,
    Event { id: 1, signals: READABLE },
);
io_blocker.add_io_event_interest(
    &amp;socket_2,
    Event { id: 2, signals: READABLE | WRITABLE },
);
let event = io_blocker.block();

// prints e.g. "Socket 1 is now READABLE" if socket one became readable.
println!("Socket {:?} is now {:?}", event.id, event.signals);</code></pre>
<p>Futures executors can use these primitives to provide asynchronous IO objects
such as sockets that can configure callbacks to be run when a particular IO
event occurs. In the case of our <code>SocketRead</code> example above, the
<code>Socket::set_readable_callback</code> function might look like the following pseudocode:</p>
<pre><code class="language-rust ignore">impl Socket {
    fn set_readable_callback(&amp;self, waker: Waker) {
        // `local_executor` is a reference to the local executor.
        // This could be provided at creation of the socket, but in practice
        // many executor implementations pass it down through thread local
        // storage for convenience.
        let local_executor = self.local_executor;

        // Unique ID for this IO object.
        let id = self.id;

        // Store the local waker in the executor's map so that it can be called
        // once the IO event arrives.
        local_executor.event_map.insert(id, waker);
        local_executor.add_io_event_interest(
            &amp;self.socket_file_descriptor,
            Event { id, signals: READABLE },
        );
    }
}</code></pre>
<p>We can now have just one executor thread which can receive and dispatch any
IO event to the appropriate <code>Waker</code>, which will wake up the corresponding
task, allowing the executor to drive more tasks to completion before returning
to check for more IO events (and the cycle continues...).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="asyncawait"><a class="header" href="#asyncawait"><code>async</code>/<code>.await</code></a></h1>
<p>In <a href="03_async_await/../01_getting_started/04_async_await_primer.html">the first chapter</a>, we took a brief look at <code>async</code>/<code>.await</code>.
This chapter will discuss <code>async</code>/<code>.await</code> in
greater detail, explaining how it works and how <code>async</code> code differs from
traditional Rust programs.</p>
<p><code>async</code>/<code>.await</code> are special pieces of Rust syntax that make it possible to
yield control of the current thread rather than blocking, allowing other
code to make progress while waiting on an operation to complete.</p>
<p>There are two main ways to use <code>async</code>: <code>async fn</code> and <code>async</code> blocks.
Each returns a value that implements the <code>Future</code> trait:</p>
<pre><code class="language-rust edition2018 ignore">
// `foo()` returns a type that implements `Future&lt;Output = u8&gt;`.
// `foo().await` will result in a value of type `u8`.
async fn foo() -&gt; u8 { 5 }

fn bar() -&gt; impl Future&lt;Output = u8&gt; {
    // This `async` block results in a type that implements
    // `Future&lt;Output = u8&gt;`.
    async {
        let x: u8 = foo().await;
        x + 5
    }
}</code></pre>
<p>As we saw in the first chapter, <code>async</code> bodies and other futures are lazy:
they do nothing until they are run. The most common way to run a <code>Future</code>
is to <code>.await</code> it. When <code>.await</code> is called on a <code>Future</code>, it will attempt
to run it to completion. If the <code>Future</code> is blocked, it will yield control
of the current thread. When more progress can be made, the <code>Future</code> will be picked
up by the executor and will resume running, allowing the <code>.await</code> to resolve.</p>
<h2 id="async-lifetimes"><a class="header" href="#async-lifetimes"><code>async</code> Lifetimes</a></h2>
<p>Unlike traditional functions, <code>async fn</code>s which take references or other
non-<code>'static</code> arguments return a <code>Future</code> which is bounded by the lifetime of
the arguments:</p>
<pre><code class="language-rust edition2018 ignore">// This function:
async fn foo(x: &amp;u8) -&gt; u8 { *x }

// Is equivalent to this function:
fn foo_expanded&lt;'a&gt;(x: &amp;'a u8) -&gt; impl Future&lt;Output = u8&gt; + 'a {
    async move { *x }
}</code></pre>
<p>This means that the future returned from an <code>async fn</code> must be <code>.await</code>ed
while its non-<code>'static</code> arguments are still valid. In the common
case of <code>.await</code>ing the future immediately after calling the function
(as in <code>foo(&amp;x).await</code>) this is not an issue. However, if storing the future
or sending it over to another task or thread, this may be an issue.</p>
<p>One common workaround for turning an <code>async fn</code> with references-as-arguments
into a <code>'static</code> future is to bundle the arguments with the call to the
<code>async fn</code> inside an <code>async</code> block:</p>
<pre><code class="language-rust edition2018 ignore">fn bad() -&gt; impl Future&lt;Output = u8&gt; {
    let x = 5;
    borrow_x(&amp;x) // ERROR: `x` does not live long enough
}

fn good() -&gt; impl Future&lt;Output = u8&gt; {
    async {
        let x = 5;
        borrow_x(&amp;x).await
    }
}</code></pre>
<p>By moving the argument into the <code>async</code> block, we extend its lifetime to match
that of the <code>Future</code> returned from the call to <code>good</code>.</p>
<h2 id="async-move"><a class="header" href="#async-move"><code>async move</code></a></h2>
<p><code>async</code> blocks and closures allow the <code>move</code> keyword, much like normal
closures. An <code>async move</code> block will take ownership of the variables it
references, allowing it to outlive the current scope, but giving up the ability
to share those variables with other code:</p>
<pre><code class="language-rust edition2018 ignore">/// `async` block:
///
/// Multiple different `async` blocks can access the same local variable
/// so long as they're executed within the variable's scope
async fn blocks() {
    let my_string = "foo".to_string();

    let future_one = async {
        // ...
        println!("{my_string}");
    };

    let future_two = async {
        // ...
        println!("{my_string}");
    };

    // Run both futures to completion, printing "foo" twice:
    let ((), ()) = futures::join!(future_one, future_two);
}

/// `async move` block:
///
/// Only one `async move` block can access the same captured variable, since
/// captures are moved into the `Future` generated by the `async move` block.
/// However, this allows the `Future` to outlive the original scope of the
/// variable:
fn move_block() -&gt; impl Future&lt;Output = ()&gt; {
    let my_string = "foo".to_string();
    async move {
        // ...
        println!("{my_string}");
    }
}</code></pre>
<h2 id="awaiting-on-a-multithreaded-executor"><a class="header" href="#awaiting-on-a-multithreaded-executor"><code>.await</code>ing on a Multithreaded Executor</a></h2>
<p>Note that, when using a multithreaded <code>Future</code> executor, a <code>Future</code> may move
between threads, so any variables used in <code>async</code> bodies must be able to travel
between threads, as any <code>.await</code> can potentially result in a switch to a new
thread.</p>
<p>This means that it is not safe to use <code>Rc</code>, <code>&amp;RefCell</code> or any other types
that don't implement the <code>Send</code> trait, including references to types that don't
implement the <code>Sync</code> trait.</p>
<p>(Caveat: it is possible to use these types as long as they aren't in scope
during a call to <code>.await</code>.)</p>
<p>Similarly, it isn't a good idea to hold a traditional non-futures-aware lock
across an <code>.await</code>, as it can cause the threadpool to lock up: one task could
take out a lock, <code>.await</code> and yield to the executor, allowing another task to
attempt to take the lock and cause a deadlock. To avoid this, use the <code>Mutex</code>
in <code>futures::lock</code> rather than the one from <code>std::sync</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pinning"><a class="header" href="#pinning">Pinning</a></h1>
<p>To poll futures, they must be pinned using a special type called
<code>Pin&lt;T&gt;</code>. If you read the explanation of <a href="04_pinning/../02_execution/02_future.html">the <code>Future</code> trait</a> in the
previous section <a href="04_pinning/../02_execution/01_chapter.html">"Executing <code>Future</code>s and Tasks"</a>, you'll recognize
<code>Pin</code> from the <code>self: Pin&lt;&amp;mut Self&gt;</code> in the <code>Future::poll</code> method's definition.
But what does it mean, and why do we need it?</p>
<h2 id="why-pinning"><a class="header" href="#why-pinning">Why Pinning</a></h2>
<p><code>Pin</code> works in tandem with the <code>Unpin</code> marker. Pinning makes it possible
to guarantee that an object implementing <code>!Unpin</code> won't ever be moved. To understand
why this is necessary, we need to remember how <code>async</code>/<code>.await</code> works. Consider
the following code:</p>
<pre><code class="language-rust edition2018 ignore">let fut_one = /* ... */;
let fut_two = /* ... */;
async move {
    fut_one.await;
    fut_two.await;
}</code></pre>
<p>Under the hood, this creates an anonymous type that implements <code>Future</code>,
providing a <code>poll</code> method that looks something like this:</p>
<pre><code class="language-rust ignore">// The `Future` type generated by our `async { ... }` block
struct AsyncFuture {
    fut_one: FutOne,
    fut_two: FutTwo,
    state: State,
}

// List of states our `async` block can be in
enum State {
    AwaitingFutOne,
    AwaitingFutTwo,
    Done,
}

impl Future for AsyncFuture {
    type Output = ();

    fn poll(mut self: Pin&lt;&amp;mut Self&gt;, cx: &amp;mut Context&lt;'_&gt;) -&gt; Poll&lt;()&gt; {
        loop {
            match self.state {
                State::AwaitingFutOne =&gt; match self.fut_one.poll(..) {
                    Poll::Ready(()) =&gt; self.state = State::AwaitingFutTwo,
                    Poll::Pending =&gt; return Poll::Pending,
                }
                State::AwaitingFutTwo =&gt; match self.fut_two.poll(..) {
                    Poll::Ready(()) =&gt; self.state = State::Done,
                    Poll::Pending =&gt; return Poll::Pending,
                }
                State::Done =&gt; return Poll::Ready(()),
            }
        }
    }
}</code></pre>
<p>When <code>poll</code> is first called, it will poll <code>fut_one</code>. If <code>fut_one</code> can't
complete, <code>AsyncFuture::poll</code> will return. Future calls to <code>poll</code> will pick
up where the previous one left off. This process continues until the future
is able to successfully complete.</p>
<p>However, what happens if we have an <code>async</code> block that uses references?
For example:</p>
<pre><code class="language-rust edition2018 ignore">async {
    let mut x = [0; 128];
    let read_into_buf_fut = read_into_buf(&amp;mut x);
    read_into_buf_fut.await;
    println!("{:?}", x);
}</code></pre>
<p>What struct does this compile down to?</p>
<pre><code class="language-rust ignore">struct ReadIntoBuf&lt;'a&gt; {
    buf: &amp;'a mut [u8], // points to `x` below
}

struct AsyncFuture {
    x: [u8; 128],
    read_into_buf_fut: ReadIntoBuf&lt;'what_lifetime?&gt;,
}</code></pre>
<p>Here, the <code>ReadIntoBuf</code> future holds a reference into the other field of our
structure, <code>x</code>. However, if <code>AsyncFuture</code> is moved, the location of <code>x</code> will
move as well, invalidating the pointer stored in <code>read_into_buf_fut.buf</code>.</p>
<p>Pinning futures to a particular spot in memory prevents this problem, making
it safe to create references to values inside an <code>async</code> block.</p>
<h2 id="pinning-in-detail"><a class="header" href="#pinning-in-detail">Pinning in Detail</a></h2>
<p>Let's try to understand pinning by using a slightly simpler example. The problem we encounter
above is a problem that ultimately boils down to how we handle references in self-referential
types in Rust.</p>
<p>For now our example will look like this:</p>
<pre><code class="language-rust  ignore">#[derive(Debug)]
struct Test {
    a: String,
    b: *const String,
}

impl Test {
    fn new(txt: &amp;str) -&gt; Self {
        Test {
            a: String::from(txt),
            b: std::ptr::null(),
        }
    }

    fn init(&amp;mut self) {
        let self_ref: *const String = &amp;self.a;
        self.b = self_ref;
    }

    fn a(&amp;self) -&gt; &amp;str {
        &amp;self.a
    }

    fn b(&amp;self) -&gt; &amp;String {
        assert!(!self.b.is_null(), "Test::b called without Test::init being called first");
        unsafe { &amp;*(self.b) }
    }
}</code></pre>
<p><code>Test</code> provides methods to get a reference to the value of the fields <code>a</code> and <code>b</code>. Since <code>b</code> is a
reference to <code>a</code> we store it as a pointer since the borrowing rules of Rust don't allow us to
define this lifetime. We now have what we call a self-referential struct.</p>
<p>Our example works fine if we don't move any of our data around as you can observe by running
this example:</p>
<pre><pre class="playground"><code class="language-rust">fn main() {
    let mut test1 = Test::new("test1");
    test1.init();
    let mut test2 = Test::new("test2");
    test2.init();

    println!("a: {}, b: {}", test1.a(), test1.b());
    println!("a: {}, b: {}", test2.a(), test2.b());

}
<span class="boring">#[derive(Debug)]
</span><span class="boring">struct Test {
</span><span class="boring">    a: String,
</span><span class="boring">    b: *const String,
</span><span class="boring">}
</span><span class="boring">
</span><span class="boring">impl Test {
</span><span class="boring">    fn new(txt: &amp;str) -&gt; Self {
</span><span class="boring">        Test {
</span><span class="boring">            a: String::from(txt),
</span><span class="boring">            b: std::ptr::null(),
</span><span class="boring">        }
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    // We need an `init` method to actually set our self-reference
</span><span class="boring">    fn init(&amp;mut self) {
</span><span class="boring">        let self_ref: *const String = &amp;self.a;
</span><span class="boring">        self.b = self_ref;
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn a(&amp;self) -&gt; &amp;str {
</span><span class="boring">        &amp;self.a
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn b(&amp;self) -&gt; &amp;String {
</span><span class="boring">        assert!(!self.b.is_null(), "Test::b called without Test::init being called first");
</span><span class="boring">        unsafe { &amp;*(self.b) }
</span><span class="boring">    }
</span><span class="boring">}</span></code></pre></pre>
<p>We get what we'd expect:</p>
<pre><code class="language-rust  ignore">a: test1, b: test1
a: test2, b: test2</code></pre>
<p>Let's see what happens if we swap <code>test1</code> with <code>test2</code> and thereby move the data:</p>
<pre><pre class="playground"><code class="language-rust">fn main() {
    let mut test1 = Test::new("test1");
    test1.init();
    let mut test2 = Test::new("test2");
    test2.init();

    println!("a: {}, b: {}", test1.a(), test1.b());
    std::mem::swap(&amp;mut test1, &amp;mut test2);
    println!("a: {}, b: {}", test2.a(), test2.b());

}
<span class="boring">#[derive(Debug)]
</span><span class="boring">struct Test {
</span><span class="boring">    a: String,
</span><span class="boring">    b: *const String,
</span><span class="boring">}
</span><span class="boring">
</span><span class="boring">impl Test {
</span><span class="boring">    fn new(txt: &amp;str) -&gt; Self {
</span><span class="boring">        Test {
</span><span class="boring">            a: String::from(txt),
</span><span class="boring">            b: std::ptr::null(),
</span><span class="boring">        }
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn init(&amp;mut self) {
</span><span class="boring">        let self_ref: *const String = &amp;self.a;
</span><span class="boring">        self.b = self_ref;
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn a(&amp;self) -&gt; &amp;str {
</span><span class="boring">        &amp;self.a
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn b(&amp;self) -&gt; &amp;String {
</span><span class="boring">        assert!(!self.b.is_null(), "Test::b called without Test::init being called first");
</span><span class="boring">        unsafe { &amp;*(self.b) }
</span><span class="boring">    }
</span><span class="boring">}</span></code></pre></pre>
<p>Naively, we could think that what we should get a debug print of <code>test1</code> two times like this:</p>
<pre><code class="language-rust  ignore">a: test1, b: test1
a: test1, b: test1</code></pre>
<p>But instead we get:</p>
<pre><code class="language-rust  ignore">a: test1, b: test1
a: test1, b: test2</code></pre>
<p>The pointer to <code>test2.b</code> still points to the old location which is inside <code>test1</code>
now. The struct is not self-referential anymore, it holds a pointer to a field
in a different object. That means we can't rely on the lifetime of <code>test2.b</code> to
be tied to the lifetime of <code>test2</code> anymore.</p>
<p>If you're still not convinced, this should at least convince you:</p>
<pre><pre class="playground"><code class="language-rust">fn main() {
    let mut test1 = Test::new("test1");
    test1.init();
    let mut test2 = Test::new("test2");
    test2.init();

    println!("a: {}, b: {}", test1.a(), test1.b());
    std::mem::swap(&amp;mut test1, &amp;mut test2);
    test1.a = "I've totally changed now!".to_string();
    println!("a: {}, b: {}", test2.a(), test2.b());

}
<span class="boring">#[derive(Debug)]
</span><span class="boring">struct Test {
</span><span class="boring">    a: String,
</span><span class="boring">    b: *const String,
</span><span class="boring">}
</span><span class="boring">
</span><span class="boring">impl Test {
</span><span class="boring">    fn new(txt: &amp;str) -&gt; Self {
</span><span class="boring">        Test {
</span><span class="boring">            a: String::from(txt),
</span><span class="boring">            b: std::ptr::null(),
</span><span class="boring">        }
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn init(&amp;mut self) {
</span><span class="boring">        let self_ref: *const String = &amp;self.a;
</span><span class="boring">        self.b = self_ref;
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn a(&amp;self) -&gt; &amp;str {
</span><span class="boring">        &amp;self.a
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn b(&amp;self) -&gt; &amp;String {
</span><span class="boring">        assert!(!self.b.is_null(), "Test::b called without Test::init being called first");
</span><span class="boring">        unsafe { &amp;*(self.b) }
</span><span class="boring">    }
</span><span class="boring">}</span></code></pre></pre>
<p>The diagram below can help visualize what's going on:</p>
<p><strong>Fig 1: Before and after swap</strong>
<img src="04_pinning/../assets/swap_problem.jpg" alt="swap_problem" /></p>
<p>It's easy to get this to show undefined behavior and fail in other spectacular ways as well.</p>
<h2 id="pinning-in-practice"><a class="header" href="#pinning-in-practice">Pinning in Practice</a></h2>
<p>Let's see how pinning and the <code>Pin</code> type can help us solve this problem.</p>
<p>The <code>Pin</code> type wraps pointer types, guaranteeing that the values behind the
pointer won't be moved if it is not implementing <code>Unpin</code>. For example, <code>Pin&lt;&amp;mut T&gt;</code>, <code>Pin&lt;&amp;T&gt;</code>, <code>Pin&lt;Box&lt;T&gt;&gt;</code> all guarantee that <code>T</code> won't be moved if <code>T: !Unpin</code>.</p>
<p>Most types don't have a problem being moved. These types implement a trait
called <code>Unpin</code>. Pointers to <code>Unpin</code> types can be freely placed into or taken
out of <code>Pin</code>. For example, <code>u8</code> is <code>Unpin</code>, so <code>Pin&lt;&amp;mut u8&gt;</code> behaves just like
a normal <code>&amp;mut u8</code>.</p>
<p>However, types that can't be moved after they're pinned have a marker called
<code>!Unpin</code>. Futures created by async/await are an example of this.</p>
<h3 id="pinning-to-the-stack"><a class="header" href="#pinning-to-the-stack">Pinning to the Stack</a></h3>
<p>Back to our example. We can solve our problem by using <code>Pin</code>. Let's take a look at what
our example would look like if we required a pinned pointer instead:</p>
<pre><code class="language-rust  ignore">use std::pin::Pin;
use std::marker::PhantomPinned;

#[derive(Debug)]
struct Test {
    a: String,
    b: *const String,
    _marker: PhantomPinned,
}


impl Test {
    fn new(txt: &amp;str) -&gt; Self {
        Test {
            a: String::from(txt),
            b: std::ptr::null(),
            _marker: PhantomPinned, // This makes our type `!Unpin`
        }
    }

    fn init(self: Pin&lt;&amp;mut Self&gt;) {
        let self_ptr: *const String = &amp;self.a;
        let this = unsafe { self.get_unchecked_mut() };
        this.b = self_ptr;
    }

    fn a(self: Pin&lt;&amp;Self&gt;) -&gt; &amp;str {
        &amp;self.get_ref().a
    }

    fn b(self: Pin&lt;&amp;Self&gt;) -&gt; &amp;String {
        assert!(!self.b.is_null(), "Test::b called without Test::init being called first");
        unsafe { &amp;*(self.b) }
    }
}</code></pre>
<p>Pinning an object to the stack will always be <code>unsafe</code> if our type implements
<code>!Unpin</code>. You can use a crate like <a href="https://docs.rs/pin-utils/"><code>pin_utils</code></a> to avoid writing
our own <code>unsafe</code> code when pinning to the stack.</p>
<p>Below, we pin the objects <code>test1</code> and <code>test2</code> to the stack:</p>
<pre><pre class="playground"><code class="language-rust">pub fn main() {
    // test1 is safe to move before we initialize it
    let mut test1 = Test::new("test1");
    // Notice how we shadow `test1` to prevent it from being accessed again
    let mut test1 = unsafe { Pin::new_unchecked(&amp;mut test1) };
    Test::init(test1.as_mut());

    let mut test2 = Test::new("test2");
    let mut test2 = unsafe { Pin::new_unchecked(&amp;mut test2) };
    Test::init(test2.as_mut());

    println!("a: {}, b: {}", Test::a(test1.as_ref()), Test::b(test1.as_ref()));
    println!("a: {}, b: {}", Test::a(test2.as_ref()), Test::b(test2.as_ref()));
}
<span class="boring">use std::pin::Pin;
</span><span class="boring">use std::marker::PhantomPinned;
</span><span class="boring">
</span><span class="boring">#[derive(Debug)]
</span><span class="boring">struct Test {
</span><span class="boring">    a: String,
</span><span class="boring">    b: *const String,
</span><span class="boring">    _marker: PhantomPinned,
</span><span class="boring">}
</span><span class="boring">
</span><span class="boring">
</span><span class="boring">impl Test {
</span><span class="boring">    fn new(txt: &amp;str) -&gt; Self {
</span><span class="boring">        Test {
</span><span class="boring">            a: String::from(txt),
</span><span class="boring">            b: std::ptr::null(),
</span><span class="boring">            // This makes our type `!Unpin`
</span><span class="boring">            _marker: PhantomPinned,
</span><span class="boring">        }
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn init(self: Pin&lt;&amp;mut Self&gt;) {
</span><span class="boring">        let self_ptr: *const String = &amp;self.a;
</span><span class="boring">        let this = unsafe { self.get_unchecked_mut() };
</span><span class="boring">        this.b = self_ptr;
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn a(self: Pin&lt;&amp;Self&gt;) -&gt; &amp;str {
</span><span class="boring">        &amp;self.get_ref().a
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn b(self: Pin&lt;&amp;Self&gt;) -&gt; &amp;String {
</span><span class="boring">        assert!(!self.b.is_null(), "Test::b called without Test::init being called first");
</span><span class="boring">        unsafe { &amp;*(self.b) }
</span><span class="boring">    }
</span><span class="boring">}</span></code></pre></pre>
<p>Now, if we try to move our data now we get a compilation error:</p>
<pre><pre class="playground"><code class="language-rust  compile_fail">pub fn main() {
    let mut test1 = Test::new("test1");
    let mut test1 = unsafe { Pin::new_unchecked(&amp;mut test1) };
    Test::init(test1.as_mut());

    let mut test2 = Test::new("test2");
    let mut test2 = unsafe { Pin::new_unchecked(&amp;mut test2) };
    Test::init(test2.as_mut());

    println!("a: {}, b: {}", Test::a(test1.as_ref()), Test::b(test1.as_ref()));
    std::mem::swap(test1.get_mut(), test2.get_mut());
    println!("a: {}, b: {}", Test::a(test2.as_ref()), Test::b(test2.as_ref()));
}
<span class="boring">use std::pin::Pin;
</span><span class="boring">use std::marker::PhantomPinned;
</span><span class="boring">
</span><span class="boring">#[derive(Debug)]
</span><span class="boring">struct Test {
</span><span class="boring">    a: String,
</span><span class="boring">    b: *const String,
</span><span class="boring">    _marker: PhantomPinned,
</span><span class="boring">}
</span><span class="boring">
</span><span class="boring">
</span><span class="boring">impl Test {
</span><span class="boring">    fn new(txt: &amp;str) -&gt; Self {
</span><span class="boring">        Test {
</span><span class="boring">            a: String::from(txt),
</span><span class="boring">            b: std::ptr::null(),
</span><span class="boring">            _marker: PhantomPinned, // This makes our type `!Unpin`
</span><span class="boring">        }
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn init(self: Pin&lt;&amp;mut Self&gt;) {
</span><span class="boring">        let self_ptr: *const String = &amp;self.a;
</span><span class="boring">        let this = unsafe { self.get_unchecked_mut() };
</span><span class="boring">        this.b = self_ptr;
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn a(self: Pin&lt;&amp;Self&gt;) -&gt; &amp;str {
</span><span class="boring">        &amp;self.get_ref().a
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn b(self: Pin&lt;&amp;Self&gt;) -&gt; &amp;String {
</span><span class="boring">        assert!(!self.b.is_null(), "Test::b called without Test::init being called first");
</span><span class="boring">        unsafe { &amp;*(self.b) }
</span><span class="boring">    }
</span><span class="boring">}</span></code></pre></pre>
<p>The type system prevents us from moving the data, as follows:</p>
<pre><code>error[E0277]: `PhantomPinned` cannot be unpinned
   --&gt; src\test.rs:56:30
    |
56  |         std::mem::swap(test1.get_mut(), test2.get_mut());
    |                              ^^^^^^^ within `test1::Test`, the trait `Unpin` is not implemented for `PhantomPinned`
    |
    = note: consider using `Box::pin`
note: required because it appears within the type `test1::Test`
   --&gt; src\test.rs:7:8
    |
7   | struct Test {
    |        ^^^^
note: required by a bound in `std::pin::Pin::&lt;&amp;'a mut T&gt;::get_mut`
   --&gt; &lt;...&gt;rustlib/src/rust\library\core\src\pin.rs:748:12
    |
748 |         T: Unpin,
    |            ^^^^^ required by this bound in `std::pin::Pin::&lt;&amp;'a mut T&gt;::get_mut`
</code></pre>
<blockquote>
<p>It's important to note that stack pinning will always rely on guarantees
you give when writing <code>unsafe</code>. While we know that the <em>pointee</em> of <code>&amp;'a mut T</code>
is pinned for the lifetime of <code>'a</code> we can't know if the data <code>&amp;'a mut T</code>
points to isn't moved after <code>'a</code> ends. If it does it will violate the Pin
contract.</p>
<p>A mistake that is easy to make is forgetting to shadow the original variable
since you could drop the <code>Pin</code> and move the data after <code>&amp;'a mut T</code>
like shown below (which violates the Pin contract):</p>
<pre><pre class="playground"><code class="language-rust">fn main() {
   let mut test1 = Test::new("test1");
   let mut test1_pin = unsafe { Pin::new_unchecked(&amp;mut test1) };
   Test::init(test1_pin.as_mut());

   drop(test1_pin);
   println!(r#"test1.b points to "test1": {:?}..."#, test1.b);

   let mut test2 = Test::new("test2");
   mem::swap(&amp;mut test1, &amp;mut test2);
   println!("... and now it points nowhere: {:?}", test1.b);
}
<span class="boring">use std::pin::Pin;
</span><span class="boring">use std::marker::PhantomPinned;
</span><span class="boring">use std::mem;
</span><span class="boring">
</span><span class="boring">#[derive(Debug)]
</span><span class="boring">struct Test {
</span><span class="boring">    a: String,
</span><span class="boring">    b: *const String,
</span><span class="boring">    _marker: PhantomPinned,
</span><span class="boring">}
</span><span class="boring">
</span><span class="boring">
</span><span class="boring">impl Test {
</span><span class="boring">    fn new(txt: &amp;str) -&gt; Self {
</span><span class="boring">        Test {
</span><span class="boring">            a: String::from(txt),
</span><span class="boring">            b: std::ptr::null(),
</span><span class="boring">            // This makes our type `!Unpin`
</span><span class="boring">            _marker: PhantomPinned,
</span><span class="boring">        }
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn init&lt;'a&gt;(self: Pin&lt;&amp;'a mut Self&gt;) {
</span><span class="boring">        let self_ptr: *const String = &amp;self.a;
</span><span class="boring">        let this = unsafe { self.get_unchecked_mut() };
</span><span class="boring">        this.b = self_ptr;
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    #[allow(unused)]
</span><span class="boring">    fn a&lt;'a&gt;(self: Pin&lt;&amp;'a Self&gt;) -&gt; &amp;'a str {
</span><span class="boring">        &amp;self.get_ref().a
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    #[allow(unused)]
</span><span class="boring">    fn b&lt;'a&gt;(self: Pin&lt;&amp;'a Self&gt;) -&gt; &amp;'a String {
</span><span class="boring">        assert!(!self.b.is_null(), "Test::b called without Test::init being called first");
</span><span class="boring">        unsafe { &amp;*(self.b) }
</span><span class="boring">    }
</span><span class="boring">}</span></code></pre></pre>
</blockquote>
<h3 id="pinning-to-the-heap"><a class="header" href="#pinning-to-the-heap">Pinning to the Heap</a></h3>
<p>Pinning an <code>!Unpin</code> type to the heap gives our data a stable address so we know
that the data we point to can't move after it's pinned. In contrast to stack
pinning, we know that the data will be pinned for the lifetime of the object.</p>
<pre><pre class="playground"><code class="language-rust  edition2018">use std::pin::Pin;
use std::marker::PhantomPinned;

#[derive(Debug)]
struct Test {
    a: String,
    b: *const String,
    _marker: PhantomPinned,
}

impl Test {
    fn new(txt: &amp;str) -&gt; Pin&lt;Box&lt;Self&gt;&gt; {
        let t = Test {
            a: String::from(txt),
            b: std::ptr::null(),
            _marker: PhantomPinned,
        };
        let mut boxed = Box::pin(t);
        let self_ptr: *const String = &amp;boxed.a;
        unsafe { boxed.as_mut().get_unchecked_mut().b = self_ptr };

        boxed
    }

    fn a(self: Pin&lt;&amp;Self&gt;) -&gt; &amp;str {
        &amp;self.get_ref().a
    }

    fn b(self: Pin&lt;&amp;Self&gt;) -&gt; &amp;String {
        unsafe { &amp;*(self.b) }
    }
}

pub fn main() {
    let test1 = Test::new("test1");
    let test2 = Test::new("test2");

    println!("a: {}, b: {}",test1.as_ref().a(), test1.as_ref().b());
    println!("a: {}, b: {}",test2.as_ref().a(), test2.as_ref().b());
}</code></pre></pre>
<p>Some functions require the futures they work with to be <code>Unpin</code>. To use a
<code>Future</code> or <code>Stream</code> that isn't <code>Unpin</code> with a function that requires
<code>Unpin</code> types, you'll first have to pin the value using either
<code>Box::pin</code> (to create a <code>Pin&lt;Box&lt;T&gt;&gt;</code>) or the <code>pin_utils::pin_mut!</code> macro
(to create a <code>Pin&lt;&amp;mut T&gt;</code>). <code>Pin&lt;Box&lt;Fut&gt;&gt;</code> and <code>Pin&lt;&amp;mut Fut&gt;</code> can both be
used as futures, and both implement <code>Unpin</code>.</p>
<p>For example:</p>
<pre><code class="language-rust edition2018 ignore">use pin_utils::pin_mut; // `pin_utils` is a handy crate available on crates.io

// A function which takes a `Future` that implements `Unpin`.
fn execute_unpin_future(x: impl Future&lt;Output = ()&gt; + Unpin) { /* ... */ }

let fut = async { /* ... */ };
execute_unpin_future(fut); // Error: `fut` does not implement `Unpin` trait

// Pinning with `Box`:
let fut = async { /* ... */ };
let fut = Box::pin(fut);
execute_unpin_future(fut); // OK

// Pinning with `pin_mut!`:
let fut = async { /* ... */ };
pin_mut!(fut);
execute_unpin_future(fut); // OK</code></pre>
<h2 id="summary-1"><a class="header" href="#summary-1">Summary</a></h2>
<ol>
<li>
<p>If <code>T: Unpin</code> (which is the default), then <code>Pin&lt;'a, T&gt;</code> is entirely
equivalent to <code>&amp;'a mut T</code>. In other words: <code>Unpin</code> means it's OK for this type
to be moved even when pinned, so <code>Pin</code> will have no effect on such a type.</p>
</li>
<li>
<p>Getting a <code>&amp;mut T</code> to a pinned T requires unsafe if <code>T: !Unpin</code>.</p>
</li>
<li>
<p>Most standard library types implement <code>Unpin</code>. The same goes for most
"normal" types you encounter in Rust. A <code>Future</code> generated by async/await is an exception to this rule.</p>
</li>
<li>
<p>You can add a <code>!Unpin</code> bound on a type on nightly with a feature flag, or
by adding <code>std::marker::PhantomPinned</code> to your type on stable.</p>
</li>
<li>
<p>You can either pin data to the stack or to the heap.</p>
</li>
<li>
<p>Pinning a <code>!Unpin</code> object to the stack requires <code>unsafe</code></p>
</li>
<li>
<p>Pinning a <code>!Unpin</code> object to the heap does not require <code>unsafe</code>. There is a shortcut for doing this using <code>Box::pin</code>.</p>
</li>
<li>
<p>For pinned data where <code>T: !Unpin</code> you have to maintain the invariant that its memory will not
get invalidated or repurposed <em>from the moment it gets pinned until when drop</em> is called. This is
an important part of the <em>pin contract</em>.</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-stream-trait"><a class="header" href="#the-stream-trait">The <code>Stream</code> Trait</a></h1>
<p>The <code>Stream</code> trait is similar to <code>Future</code> but can yield multiple values before
completing, similar to the <code>Iterator</code> trait from the standard library:</p>
<pre><code class="language-rust ignore">trait Stream {
    /// The type of the value yielded by the stream.
    type Item;

    /// Attempt to resolve the next item in the stream.
    /// Returns `Poll::Pending` if not ready, `Poll::Ready(Some(x))` if a value
    /// is ready, and `Poll::Ready(None)` if the stream has completed.
    fn poll_next(self: Pin&lt;&amp;mut Self&gt;, cx: &amp;mut Context&lt;'_&gt;)
        -&gt; Poll&lt;Option&lt;Self::Item&gt;&gt;;
}</code></pre>
<p>One common example of a <code>Stream</code> is the <code>Receiver</code> for the channel type from
the <code>futures</code> crate. It will yield <code>Some(val)</code> every time a value is sent
from the <code>Sender</code> end, and will yield <code>None</code> once the <code>Sender</code> has been
dropped and all pending messages have been received:</p>
<pre><code class="language-rust edition2018 ignore">async fn send_recv() {
    const BUFFER_SIZE: usize = 10;
    let (mut tx, mut rx) = mpsc::channel::&lt;i32&gt;(BUFFER_SIZE);

    tx.send(1).await.unwrap();
    tx.send(2).await.unwrap();
    drop(tx);

    // `StreamExt::next` is similar to `Iterator::next`, but returns a
    // type that implements `Future&lt;Output = Option&lt;T&gt;&gt;`.
    assert_eq!(Some(1), rx.next().await);
    assert_eq!(Some(2), rx.next().await);
    assert_eq!(None, rx.next().await);
}</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="iteration-and-concurrency"><a class="header" href="#iteration-and-concurrency">Iteration and Concurrency</a></h1>
<p>Similar to synchronous <code>Iterator</code>s, there are many different ways to iterate
over and process the values in a <code>Stream</code>. There are combinator-style methods
such as <code>map</code>, <code>filter</code>, and <code>fold</code>, and their early-exit-on-error cousins
<code>try_map</code>, <code>try_filter</code>, and <code>try_fold</code>.</p>
<p>Unfortunately, <code>for</code> loops are not usable with <code>Stream</code>s, but for
imperative-style code, <code>while let</code> and the <code>next</code>/<code>try_next</code> functions can
be used:</p>
<pre><code class="language-rust edition2018 ignore">async fn sum_with_next(mut stream: Pin&lt;&amp;mut dyn Stream&lt;Item = i32&gt;&gt;) -&gt; i32 {
    use futures::stream::StreamExt; // for `next`
    let mut sum = 0;
    while let Some(item) = stream.next().await {
        sum += item;
    }
    sum
}

async fn sum_with_try_next(
    mut stream: Pin&lt;&amp;mut dyn Stream&lt;Item = Result&lt;i32, io::Error&gt;&gt;&gt;,
) -&gt; Result&lt;i32, io::Error&gt; {
    use futures::stream::TryStreamExt; // for `try_next`
    let mut sum = 0;
    while let Some(item) = stream.try_next().await? {
        sum += item;
    }
    Ok(sum)
}</code></pre>
<p>However, if we're just processing one element at a time, we're potentially
leaving behind opportunity for concurrency, which is, after all, why we're
writing async code in the first place. To process multiple items from a stream
concurrently, use the <code>for_each_concurrent</code> and <code>try_for_each_concurrent</code>
methods:</p>
<pre><code class="language-rust edition2018 ignore">async fn jump_around(
    mut stream: Pin&lt;&amp;mut dyn Stream&lt;Item = Result&lt;u8, io::Error&gt;&gt;&gt;,
) -&gt; Result&lt;(), io::Error&gt; {
    use futures::stream::TryStreamExt; // for `try_for_each_concurrent`
    const MAX_CONCURRENT_JUMPERS: usize = 100;

    stream.try_for_each_concurrent(MAX_CONCURRENT_JUMPERS, |num| async move {
        jump_n_times(num).await?;
        report_n_jumps(num).await?;
        Ok(())
    }).await?;

    Ok(())
}</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="executing-multiple-futures-at-a-time"><a class="header" href="#executing-multiple-futures-at-a-time">Executing Multiple Futures at a Time</a></h1>
<p>Up until now, we've mostly executed futures by using <code>.await</code>, which blocks
the current task until a particular <code>Future</code> completes. However, real
asynchronous applications often need to execute several different
operations concurrently.</p>
<p>In this chapter, we'll cover some ways to execute multiple asynchronous
operations at the same time:</p>
<ul>
<li><code>join!</code>: waits for futures to all complete</li>
<li><code>select!</code>: waits for one of several futures to complete</li>
<li>Spawning: creates a top-level task which ambiently runs a future to completion</li>
<li><code>FuturesUnordered</code>: a group of futures which yields the result of each subfuture</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="join-1"><a class="header" href="#join-1"><code>join!</code></a></h1>
<p>The <code>futures::join</code> macro makes it possible to wait for multiple different
futures to complete while executing them all concurrently.</p>
<h2 id="join-2"><a class="header" href="#join-2"><code>join!</code></a></h2>
<p>When performing multiple asynchronous operations, it's tempting to simply
<code>.await</code> them in a series:</p>
<pre><code class="language-rust edition2018 ignore">async fn get_book_and_music() -&gt; (Book, Music) {
    let book = get_book().await;
    let music = get_music().await;
    (book, music)
}</code></pre>
<p>However, this will be slower than necessary, since it won't start trying to
<code>get_music</code> until after <code>get_book</code> has completed. In some other languages,
futures are ambiently run to completion, so two operations can be
run concurrently by first calling each <code>async fn</code> to start the futures, and
then awaiting them both:</p>
<pre><code class="language-rust edition2018 ignore">// WRONG -- don't do this
async fn get_book_and_music() -&gt; (Book, Music) {
    let book_future = get_book();
    let music_future = get_music();
    (book_future.await, music_future.await)
}</code></pre>
<p>However, Rust futures won't do any work until they're actively <code>.await</code>ed.
This means that the two code snippets above will both run
<code>book_future</code> and <code>music_future</code> in series rather than running them
concurrently. To correctly run the two futures concurrently, use
<code>futures::join!</code>:</p>
<pre><code class="language-rust edition2018 ignore">use futures::join;

async fn get_book_and_music() -&gt; (Book, Music) {
    let book_fut = get_book();
    let music_fut = get_music();
    join!(book_fut, music_fut)
}</code></pre>
<p>The value returned by <code>join!</code> is a tuple containing the output of each
<code>Future</code> passed in.</p>
<h2 id="try_join"><a class="header" href="#try_join"><code>try_join!</code></a></h2>
<p>For futures which return <code>Result</code>, consider using <code>try_join!</code> rather than
<code>join!</code>. Since <code>join!</code> only completes once all subfutures have completed,
it'll continue processing other futures even after one of its subfutures
has returned an <code>Err</code>.</p>
<p>Unlike <code>join!</code>, <code>try_join!</code> will complete immediately if one of the subfutures
returns an error.</p>
<pre><code class="language-rust edition2018 ignore">use futures::try_join;

async fn get_book() -&gt; Result&lt;Book, String&gt; { /* ... */ Ok(Book) }
async fn get_music() -&gt; Result&lt;Music, String&gt; { /* ... */ Ok(Music) }

async fn get_book_and_music() -&gt; Result&lt;(Book, Music), String&gt; {
    let book_fut = get_book();
    let music_fut = get_music();
    try_join!(book_fut, music_fut)
}</code></pre>
<p>Note that the futures passed to <code>try_join!</code> must all have the same error type.
Consider using the <code>.map_err(|e| ...)</code> and <code>.err_into()</code> functions from
<code>futures::future::TryFutureExt</code> to consolidate the error types:</p>
<pre><code class="language-rust edition2018 ignore">use futures::{
    future::TryFutureExt,
    try_join,
};

async fn get_book() -&gt; Result&lt;Book, ()&gt; { /* ... */ Ok(Book) }
async fn get_music() -&gt; Result&lt;Music, String&gt; { /* ... */ Ok(Music) }

async fn get_book_and_music() -&gt; Result&lt;(Book, Music), String&gt; {
    let book_fut = get_book().map_err(|()| "Unable to get book".to_string());
    let music_fut = get_music();
    try_join!(book_fut, music_fut)
}</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="select"><a class="header" href="#select"><code>select!</code></a></h1>
<p>The <code>futures::select</code> macro runs multiple futures simultaneously, allowing
the user to respond as soon as any future completes.</p>
<pre><pre class="playground"><code class="language-rust edition2018"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use futures::{
    future::FutureExt, // for `.fuse()`
    pin_mut,
    select,
};

async fn task_one() { /* ... */ }
async fn task_two() { /* ... */ }

async fn race_tasks() {
    let t1 = task_one().fuse();
    let t2 = task_two().fuse();

    pin_mut!(t1, t2);

    select! {
        () = t1 =&gt; println!("task one completed first"),
        () = t2 =&gt; println!("task two completed first"),
    }
}
<span class="boring">}</span></code></pre></pre>
<p>The function above will run both <code>t1</code> and <code>t2</code> concurrently. When either
<code>t1</code> or <code>t2</code> finishes, the corresponding handler will call <code>println!</code>, and
the function will end without completing the remaining task.</p>
<p>The basic syntax for <code>select</code> is <code>&lt;pattern&gt; = &lt;expression&gt; =&gt; &lt;code&gt;,</code>,
repeated for as many futures as you would like to <code>select</code> over.</p>
<h2 id="default---and-complete--"><a class="header" href="#default---and-complete--"><code>default =&gt; ...</code> and <code>complete =&gt; ...</code></a></h2>
<p><code>select</code> also supports <code>default</code> and <code>complete</code> branches.</p>
<p>A <code>default</code> branch will run if none of the futures being <code>select</code>ed
over are yet complete. A <code>select</code> with a <code>default</code> branch will
therefore always return immediately, since <code>default</code> will be run
if none of the other futures are ready.</p>
<p><code>complete</code> branches can be used to handle the case where all futures
being <code>select</code>ed over have completed and will no longer make progress.
This is often handy when looping over a <code>select!</code>.</p>
<pre><pre class="playground"><code class="language-rust edition2018"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use futures::{future, select};

async fn count() {
    let mut a_fut = future::ready(4);
    let mut b_fut = future::ready(6);
    let mut total = 0;

    loop {
        select! {
            a = a_fut =&gt; total += a,
            b = b_fut =&gt; total += b,
            complete =&gt; break,
            default =&gt; unreachable!(), // never runs (futures are ready, then complete)
        };
    }
    assert_eq!(total, 10);
}
<span class="boring">}</span></code></pre></pre>
<h2 id="interaction-with-unpin-and-fusedfuture"><a class="header" href="#interaction-with-unpin-and-fusedfuture">Interaction with <code>Unpin</code> and <code>FusedFuture</code></a></h2>
<p>One thing you may have noticed in the first example above is that we
had to call <code>.fuse()</code> on the futures returned by the two <code>async fn</code>s,
as well as pinning them with <code>pin_mut</code>. Both of these calls are necessary
because the futures used in <code>select</code> must implement both the <code>Unpin</code>
trait and the <code>FusedFuture</code> trait.</p>
<p><code>Unpin</code> is necessary because the futures used by <code>select</code> are not
taken by value, but by mutable reference. By not taking ownership
of the future, uncompleted futures can be used again after the
call to <code>select</code>.</p>
<p>Similarly, the <code>FusedFuture</code> trait is required because <code>select</code> must
not poll a future after it has completed. <code>FusedFuture</code> is implemented
by futures which track whether or not they have completed. This makes
it possible to use <code>select</code> in a loop, only polling the futures which
still have yet to complete. This can be seen in the example above,
where <code>a_fut</code> or <code>b_fut</code> will have completed the second time through
the loop. Because the future returned by <code>future::ready</code> implements
<code>FusedFuture</code>, it's able to tell <code>select</code> not to poll it again.</p>
<p>Note that streams have a corresponding <code>FusedStream</code> trait. Streams
which implement this trait or have been wrapped using <code>.fuse()</code>
will yield <code>FusedFuture</code> futures from their
<code>.next()</code> / <code>.try_next()</code> combinators.</p>
<pre><pre class="playground"><code class="language-rust edition2018"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use futures::{
    stream::{Stream, StreamExt, FusedStream},
    select,
};

async fn add_two_streams(
    mut s1: impl Stream&lt;Item = u8&gt; + FusedStream + Unpin,
    mut s2: impl Stream&lt;Item = u8&gt; + FusedStream + Unpin,
) -&gt; u8 {
    let mut total = 0;

    loop {
        let item = select! {
            x = s1.next() =&gt; x,
            x = s2.next() =&gt; x,
            complete =&gt; break,
        };
        if let Some(next_num) = item {
            total += next_num;
        }
    }

    total
}
<span class="boring">}</span></code></pre></pre>
<h2 id="concurrent-tasks-in-a-select-loop-with-fuse-and-futuresunordered"><a class="header" href="#concurrent-tasks-in-a-select-loop-with-fuse-and-futuresunordered">Concurrent tasks in a <code>select</code> loop with <code>Fuse</code> and <code>FuturesUnordered</code></a></h2>
<p>One somewhat hard-to-discover but handy function is <code>Fuse::terminated()</code>,
which allows constructing an empty future which is already terminated,
and can later be filled in with a future that needs to be run.</p>
<p>This can be handy when there's a task that needs to be run during a <code>select</code>
loop but which is created inside the <code>select</code> loop itself.</p>
<p>Note the use of the <code>.select_next_some()</code> function. This can be
used with <code>select</code> to only run the branch for <code>Some(_)</code> values
returned from the stream, ignoring <code>None</code>s.</p>
<pre><pre class="playground"><code class="language-rust edition2018"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use futures::{
    future::{Fuse, FusedFuture, FutureExt},
    stream::{FusedStream, Stream, StreamExt},
    pin_mut,
    select,
};

async fn get_new_num() -&gt; u8 { /* ... */ 5 }

async fn run_on_new_num(_: u8) { /* ... */ }

async fn run_loop(
    mut interval_timer: impl Stream&lt;Item = ()&gt; + FusedStream + Unpin,
    starting_num: u8,
) {
    let run_on_new_num_fut = run_on_new_num(starting_num).fuse();
    let get_new_num_fut = Fuse::terminated();
    pin_mut!(run_on_new_num_fut, get_new_num_fut);
    loop {
        select! {
            () = interval_timer.select_next_some() =&gt; {
                // The timer has elapsed. Start a new `get_new_num_fut`
                // if one was not already running.
                if get_new_num_fut.is_terminated() {
                    get_new_num_fut.set(get_new_num().fuse());
                }
            },
            new_num = get_new_num_fut =&gt; {
                // A new number has arrived -- start a new `run_on_new_num_fut`,
                // dropping the old one.
                run_on_new_num_fut.set(run_on_new_num(new_num).fuse());
            },
            // Run the `run_on_new_num_fut`
            () = run_on_new_num_fut =&gt; {},
            // panic if everything completed, since the `interval_timer` should
            // keep yielding values indefinitely.
            complete =&gt; panic!("`interval_timer` completed unexpectedly"),
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<p>When many copies of the same future need to be run simultaneously,
use the <code>FuturesUnordered</code> type. The following example is similar
to the one above, but will run each copy of <code>run_on_new_num_fut</code>
to completion, rather than aborting them when a new one is created.
It will also print out a value returned by <code>run_on_new_num_fut</code>.</p>
<pre><pre class="playground"><code class="language-rust edition2018"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use futures::{
    future::{Fuse, FusedFuture, FutureExt},
    stream::{FusedStream, FuturesUnordered, Stream, StreamExt},
    pin_mut,
    select,
};

async fn get_new_num() -&gt; u8 { /* ... */ 5 }

async fn run_on_new_num(_: u8) -&gt; u8 { /* ... */ 5 }

async fn run_loop(
    mut interval_timer: impl Stream&lt;Item = ()&gt; + FusedStream + Unpin,
    starting_num: u8,
) {
    let mut run_on_new_num_futs = FuturesUnordered::new();
    run_on_new_num_futs.push(run_on_new_num(starting_num));
    let get_new_num_fut = Fuse::terminated();
    pin_mut!(get_new_num_fut);
    loop {
        select! {
            () = interval_timer.select_next_some() =&gt; {
                // The timer has elapsed. Start a new `get_new_num_fut`
                // if one was not already running.
                if get_new_num_fut.is_terminated() {
                    get_new_num_fut.set(get_new_num().fuse());
                }
            },
            new_num = get_new_num_fut =&gt; {
                // A new number has arrived -- start a new `run_on_new_num_fut`.
                run_on_new_num_futs.push(run_on_new_num(new_num));
            },
            // Run the `run_on_new_num_futs` and check if any have completed
            res = run_on_new_num_futs.select_next_some() =&gt; {
                println!("run_on_new_num_fut returned {:?}", res);
            },
            // panic if everything completed, since the `interval_timer` should
            // keep yielding values indefinitely.
            complete =&gt; panic!("`interval_timer` completed unexpectedly"),
        }
    }
}

<span class="boring">}</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="spawning"><a class="header" href="#spawning"><code>Spawning</code></a></h1>
<p>Spawning allows you to run a new asynchronous task in the background. This allows us to continue executing other code
while it runs.</p>
<p>Say we have a web server that wants to accept connections without blocking the main thread.
To achieve this, we can use the <code>async_std::task::spawn</code> function to create and run a new task that handles the
connections. This function takes a future and returns a <code>JoinHandle</code>, which can be used to wait for the result of the
task once it's completed.</p>
<pre><pre class="playground"><code class="language-rust edition2018">use async_std::{task, net::TcpListener, net::TcpStream};
use futures::AsyncWriteExt;

async fn process_request(stream: &amp;mut TcpStream) -&gt; Result&lt;(), std::io::Error&gt;{
    stream.write_all(b"HTTP/1.1 200 OK\r\n\r\n").await?;
    stream.write_all(b"Hello World").await?;
    Ok(())
}

async fn main() {
    let listener = TcpListener::bind("127.0.0.1:8080").await.unwrap();
    loop {
        // Accept a new connection
        let (mut stream, _) = listener.accept().await.unwrap();
        // Now process this request without blocking the main loop
        task::spawn(async move {process_request(&amp;mut stream).await});
    }
}</code></pre></pre>
<p>The <code>JoinHandle</code> returned by <code>spawn</code> implements the <code>Future</code> trait, so we can <code>.await</code> it to get the result of the task.
This will block the current task until the spawned task completes. If the task is not awaited, your program will
continue executing without waiting for the task, cancelling it if the function is completed before the task is finished.</p>
<pre><pre class="playground"><code class="language-rust edition2018"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use futures::future::join_all;
async fn task_spawner(){
    let tasks = vec![
        task::spawn(my_task(Duration::from_secs(1))),
        task::spawn(my_task(Duration::from_secs(2))),
        task::spawn(my_task(Duration::from_secs(3))),
    ];
    // If we do not await these tasks and the function finishes, they will be dropped
    join_all(tasks).await;
}
<span class="boring">}</span></code></pre></pre>
<p>To communicate between the main task and the spawned task, we can use channels
provided by the async runtime used.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="workarounds-to-know-and-love"><a class="header" href="#workarounds-to-know-and-love">Workarounds to Know and Love</a></h1>
<p>Rust's <code>async</code> support is still fairly new, and there are a handful of
highly-requested features still under active development, as well
as some subpar diagnostics. This chapter will discuss some common pain
points and explain how to work around them.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="send-approximation"><a class="header" href="#send-approximation"><code>Send</code> Approximation</a></h1>
<p>Some <code>async fn</code> state machines are safe to be sent across threads, while
others are not. Whether or not an <code>async fn</code> <code>Future</code> is <code>Send</code> is determined
by whether a non-<code>Send</code> type is held across an <code>.await</code> point. The compiler
does its best to approximate when values may be held across an <code>.await</code>
point, but this analysis is too conservative in a number of places today.</p>
<p>For example, consider a simple non-<code>Send</code> type, perhaps a type
which contains an <code>Rc</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::rc::Rc;

#[derive(Default)]
struct NotSend(Rc&lt;()&gt;);
<span class="boring">}</span></code></pre></pre>
<p>Variables of type <code>NotSend</code> can briefly appear as temporaries in <code>async fn</code>s
even when the resulting <code>Future</code> type returned by the <code>async fn</code> must be <code>Send</code>:</p>
<pre><pre class="playground"><code class="language-rust edition2018"><span class="boring">use std::rc::Rc;
</span><span class="boring">#[derive(Default)]
</span><span class="boring">struct NotSend(Rc&lt;()&gt;);
</span>async fn bar() {}
async fn foo() {
    NotSend::default();
    bar().await;
}

fn require_send(_: impl Send) {}

fn main() {
    require_send(foo());
}</code></pre></pre>
<p>However, if we change <code>foo</code> to store <code>NotSend</code> in a variable, this example no
longer compiles:</p>
<pre><pre class="playground"><code class="language-rust edition2018"><span class="boring">use std::rc::Rc;
</span><span class="boring">#[derive(Default)]
</span><span class="boring">struct NotSend(Rc&lt;()&gt;);
</span><span class="boring">async fn bar() {}
</span>async fn foo() {
    let x = NotSend::default();
    bar().await;
}
<span class="boring">fn require_send(_: impl Send) {}
</span><span class="boring">fn main() {
</span><span class="boring">   require_send(foo());
</span><span class="boring">}</span></code></pre></pre>
<pre><code>error[E0277]: `std::rc::Rc&lt;()&gt;` cannot be sent between threads safely
  --&gt; src/main.rs:15:5
   |
15 |     require_send(foo());
   |     ^^^^^^^^^^^^ `std::rc::Rc&lt;()&gt;` cannot be sent between threads safely
   |
   = help: within `impl std::future::Future`, the trait `std::marker::Send` is not implemented for `std::rc::Rc&lt;()&gt;`
   = note: required because it appears within the type `NotSend`
   = note: required because it appears within the type `{NotSend, impl std::future::Future, ()}`
   = note: required because it appears within the type `[static generator@src/main.rs:7:16: 10:2 {NotSend, impl std::future::Future, ()}]`
   = note: required because it appears within the type `std::future::GenFuture&lt;[static generator@src/main.rs:7:16: 10:2 {NotSend, impl std::future::Future, ()}]&gt;`
   = note: required because it appears within the type `impl std::future::Future`
   = note: required because it appears within the type `impl std::future::Future`
note: required by `require_send`
  --&gt; src/main.rs:12:1
   |
12 | fn require_send(_: impl Send) {}
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

error: aborting due to previous error

For more information about this error, try `rustc --explain E0277`.
</code></pre>
<p>This error is correct. If we store <code>x</code> into a variable, it won't be dropped
until after the <code>.await</code>, at which point the <code>async fn</code> may be running on
a different thread. Since <code>Rc</code> is not <code>Send</code>, allowing it to travel across
threads would be unsound. One simple solution to this would be to <code>drop</code>
the <code>Rc</code> before the <code>.await</code>, but unfortunately that does not work today.</p>
<p>In order to successfully work around this issue, you may have to introduce
a block scope encapsulating any non-<code>Send</code> variables. This makes it easier
for the compiler to tell that these variables do not live across an
<code>.await</code> point.</p>
<pre><pre class="playground"><code class="language-rust edition2018"><span class="boring">use std::rc::Rc;
</span><span class="boring">#[derive(Default)]
</span><span class="boring">struct NotSend(Rc&lt;()&gt;);
</span><span class="boring">async fn bar() {}
</span>async fn foo() {
    {
        let x = NotSend::default();
    }
    bar().await;
}
<span class="boring">fn require_send(_: impl Send) {}
</span><span class="boring">fn main() {
</span><span class="boring">   require_send(foo());
</span><span class="boring">}</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="recursion-1"><a class="header" href="#recursion-1">Recursion</a></h1>
<p>Internally, <code>async fn</code> creates a state machine type containing each
sub-<code>Future</code> being <code>.await</code>ed. This makes recursive <code>async fn</code>s a little
tricky, since the resulting state machine type has to contain itself:</p>
<pre><pre class="playground"><code class="language-rust edition2018"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">async fn step_one() { /* ... */ }
</span><span class="boring">async fn step_two() { /* ... */ }
</span><span class="boring">struct StepOne;
</span><span class="boring">struct StepTwo;
</span>// This function:
async fn foo() {
    step_one().await;
    step_two().await;
}
// generates a type like this:
enum Foo {
    First(StepOne),
    Second(StepTwo),
}

// So this function:
async fn recursive() {
    recursive().await;
    recursive().await;
}

// generates a type like this:
enum Recursive {
    First(Recursive),
    Second(Recursive),
}
<span class="boring">}</span></code></pre></pre>
<p>This won't work—we've created an infinitely-sized type!
The compiler will complain:</p>
<pre><code>error[E0733]: recursion in an async fn requires boxing
 --&gt; src/lib.rs:1:1
  |
1 | async fn recursive() {
  | ^^^^^^^^^^^^^^^^^^^^
  |
  = note: a recursive `async fn` call must introduce indirection such as `Box::pin` to avoid an infinitely sized future
</code></pre>
<p>In order to allow this, we have to introduce an indirection using <code>Box</code>.</p>
<p>Prior to Rust 1.77, due to compiler limitations, just wrapping the calls to
<code>recursive()</code> in <code>Box::pin</code> isn't enough. To make this work, we have
to make <code>recursive</code> into a non-<code>async</code> function which returns a <code>.boxed()</code>
<code>async</code> block:</p>
<pre><pre class="playground"><code class="language-rust edition2018"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use futures::future::{BoxFuture, FutureExt};

fn recursive() -&gt; BoxFuture&lt;'static, ()&gt; {
    async move {
        recursive().await;
        recursive().await;
    }.boxed()
}
<span class="boring">}</span></code></pre></pre>
<p>In newer version of Rust, <a href="https://github.com/rust-lang/rust/pull/117703/">that compiler limitation has been lifted</a>.</p>
<p>Since Rust 1.77, support for recursion in <code>async fn</code> with allocation
indirection <a href="https://blog.rust-lang.org/2024/03/21/Rust-1.77.0.html#support-for-recursion-in-async-fn">becomes stable</a>, so recursive calls are permitted so long as they
use some form of indirection to avoid an infinite size for the state of the
function.</p>
<p>This means that code like this now works:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn recursive_pinned() {
    Box::pin(recursive_pinned()).await;
    Box::pin(recursive_pinned()).await;
}
<span class="boring">}</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="async-in-traits"><a class="header" href="#async-in-traits"><code>async</code> in Traits</a></h1>
<p>Currently, <code>async fn</code> cannot be used in traits on the stable release of Rust.
Since the 17th November 2022, an MVP of async-fn-in-trait is available on the nightly
version of the compiler tool chain, <a href="https://blog.rust-lang.org/inside-rust/2022/11/17/async-fn-in-trait-nightly.html">see here for details</a>.</p>
<p>In the meantime, there is a work around for the stable tool chain using the
<a href="https://github.com/dtolnay/async-trait">async-trait crate from crates.io</a>.</p>
<p>Note that using these trait methods will result in a heap allocation
per-function-call. This is not a significant cost for the vast majority
of applications, but should be considered when deciding whether to use
this functionality in the public API of a low-level function that is expected
to be called millions of times a second.</p>
<p>Last updates: https://blog.rust-lang.org/2023/12/21/async-fn-rpit-in-traits.html</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-async-ecosystem"><a class="header" href="#the-async-ecosystem">The Async Ecosystem</a></h1>
<p>Rust currently provides only the bare essentials for writing async code.
Importantly, executors, tasks, reactors, combinators, and low-level I/O futures and traits
are not yet provided in the standard library. In the meantime,
community-provided async ecosystems fill in these gaps.</p>
<p>The Async Foundations Team is interested in extending examples in the Async Book to cover multiple runtimes.
If you're interested in contributing to this project, please reach out to us on
<a href="https://rust-lang.zulipchat.com/#narrow/stream/201246-wg-async-foundations.2Fbook">Zulip</a>.</p>
<h2 id="async-runtimes"><a class="header" href="#async-runtimes">Async Runtimes</a></h2>
<p>Async runtimes are libraries used for executing async applications.
Runtimes usually bundle together a <em>reactor</em> with one or more <em>executors</em>.
Reactors provide subscription mechanisms for external events, like async I/O, interprocess communication, and timers.
In an async runtime, subscribers are typically futures representing low-level I/O operations.
Executors handle the scheduling and execution of tasks.
They keep track of running and suspended tasks, poll futures to completion, and wake tasks when they can make progress.
The word "executor" is frequently used interchangeably with "runtime".
Here, we use the word "ecosystem" to describe a runtime bundled with compatible traits and features.</p>
<h2 id="community-provided-async-crates"><a class="header" href="#community-provided-async-crates">Community-Provided Async Crates</a></h2>
<h3 id="the-futures-crate"><a class="header" href="#the-futures-crate">The Futures Crate</a></h3>
<p>The <a href="https://docs.rs/futures/"><code>futures</code> crate</a> contains traits and functions useful for writing async code.
This includes the <code>Stream</code>, <code>Sink</code>, <code>AsyncRead</code>, and <code>AsyncWrite</code> traits, and utilities such as combinators.
These utilities and traits may eventually become part of the standard library.</p>
<p><code>futures</code> has its own executor, but not its own reactor, so it does not support execution of async I/O or timer futures.
For this reason, it's not considered a full runtime.
A common choice is to use utilities from <code>futures</code> with an executor from another crate.</p>
<h3 id="popular-async-runtimes"><a class="header" href="#popular-async-runtimes">Popular Async Runtimes</a></h3>
<p>There is no asynchronous runtime in the standard library, and none are officially recommended.
The following crates provide popular runtimes.</p>
<ul>
<li><a href="https://docs.rs/tokio/">Tokio</a>: A popular async ecosystem with HTTP, gRPC, and tracing frameworks.</li>
<li><a href="https://docs.rs/async-std/">async-std</a>: A crate that provides asynchronous counterparts to standard library components.</li>
<li><a href="https://docs.rs/smol/">smol</a>: A small, simplified async runtime.
Provides the <code>Async</code> trait that can be used to wrap structs like <code>UnixStream</code> or <code>TcpListener</code>.</li>
<li><a href="https://fuchsia.googlesource.com/fuchsia/+/master/src/lib/fuchsia-async/">fuchsia-async</a>:
An executor for use in the Fuchsia OS.</li>
</ul>
<h2 id="determining-ecosystem-compatibility"><a class="header" href="#determining-ecosystem-compatibility">Determining Ecosystem Compatibility</a></h2>
<p>Not all async applications, frameworks, and libraries are compatible with each other, or with every OS or platform.
Most async code can be used with any ecosystem, but some frameworks and libraries require the use of a specific ecosystem.
Ecosystem constraints are not always documented, but there are several rules of thumb to determine
whether a library, trait, or function depends on a specific ecosystem.</p>
<p>Any async code that interacts with async I/O, timers, interprocess communication, or tasks
generally depends on a specific async executor or reactor.
All other async code, such as async expressions, combinators, synchronization types, and streams
are usually ecosystem independent, provided that any nested futures are also ecosystem independent.
Before beginning a project, it's recommended to research relevant async frameworks and libraries to ensure
compatibility with your chosen runtime and with each other.</p>
<p>Notably, <code>Tokio</code> uses the <code>mio</code> reactor and defines its own versions of async I/O traits,
including <code>AsyncRead</code> and <code>AsyncWrite</code>.
On its own, it's not compatible with <code>async-std</code> and <code>smol</code>,
which rely on the <a href="https://docs.rs/async-executor"><code>async-executor</code> crate</a>, and the <code>AsyncRead</code> and <code>AsyncWrite</code>
traits defined in <code>futures</code>.</p>
<p>Conflicting runtime requirements can sometimes be resolved by compatibility layers
that allow you to call code written for one runtime within another.
For example, the <a href="https://docs.rs/async_compat"><code>async_compat</code> crate</a> provides a compatibility layer between
<code>Tokio</code> and other runtimes.</p>
<p>Libraries exposing async APIs should not depend on a specific executor or reactor,
unless they need to spawn tasks or define their own async I/O or timer futures.
Ideally, only binaries should be responsible for scheduling and running tasks.</p>
<h2 id="single-threaded-vs-multi-threaded-executors"><a class="header" href="#single-threaded-vs-multi-threaded-executors">Single Threaded vs Multi-Threaded Executors</a></h2>
<p>Async executors can be single-threaded or multi-threaded.
For example, the <code>async-executor</code> crate has both a single-threaded <code>LocalExecutor</code> and a multi-threaded <code>Executor</code>.</p>
<p>A multi-threaded executor makes progress on several tasks simultaneously.
It can speed up the execution greatly for workloads with many tasks,
but synchronizing data between tasks is usually more expensive.
It is recommended to measure performance for your application
when you are choosing between a single- and a multi-threaded runtime.</p>
<p>Tasks can either be run on the thread that created them or on a separate thread.
Async runtimes often provide functionality for spawning tasks onto separate threads.
Even if tasks are executed on separate threads, they should still be non-blocking.
In order to schedule tasks on a multi-threaded executor, they must also be <code>Send</code>.
Some runtimes provide functions for spawning non-<code>Send</code> tasks,
which ensures every task is executed on the thread that spawned it.
They may also provide functions for spawning blocking tasks onto dedicated threads,
which is useful for running blocking synchronous code from other libraries.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="final-project-building-a-concurrent-web-server-with-async-rust"><a class="header" href="#final-project-building-a-concurrent-web-server-with-async-rust">Final Project: Building a Concurrent Web Server with Async Rust</a></h1>
<p>In this chapter, we'll use asynchronous Rust to modify the Rust book's
<a href="https://doc.rust-lang.org/book/ch20-01-single-threaded.html">single-threaded web server</a>
to serve requests concurrently.</p>
<h2 id="recap"><a class="header" href="#recap">Recap</a></h2>
<p>Here's what the code looked like at the end of the lesson.</p>
<p><code>src/main.rs</code>:</p>
<pre><pre class="playground"><code class="language-rust">use std::fs;
use std::io::prelude::*;
use std::net::TcpListener;
use std::net::TcpStream;

fn main() {
    // Listen for incoming TCP connections on localhost port 7878
    let listener = TcpListener::bind("127.0.0.1:7878").unwrap();

    // Block forever, handling each request that arrives at this IP address
    for stream in listener.incoming() {
        let stream = stream.unwrap();

        handle_connection(stream);
    }
}

fn handle_connection(mut stream: TcpStream) {
    // Read the first 1024 bytes of data from the stream
    let mut buffer = [0; 1024];
    stream.read(&amp;mut buffer).unwrap();

    let get = b"GET / HTTP/1.1\r\n";

    // Respond with greetings or a 404,
    // depending on the data in the request
    let (status_line, filename) = if buffer.starts_with(get) {
        ("HTTP/1.1 200 OK\r\n\r\n", "hello.html")
    } else {
        ("HTTP/1.1 404 NOT FOUND\r\n\r\n", "404.html")
    };
    let contents = fs::read_to_string(filename).unwrap();

    // Write response back to the stream,
    // and flush the stream to ensure the response is sent back to the client
    let response = format!("{status_line}{contents}");
    stream.write_all(response.as_bytes()).unwrap();
    stream.flush().unwrap();
}</code></pre></pre>
<p><code>hello.html</code>:</p>
<pre><code class="language-html">&lt;!DOCTYPE html&gt;
&lt;html lang="en"&gt;
  &lt;head&gt;
    &lt;meta charset="utf-8"&gt;
    &lt;title&gt;Hello!&lt;/title&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;h1&gt;Hello!&lt;/h1&gt;
    &lt;p&gt;Hi from Rust&lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p><code>404.html</code>:</p>
<pre><code class="language-html">&lt;!DOCTYPE html&gt;
&lt;html lang="en"&gt;
  &lt;head&gt;
    &lt;meta charset="utf-8"&gt;
    &lt;title&gt;Hello!&lt;/title&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;h1&gt;Oops!&lt;/h1&gt;
    &lt;p&gt;Sorry, I don't know what you're asking for.&lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>If you run the server with <code>cargo run</code> and visit <code>127.0.0.1:7878</code> in your browser,
you'll be greeted with a friendly message from Ferris!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="running-asynchronous-code"><a class="header" href="#running-asynchronous-code">Running Asynchronous Code</a></h1>
<p>An HTTP server should be able to serve multiple clients concurrently;
that is, it should not wait for previous requests to complete before handling the current request.
The book
<a href="https://doc.rust-lang.org/book/ch20-02-multithreaded.html#turning-our-single-threaded-server-into-a-multithreaded-server">solves this problem</a>
by creating a thread pool where each connection is handled on its own thread.
Here, instead of improving throughput by adding threads, we'll achieve the same effect using asynchronous code.</p>
<p>Let's modify <code>handle_connection</code> to return a future by declaring it an <code>async fn</code>:</p>
<pre><code class="language-rust ignore">async fn handle_connection(mut stream: TcpStream) {
    //&lt;-- snip --&gt;
}</code></pre>
<p>Adding <code>async</code> to the function declaration changes its return type
from the unit type <code>()</code> to a type that implements <code>Future&lt;Output=()&gt;</code>.</p>
<p>If we try to compile this, the compiler warns us that it will not work:</p>
<pre><code class="language-console">$ cargo check
    Checking async-rust v0.1.0 (file:///projects/async-rust)
warning: unused implementer of `std::future::Future` that must be used
  --&gt; src/main.rs:12:9
   |
12 |         handle_connection(stream);
   |         ^^^^^^^^^^^^^^^^^^^^^^^^^^
   |
   = note: `#[warn(unused_must_use)]` on by default
   = note: futures do nothing unless you `.await` or poll them
</code></pre>
<p>Because we haven't <code>await</code>ed or <code>poll</code>ed the result of <code>handle_connection</code>,
it'll never run. If you run the server and visit <code>127.0.0.1:7878</code> in a browser,
you'll see that the connection is refused; our server is not handling requests.</p>
<p>We can't <code>await</code> or <code>poll</code> futures within synchronous code by itself.
We'll need an asynchronous runtime to handle scheduling and running futures to completion.
Please consult the <a href="09_example/../08_ecosystem/00_chapter.html">section on choosing a runtime</a>
for more information on asynchronous runtimes, executors, and reactors.
Any of the runtimes listed will work for this project, but for these examples,
we've chosen to use the <code>async-std</code> crate.</p>
<h2 id="adding-an-async-runtime"><a class="header" href="#adding-an-async-runtime">Adding an Async Runtime</a></h2>
<p>The following example will demonstrate refactoring synchronous code to use an async runtime; here, <code>async-std</code>.
The <code>#[async_std::main]</code> attribute from <code>async-std</code> allows us to write an asynchronous main function.
To use it, enable the <code>attributes</code> feature of <code>async-std</code> in <code>Cargo.toml</code>:</p>
<pre><code class="language-toml">[dependencies.async-std]
version = "1.6"
features = ["attributes"]
</code></pre>
<p>As a first step, we'll switch to an asynchronous main function,
and <code>await</code> the future returned by the async version of <code>handle_connection</code>.
Then, we'll test how the server responds.
Here's what that would look like:</p>
<pre><pre class="playground"><code class="language-rust">#[async_std::main]
async fn main() {
    let listener = TcpListener::bind("127.0.0.1:7878").unwrap();
    for stream in listener.incoming() {
        let stream = stream.unwrap();
        // Warning: This is not concurrent!
        handle_connection(stream).await;
    }
}</code></pre></pre>
<p>Now, let's test to see if our server can handle connections concurrently.
Simply making <code>handle_connection</code> asynchronous doesn't mean that the server
can handle multiple connections at the same time, and we'll soon see why.</p>
<p>To illustrate this, let's simulate a slow request.
When a client makes a request to <code>127.0.0.1:7878/sleep</code>,
our server will sleep for 5 seconds:</p>
<pre><code class="language-rust ignore">use std::time::Duration;
use async_std::task;

async fn handle_connection(mut stream: TcpStream) {
    let mut buffer = [0; 1024];
    stream.read(&amp;mut buffer).unwrap();

    let get = b"GET / HTTP/1.1\r\n";
    let sleep = b"GET /sleep HTTP/1.1\r\n";

    let (status_line, filename) = if buffer.starts_with(get) {
        ("HTTP/1.1 200 OK\r\n\r\n", "hello.html")
    } else if buffer.starts_with(sleep) {
        task::sleep(Duration::from_secs(5)).await;
        ("HTTP/1.1 200 OK\r\n\r\n", "hello.html")
    } else {
        ("HTTP/1.1 404 NOT FOUND\r\n\r\n", "404.html")
    };
    let contents = fs::read_to_string(filename).unwrap();

    let response = format!("{status_line}{contents}");
    stream.write(response.as_bytes()).unwrap();
    stream.flush().unwrap();
}</code></pre>
<p>This is very similar to the
<a href="https://doc.rust-lang.org/book/ch20-02-multithreaded.html#simulating-a-slow-request-in-the-current-server-implementation">simulation of a slow request</a>
from the Book, but with one important difference:
we're using the non-blocking function <code>async_std::task::sleep</code> instead of the blocking function <code>std::thread::sleep</code>.
It's important to remember that even if a piece of code is run within an <code>async fn</code> and <code>await</code>ed, it may still block.
To test whether our server handles connections concurrently, we'll need to ensure that <code>handle_connection</code> is non-blocking.</p>
<p>If you run the server, you'll see that a request to <code>127.0.0.1:7878/sleep</code>
will block any other incoming requests for 5 seconds!
This is because there are no other concurrent tasks that can make progress
while we are <code>await</code>ing the result of <code>handle_connection</code>.
In the next section, we'll see how to use async code to handle connections concurrently.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="handling-connections-concurrently"><a class="header" href="#handling-connections-concurrently">Handling Connections Concurrently</a></h1>
<p>The problem with our code so far is that <code>listener.incoming()</code> is a blocking iterator.
The executor can't run other futures while <code>listener</code> waits on incoming connections,
and we can't handle a new connection until we're done with the previous one.</p>
<p>In order to fix this, we'll transform <code>listener.incoming()</code> from a blocking Iterator
to a non-blocking Stream. Streams are similar to Iterators, but can be consumed asynchronously.
For more information, see the <a href="09_example/../05_streams/01_chapter.html">chapter on Streams</a>.</p>
<p>Let's replace our blocking <code>std::net::TcpListener</code> with the non-blocking <code>async_std::net::TcpListener</code>,
and update our connection handler to accept an <code>async_std::net::TcpStream</code>:</p>
<pre><code class="language-rust ignore">use async_std::prelude::*;

async fn handle_connection(mut stream: TcpStream) {
    let mut buffer = [0; 1024];
    stream.read(&amp;mut buffer).await.unwrap();

    //&lt;-- snip --&gt;
    stream.write(response.as_bytes()).await.unwrap();
    stream.flush().await.unwrap();
}</code></pre>
<p>The asynchronous version of <code>TcpListener</code> implements the <code>Stream</code> trait for <code>listener.incoming()</code>,
a change which provides two benefits.
The first is that <code>listener.incoming()</code> no longer blocks the executor.
The executor can now yield to other pending futures
while there are no incoming TCP connections to be processed.</p>
<p>The second benefit is that elements from the Stream can optionally be processed concurrently,
using a Stream's <code>for_each_concurrent</code> method.
Here, we'll take advantage of this method to handle each incoming request concurrently.
We'll need to import the <code>Stream</code> trait from the <code>futures</code> crate, so our Cargo.toml now looks like this:</p>
<pre><code class="language-diff">+[dependencies]
+futures = "0.3"

 [dependencies.async-std]
 version = "1.6"
 features = ["attributes"]
</code></pre>
<p>Now, we can handle each connection concurrently by passing <code>handle_connection</code> in through a closure function.
The closure function takes ownership of each <code>TcpStream</code>, and is run as soon as a new <code>TcpStream</code> becomes available.
As long as <code>handle_connection</code> does not block, a slow request will no longer prevent other requests from completing.</p>
<pre><code class="language-rust ignore">use async_std::net::TcpListener;
use async_std::net::TcpStream;
use futures::stream::StreamExt;

#[async_std::main]
async fn main() {
    let listener = TcpListener::bind("127.0.0.1:7878").await.unwrap();
    listener
        .incoming()
        .for_each_concurrent(/* limit */ None, |tcpstream| async move {
            let tcpstream = tcpstream.unwrap();
            handle_connection(tcpstream).await;
        })
        .await;
}</code></pre>
<h1 id="serving-requests-in-parallel"><a class="header" href="#serving-requests-in-parallel">Serving Requests in Parallel</a></h1>
<p>Our example so far has largely presented cooperative multitasking concurrency (using async code)
as an alternative to preemptive multitasking (using threads).
However, async code and threads are not mutually exclusive.
In our example, <code>for_each_concurrent</code> processes each connection concurrently, but on the same thread.
The <code>async-std</code> crate allows us to spawn tasks onto separate threads as well.
Because <code>handle_connection</code> is both <code>Send</code> and non-blocking, it's safe to use with <code>async_std::task::spawn</code>.
Here's what that would look like:</p>
<pre><pre class="playground"><code class="language-rust">use async_std::task::spawn;

#[async_std::main]
async fn main() {
    let listener = TcpListener::bind("127.0.0.1:7878").await.unwrap();
    listener
        .incoming()
        .for_each_concurrent(/* limit */ None, |stream| async move {
            let stream = stream.unwrap();
            spawn(handle_connection(stream));
        })
        .await;
}</code></pre></pre>
<p>Now we are using both cooperative multitasking concurrency and preemptive multitasking to handle multiple requests at the same time!
See the <a href="09_example/../08_ecosystem/00_chapter.html#single-threading-vs-multithreading">section on multithreaded executors</a>
for more information.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="testing-the-tcp-server"><a class="header" href="#testing-the-tcp-server">Testing the TCP Server</a></h1>
<p>Let's move on to testing our <code>handle_connection</code> function.</p>
<p>First, we need a <code>TcpStream</code> to work with.
In an end-to-end or integration test, we might want to make a real TCP connection
to test our code.
One strategy for doing this is to start a listener on <code>localhost</code> port 0.
Port 0 isn't a valid UNIX port, but it'll work for testing.
The operating system will pick an open TCP port for us.</p>
<p>Instead, in this example we'll write a unit test for the connection handler,
to check that the correct responses are returned for the respective inputs.
To keep our unit test isolated and deterministic, we'll replace the <code>TcpStream</code> with a mock.</p>
<p>First, we'll change the signature of <code>handle_connection</code> to make it easier to test.
<code>handle_connection</code> doesn't actually require an <code>async_std::net::TcpStream</code>;
it requires any struct that implements <code>async_std::io::Read</code>, <code>async_std::io::Write</code>, and <code>marker::Unpin</code>.
Changing the type signature to reflect this allows us to pass a mock for testing.</p>
<pre><code class="language-rust ignore">use async_std::io::{Read, Write};

async fn handle_connection(mut stream: impl Read + Write + Unpin) {</code></pre>
<p>Next, let's build a mock <code>TcpStream</code> that implements these traits.
First, let's implement the <code>Read</code> trait, with one method, <code>poll_read</code>.
Our mock <code>TcpStream</code> will contain some data that is copied into the read buffer,
and we'll return <code>Poll::Ready</code> to signify that the read is complete.</p>
<pre><code class="language-rust ignore">    use super::*;
    use futures::io::Error;
    use futures::task::{Context, Poll};

    use std::cmp::min;
    use std::pin::Pin;

    struct MockTcpStream {
        read_data: Vec&lt;u8&gt;,
        write_data: Vec&lt;u8&gt;,
    }

    impl Read for MockTcpStream {
        fn poll_read(
            self: Pin&lt;&amp;mut Self&gt;,
            _: &amp;mut Context,
            buf: &amp;mut [u8],
        ) -&gt; Poll&lt;Result&lt;usize, Error&gt;&gt; {
            let size: usize = min(self.read_data.len(), buf.len());
            buf[..size].copy_from_slice(&amp;self.read_data[..size]);
            Poll::Ready(Ok(size))
        }
    }</code></pre>
<p>Our implementation of <code>Write</code> is very similar,
although we'll need to write three methods: <code>poll_write</code>, <code>poll_flush</code>, and <code>poll_close</code>.
<code>poll_write</code> will copy any input data into the mock <code>TcpStream</code>, and return <code>Poll::Ready</code> when complete.
No work needs to be done to flush or close the mock <code>TcpStream</code>, so <code>poll_flush</code> and <code>poll_close</code>
can just return <code>Poll::Ready</code>.</p>
<pre><code class="language-rust ignore">    impl Write for MockTcpStream {
        fn poll_write(
            mut self: Pin&lt;&amp;mut Self&gt;,
            _: &amp;mut Context,
            buf: &amp;[u8],
        ) -&gt; Poll&lt;Result&lt;usize, Error&gt;&gt; {
            self.write_data = Vec::from(buf);

            Poll::Ready(Ok(buf.len()))
        }

        fn poll_flush(self: Pin&lt;&amp;mut Self&gt;, _: &amp;mut Context) -&gt; Poll&lt;Result&lt;(), Error&gt;&gt; {
            Poll::Ready(Ok(()))
        }

        fn poll_close(self: Pin&lt;&amp;mut Self&gt;, _: &amp;mut Context) -&gt; Poll&lt;Result&lt;(), Error&gt;&gt; {
            Poll::Ready(Ok(()))
        }
    }</code></pre>
<p>Lastly, our mock will need to implement <code>Unpin</code>, signifying that its location in memory can safely be moved.
For more information on pinning and the <code>Unpin</code> trait, see the <a href="09_example/../04_pinning/01_chapter.html">section on pinning</a>.</p>
<pre><code class="language-rust ignore">    impl Unpin for MockTcpStream {}</code></pre>
<p>Now we're ready to test the <code>handle_connection</code> function.
After setting up the <code>MockTcpStream</code> containing some initial data,
we can run <code>handle_connection</code> using the attribute <code>#[async_std::test]</code>, similarly to how we used <code>#[async_std::main]</code>.
To ensure that <code>handle_connection</code> works as intended, we'll check that the correct data
was written to the <code>MockTcpStream</code> based on its initial contents.</p>
<pre><code class="language-rust ignore">    use std::fs;

    #[async_std::test]
    async fn test_handle_connection() {
        let input_bytes = b"GET / HTTP/1.1\r\n";
        let mut contents = vec![0u8; 1024];
        contents[..input_bytes.len()].clone_from_slice(input_bytes);
        let mut stream = MockTcpStream {
            read_data: contents,
            write_data: Vec::new(),
        };

        handle_connection(&amp;mut stream).await;

        let expected_contents = fs::read_to_string("hello.html").unwrap();
        let expected_response = format!("HTTP/1.1 200 OK\r\n\r\n{}", expected_contents);
        assert!(stream.write_data.starts_with(expected_response.as_bytes()));
    }</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="appendix--translations-of-the-book"><a class="header" href="#appendix--translations-of-the-book">Appendix : Translations of the Book</a></h1>
<p>For resources in languages other than English.</p>
<ul>
<li><a href="https://doc.rust-lang.ru/async-book/">Русский</a></li>
<li><a href="https://jimskapt.github.io/async-book-fr/">Français</a></li>
<li><a href="https://rouzbehsbz.github.io/rust-async-book/">فارسی</a></li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
